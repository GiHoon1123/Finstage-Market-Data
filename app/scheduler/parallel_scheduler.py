"""
ë³‘ë ¬ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ìŠ¤ì¼€ì¤„ëŸ¬ ëŸ¬ë„ˆ
"""

import asyncio
from apscheduler.schedulers.background import BackgroundScheduler

# ìœ í‹¸ë¦¬í‹° imports
from app.common.utils.parallel_executor import ParallelExecutor, measure_execution_time
from app.common.utils.logging_config import get_logger
from app.common.utils.memory_optimizer import memory_monitor, auto_memory_optimization
from app.common.utils.memory_utils import optimize_memory
from app.common.utils.db_session_manager import session_scope
from app.common.utils.task_queue import TaskQueue

# ì˜ˆì™¸ ì²˜ë¦¬ imports
from app.common.exceptions.handlers import handle_scheduler_errors, safe_execute
from app.common.exceptions.base import SchedulerError, ErrorCode

# ìƒìˆ˜ imports
from app.common.constants.symbol_names import (
    INDEX_SYMBOLS,
    FUTURES_SYMBOLS,
    STOCK_SYMBOLS,
    SYMBOL_PRICE_MAP,
)
from app.common.constants.rss_feeds import (
    INVESTING_ECONOMIC_SYMBOLS,
    INVESTING_MARKET_SYMBOLS,
)

# ì„œë¹„ìŠ¤ imports
from app.news_crawler.service.investing_news_crawler import InvestingNewsCrawler
from app.news_crawler.service.yahoo_news_crawler import YahooNewsCrawler
from app.market_price.service.price_high_record_service import PriceHighRecordService
from app.market_price.service.price_snapshot_service import PriceSnapshotService
from app.market_price.service.price_monitor_service import PriceMonitorService
from app.technical_analysis.service.async_technical_indicator_service import (
    AsyncTechnicalIndicatorService,
)
from app.market_price.service.async_price_service import AsyncPriceService
from app.technical_analysis.service.daily_comprehensive_report_service import (
    DailyComprehensiveReportService,
)
from app.common.services.background_tasks import (
    run_daily_comprehensive_report_background,
    run_historical_data_collection_background,
    run_technical_analysis_batch_background,
)
from app.scheduler.scheduler_runner import (
    run_daily_index_analysis,
    run_outcome_tracking_update,
    initialize_recent_signals_tracking,
    run_pattern_discovery,
)

logger = get_logger("parallel_scheduler")


# ë³‘ë ¬ ì‹¤í–‰ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (max_workers ê°ì†Œë¡œ DB ì—°ê²° ë¶€í•˜ ê°ì†Œ)
executor = ParallelExecutor(max_workers=2)  # 3 â†’ 2ë¡œ ë” ê°ì†Œ


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
@auto_memory_optimization(threshold_percent=80.0)
@memory_monitor()
def run_integrated_news_crawling_parallel():
    """í†µí•© ë‰´ìŠ¤ í¬ë¡¤ë§ (ê²½ì œ ë‰´ìŠ¤ + ì§€ìˆ˜ ë‰´ìŠ¤)"""
    logger.info(
        "integrated_news_crawling_started",
        sources=["investing_economic", "yahoo_index"],
    )

    # 1. Investing ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
    def process_investing_symbol(symbol):
        logger.debug("processing_symbol", source="investing_economic", symbol=symbol)
        crawler = InvestingNewsCrawler(symbol)
        result = crawler.process_all()
        return result

    investing_results = executor.run_symbol_tasks_parallel(
        process_investing_symbol, INVESTING_ECONOMIC_SYMBOLS, delay=0.5
    )

    investing_success = sum(1 for r in investing_results if r is not None)
    logger.info(
        "news_crawling_completed",
        source="investing_economic",
        success_count=investing_success,
        total_count=len(INVESTING_ECONOMIC_SYMBOLS),
        success_rate=investing_success / len(INVESTING_ECONOMIC_SYMBOLS),
    )

    # 2. Yahoo ì§€ìˆ˜ ë‰´ìŠ¤ í¬ë¡¤ë§
    def process_yahoo_symbol(symbol):
        logger.debug("processing_symbol", source="yahoo_index", symbol=symbol)
        crawler = YahooNewsCrawler(symbol)
        result = crawler.process_all()
        return result

    yahoo_results = executor.run_symbol_tasks_parallel(
        process_yahoo_symbol, INDEX_SYMBOLS, delay=0.5
    )

    yahoo_success = sum(1 for r in yahoo_results if r is not None)
    logger.info(
        "news_crawling_completed",
        source="yahoo_index",
        success_count=yahoo_success,
        total_count=len(INDEX_SYMBOLS),
        success_rate=yahoo_success / len(INDEX_SYMBOLS),
    )

    total_success = investing_success + yahoo_success
    total_symbols = len(INVESTING_ECONOMIC_SYMBOLS) + len(INDEX_SYMBOLS)
    logger.info(
        "integrated_news_crawling_completed",
        total_success=total_success,
        total_symbols=total_symbols,
        overall_success_rate=total_success / total_symbols,
    )


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
@memory_monitor()
def run_investing_market_news_parallel():
    """Investing ì‹œì¥ ë‰´ìŠ¤ í¬ë¡¤ë§ (ë³‘ë ¬)"""
    logger.info("news_crawling_started", source="investing_market")

    def process_symbol(symbol):
        logger.debug("processing_symbol", source="investing_market", symbol=symbol)
        crawler = InvestingNewsCrawler(symbol)
        result = crawler.process_all()
        return result

    # ë³‘ë ¬ ì‹¤í–‰ (API ì œí•œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€)
    results = executor.run_symbol_tasks_parallel(
        process_symbol, INVESTING_MARKET_SYMBOLS, delay=0.5
    )

    success_count = sum(1 for r in results if r is not None)
    logger.info(
        "news_crawling_completed",
        source="investing_market",
        success_count=success_count,
        total_count=len(INVESTING_MARKET_SYMBOLS),
        success_rate=success_count / len(INVESTING_MARKET_SYMBOLS),
    )


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
@memory_monitor()
def run_yahoo_futures_news_parallel():
    """Yahoo ì„ ë¬¼ ë‰´ìŠ¤ í¬ë¡¤ë§ (ë³‘ë ¬)"""
    logger.info("news_crawling_started", source="yahoo_futures")

    def process_symbol(symbol):
        logger.debug("processing_symbol", source="yahoo_futures", symbol=symbol)
        crawler = YahooNewsCrawler(symbol)
        result = crawler.process_all()
        return result

    # ë³‘ë ¬ ì‹¤í–‰ (API ì œí•œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€)
    results = executor.run_symbol_tasks_parallel(
        process_symbol, FUTURES_SYMBOLS, delay=0.5
    )

    success_count = sum(1 for r in results if r is not None)
    logger.info(
        "news_crawling_completed",
        source="yahoo_futures",
        success_count=success_count,
        total_count=len(FUTURES_SYMBOLS),
        success_rate=success_count / len(FUTURES_SYMBOLS),
    )


# ê¸°ì¡´ ê°œë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ì€ í†µí•© í•¨ìˆ˜ë¡œ ëŒ€ì²´ë¨
# run_investing_economic_news_parallel() -> run_integrated_news_crawling_parallel()
# run_yahoo_index_news_parallel() -> run_integrated_news_crawling_parallel()


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
@memory_monitor()
def run_yahoo_stock_news_parallel():
    """Yahoo ì¢…ëª© ë‰´ìŠ¤ í¬ë¡¤ë§ (ë³‘ë ¬)"""
    logger.info("news_crawling_started", source="yahoo_stocks")

    def process_symbol(symbol):
        logger.debug("processing_symbol", source="yahoo_stocks", symbol=symbol)
        crawler = YahooNewsCrawler(symbol)
        result = crawler.process_all()
        return result

    # ë³‘ë ¬ ì‹¤í–‰ (API ì œí•œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€)
    results = executor.run_symbol_tasks_parallel(
        process_symbol, STOCK_SYMBOLS, delay=0.5
    )

    success_count = sum(1 for r in results if r is not None)
    logger.info(
        "news_crawling_completed",
        source="yahoo_stocks",
        success_count=success_count,
        total_count=len(STOCK_SYMBOLS),
        success_rate=success_count / len(STOCK_SYMBOLS),
    )


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
@memory_monitor()
def run_high_price_update_job_parallel():
    """ìƒì¥ í›„ ìµœê³ ê°€ ê°±ì‹  (ë³‘ë ¬)"""
    logger.info("high_price_update_started")

    def update_high_price(symbol):
        return safe_execute(
            lambda: _update_high_price_for_symbol(symbol),
            default_return=None,
            log_errors=True,
        )

    def _update_high_price_for_symbol(symbol):
        service = PriceHighRecordService()
        result = service.update_all_time_high(symbol)
        # ì„œë¹„ìŠ¤ ì‚¬ìš© í›„ ì„¸ì…˜ ì •ë¦¬
        if hasattr(service, "__del__"):
            service.__del__()
        return result

    # ë³‘ë ¬ ì‹¤í–‰ (API ì œí•œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€)
    results = executor.run_symbol_tasks_parallel(
        update_high_price,
        list(SYMBOL_PRICE_MAP.keys()),
        delay=2.0,  # 0.5 â†’ 2.0ìœ¼ë¡œ ì¦ê°€
    )

    success_count = sum(1 for r in results if r is not None)
    logger.info(
        "high_price_update_completed",
        success_count=success_count,
        total_count=len(SYMBOL_PRICE_MAP),
        success_rate=success_count / len(SYMBOL_PRICE_MAP),
    )


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
@memory_monitor()
def run_previous_close_snapshot_job_parallel():
    """ì „ì¼ ì¢…ê°€ ì €ì¥ (ë³‘ë ¬)"""
    logger.info("previous_close_snapshot_started")

    def save_previous_close(symbol):
        service = PriceSnapshotService()
        result = service.save_previous_close_if_needed(symbol)
        return result

    # ë³‘ë ¬ ì‹¤í–‰ (API ì œí•œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€)
    results = executor.run_symbol_tasks_parallel(
        save_previous_close, list(SYMBOL_PRICE_MAP.keys()), delay=0.5
    )

    success_count = sum(1 for r in results if r is not None)
    logger.info(
        "previous_close_snapshot_completed",
        success_count=success_count,
        total_count=len(SYMBOL_PRICE_MAP),
        success_rate=success_count / len(SYMBOL_PRICE_MAP),
    )


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
@memory_monitor()
def run_previous_high_snapshot_job_parallel():
    """ì „ì¼ ê³ ì  ì €ì¥ (ë³‘ë ¬)"""
    logger.info("previous_high_snapshot_started")

    def save_previous_high(symbol):
        service = PriceSnapshotService()
        result = service.save_previous_high_if_needed(symbol)
        return result

    # ë³‘ë ¬ ì‹¤í–‰ (API ì œí•œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€)
    results = executor.run_symbol_tasks_parallel(
        save_previous_high, list(SYMBOL_PRICE_MAP.keys()), delay=0.5
    )

    success_count = sum(1 for r in results if r is not None)
    logger.info(
        "previous_high_snapshot_completed",
        success_count=success_count,
        total_count=len(SYMBOL_PRICE_MAP),
        success_rate=success_count / len(SYMBOL_PRICE_MAP),
    )


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
@memory_monitor()
def run_previous_low_snapshot_job_parallel():
    """ì „ì¼ ì €ì  ì €ì¥ (ë³‘ë ¬)"""
    logger.info("previous_low_snapshot_started")

    def save_previous_low(symbol):
        service = PriceSnapshotService()
        result = service.save_previous_low_if_needed(symbol)
        return result

    # ë³‘ë ¬ ì‹¤í–‰ (API ì œí•œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€)
    results = executor.run_symbol_tasks_parallel(
        save_previous_low, list(SYMBOL_PRICE_MAP.keys()), delay=0.5
    )

    success_count = sum(1 for r in results if r is not None)
    logger.info(
        "previous_low_snapshot_completed",
        success_count=success_count,
        total_count=len(SYMBOL_PRICE_MAP),
        success_rate=success_count / len(SYMBOL_PRICE_MAP),
    )


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
@memory_monitor(threshold_mb=150.0)
def run_realtime_price_monitor_job_parallel():
    """ì‹¤ì‹œê°„ ê°€ê²© ëª¨ë‹ˆí„°ë§ (ë³‘ë ¬)"""
    logger.info("realtime_price_monitoring_started")

    async def run_async_price_monitoring():
        """ë¹„ë™ê¸° ê°€ê²© ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
        import asyncio

        async def check_price_async(symbol):
            try:
                with session_scope() as session:
                    service = PriceMonitorService()
                    if hasattr(service, "set_session"):
                        service.set_session(session)

                    result = await service.check_price_against_baseline(symbol)
                    return result
            except Exception as e:
                logger.error(f"price_monitoring_failed", symbol=symbol, error=str(e))
                return None

        # ì‹¬ë³¼ë“¤ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (ë™ì‹œì„± ì œí•œ)
        symbols = list(SYMBOL_PRICE_MAP.keys())
        batch_size = 5  # ë™ì‹œ ì²˜ë¦¬í•  ì‹¬ë³¼ ìˆ˜ ì œí•œ
        results = []

        for i in range(0, len(symbols), batch_size):
            batch = symbols[i : i + batch_size]

            # ë°°ì¹˜ ë‚´ ì‹¬ë³¼ë“¤ì„ ë™ì‹œì— ì²˜ë¦¬
            batch_tasks = [check_price_async(symbol) for symbol in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            results.extend(batch_results)

            # ë°°ì¹˜ ê°„ ì§€ì—° (API ì œí•œ ê³ ë ¤)
            await asyncio.sleep(1.0)

        return results

    # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    try:
        results = loop.run_until_complete(run_async_price_monitoring())
    finally:
        # ìƒˆë¡œ ìƒì„±ëœ ë£¨í”„ë§Œ ë‹«ê¸°
        if not loop.is_running():
            loop.close()

    success_count = sum(
        1 for r in results if r is not None and not isinstance(r, Exception)
    )
    logger.info(
        "realtime_price_monitoring_completed",
        success_count=success_count,
        total_count=len(SYMBOL_PRICE_MAP),
        success_rate=success_count / len(SYMBOL_PRICE_MAP),
    )


@memory_monitor()
def start_parallel_scheduler():
    """ë³‘ë ¬ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
    scheduler = BackgroundScheduler()

    logger.info("parallel_scheduler_starting")

    # ğŸ”§ í…ŒìŠ¤íŠ¸ìš©: ëª¨ë“  ì‘ì—…ì„ 3ë¶„ ê°„ê²©ìœ¼ë¡œ ì‹¤í–‰ (ë°ì´í„° íë¦„ í™•ì¸ìš©)
    scheduler.add_job(
        run_integrated_news_crawling_parallel, "interval", minutes=3
    )  # í†µí•© ë‰´ìŠ¤ 3ë¶„ë§ˆë‹¤

    scheduler.add_job(
        run_yahoo_futures_news_parallel, "interval", minutes=3
    )  # ì„ ë¬¼ ë‰´ìŠ¤ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(
        run_yahoo_stock_news_parallel, "interval", minutes=3
    )  # ì¢…ëª© ë‰´ìŠ¤ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(
        run_investing_market_news_parallel, "interval", minutes=3
    )  # ì‹œì¥ ë‰´ìŠ¤ 3ë¶„ë§ˆë‹¤

    # ê°€ê²© ê´€ë ¨ ì‘ì—…ë„ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(
        run_high_price_update_job_parallel, "interval", minutes=3
    )  # ìµœê³ ê°€ ê°±ì‹  3ë¶„ë§ˆë‹¤

    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ë„ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(
        run_realtime_price_monitor_job_parallel, "interval", minutes=3
    )  # ê°€ê²© ëª¨ë‹ˆí„°ë§ 3ë¶„ë§ˆë‹¤

    # ìŠ¤ëƒ…ìƒ· ì‘ì—…ë“¤ë„ 3ë¶„ë§ˆë‹¤ í™œì„±í™”
    scheduler.add_job(
        run_previous_close_snapshot_job_parallel, "interval", minutes=3
    )  # ì „ì¼ ì¢…ê°€ ìŠ¤ëƒ…ìƒ· 3ë¶„ë§ˆë‹¤
    scheduler.add_job(
        run_previous_high_snapshot_job_parallel, "interval", minutes=3
    )  # ì „ì¼ ê³ ì  ìŠ¤ëƒ…ìƒ· 3ë¶„ë§ˆë‹¤
    scheduler.add_job(
        run_previous_low_snapshot_job_parallel, "interval", minutes=3
    )  # ì „ì¼ ì €ì  ìŠ¤ëƒ…ìƒ· 3ë¶„ë§ˆë‹¤

    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ë“¤ë„ 3ë¶„ë§ˆë‹¤ í…ŒìŠ¤íŠ¸
    task_queue = TaskQueue()

    # ì¼ì¼ ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ë°±ê·¸ë¼ìš´ë“œë¡œ 3ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ë§
    scheduler.add_job(
        lambda: task_queue.enqueue_task(
            run_daily_comprehensive_report_background,
            symbols=["^IXIC", "^GSPC", "^DJI"],
        ),
        "interval",
        minutes=3,
    )

    # íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘ë„ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(
        lambda: task_queue.enqueue_task(
            run_historical_data_collection_background,
            symbols=list(SYMBOL_PRICE_MAP.keys()),
            period="3mo",
        ),
        "interval",
        minutes=3,
    )

    # ê¸°ìˆ ì  ë¶„ì„ ë°°ì¹˜ë„ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(
        lambda: task_queue.enqueue_task(
            run_technical_analysis_batch_background,
            symbols=["^IXIC", "^GSPC", "^DJI", "AAPL", "MSFT"],
            analysis_types=["indicators", "signals"],
        ),
        "interval",
        minutes=3,
    )

    # ê¸°ì¡´ ê¸°ìˆ ì  ì§€í‘œ ëª¨ë‹ˆí„°ë§ ì‘ì—…ë“¤ë„ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(run_daily_index_analysis, "interval", minutes=3)
    scheduler.add_job(run_outcome_tracking_update, "interval", minutes=3)
    scheduler.add_job(initialize_recent_signals_tracking, "interval", minutes=3)

    # íŒ¨í„´ ë°œê²¬ ë° ë¶„ì„ë„ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(run_pattern_discovery, "interval", minutes=3)

    # ì¼ì¼ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ë„ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(run_daily_comprehensive_report, "interval", minutes=3)

    # ë©”ëª¨ë¦¬ ìµœì í™” ì‘ì—…ë„ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(run_memory_optimization_job, "interval", minutes=3)

    # ë¹„ë™ê¸° ê¸°ìˆ ì  ë¶„ì„ ì‘ì—…ë„ 3ë¶„ë§ˆë‹¤
    scheduler.add_job(run_async_technical_analysis_job, "interval", minutes=3)

    logger.info("parallel_scheduler_started")
    scheduler.start()


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
def run_memory_optimization_job():
    """
    ì •ê¸°ì ì¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‘ì—…
    - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
    - ìºì‹œ ì •ë¦¬
    - ë©”ëª¨ë¦¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§
    """
    logger.info("memory_optimization_job_started")

    try:
        # ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰
        result = optimize_memory(aggressive=False)

        logger.info(
            "memory_optimization_completed",
            memory_freed_mb=result.get("memory_freed_mb", 0),
            optimization_success=result.get("success", False),
        )

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìœ¼ë©´ ê³µê²©ì  ìµœì í™”
        if result.get("final_state", {}).get("memory_percent", 0) > 85:
            logger.warning("high_memory_usage_detected_running_aggressive_optimization")
            aggressive_result = optimize_memory(aggressive=True)

            logger.info(
                "aggressive_memory_optimization_completed",
                memory_freed_mb=aggressive_result.get("memory_freed_mb", 0),
            )

    except Exception as e:
        logger.error("memory_optimization_job_failed", error=str(e))
        raise SchedulerError(
            message=f"ë©”ëª¨ë¦¬ ìµœì í™” ì‘ì—… ì‹¤íŒ¨: {str(e)}",
            error_code=ErrorCode.TASK_EXECUTION_ERROR,
            details={"service": "memory_optimization", "error": str(e)},
        )


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
def run_async_technical_analysis_job():
    """
    ë¹„ë™ê¸° ê¸°ìˆ ì  ë¶„ì„ ì‘ì—…
    - ì£¼ìš” ì‹¬ë³¼ë“¤ì˜ ê¸°ìˆ ì  ì§€í‘œë¥¼ ë¹„ë™ê¸°ë¡œ ê³„ì‚°
    - ì„±ëŠ¥ í–¥ìƒëœ ë³‘ë ¬ ì²˜ë¦¬
    """
    logger.info("async_technical_analysis_job_started")

    try:

        async def run_async_analysis():
            # ì£¼ìš” ì‹¬ë³¼ë“¤ ì„ íƒ (ì „ì²´ ëŒ€ì‹  ì£¼ìš” ì§€ìˆ˜ë§Œ)
            major_symbols = [
                "^IXIC",
                "^GSPC",
                "^DJI",
                "AAPL",
                "GOOGL",
                "MSFT",
                "TSLA",
                "AMZN",
            ]

            technical_service = AsyncTechnicalIndicatorService(max_workers=3)
            price_service = AsyncPriceService(max_workers=4, max_concurrency=8)

            try:
                async with price_service:
                    # ê°€ê²© íˆìŠ¤í† ë¦¬ ì¡°íšŒ
                    price_histories = (
                        await price_service.fetch_multiple_histories_async(
                            major_symbols, period="1mo", interval="1d"
                        )
                    )

                    # DataFrame ë³€í™˜
                    import pandas as pd

                    symbol_data_map = {}
                    for symbol, history in price_histories.items():
                        if history and history.get("timestamps"):
                            df = pd.DataFrame(
                                {
                                    "timestamp": pd.to_datetime(
                                        history["timestamps"], unit="s"
                                    ),
                                    "open": history["open"],
                                    "high": history["high"],
                                    "low": history["low"],
                                    "close": history["close"],
                                    "volume": history["volume"],
                                }
                            ).dropna()

                            if not df.empty:
                                symbol_data_map[symbol] = df

                    # ë¹„ë™ê¸° ê¸°ìˆ ì  ë¶„ì„ ì‹¤í–‰
                    if symbol_data_map:
                        analysis_results = (
                            await technical_service.analyze_multiple_symbols_async(
                                symbol_data_map, batch_size=3
                            )
                        )

                        logger.info(
                            "async_technical_analysis_completed",
                            analyzed_symbols=len(analysis_results),
                            total_symbols=len(major_symbols),
                        )

                        return analysis_results
                    else:
                        logger.warning("no_valid_price_data_for_analysis")
                        return {}

            finally:
                # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                if hasattr(technical_service, "__del__"):
                    technical_service.__del__()

        # ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(run_async_analysis())
            logger.info(
                "async_technical_analysis_job_completed",
                results_count=len(result) if result else 0,
            )
        finally:
            loop.close()

    except Exception as e:
        logger.error("async_technical_analysis_job_failed", error=str(e))
        raise SchedulerError(
            message=f"ë¹„ë™ê¸° ê¸°ìˆ ì  ë¶„ì„ ì‘ì—… ì‹¤íŒ¨: {str(e)}",
            error_code=ErrorCode.TASK_EXECUTION_ERROR,
            details={"service": "async_technical_analysis", "error": str(e)},
        )


@measure_execution_time
@handle_scheduler_errors(reraise=False, return_on_error=None)
def run_daily_comprehensive_report():
    """
    ì¼ì¼ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ë° í…”ë ˆê·¸ë¨ ì „ì†¡
    - ë°±í…ŒìŠ¤íŒ… ì„±ê³¼ ë¶„ì„
    - íŒ¨í„´ ë¶„ì„ ê²°ê³¼
    - ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë¶„ì„
    - íˆ¬ì ì¸ì‚¬ì´íŠ¸ ì œê³µ
    """
    logger.info("daily_comprehensive_report_started")

    service = DailyComprehensiveReportService()
    # ì‹¤ì œ í…”ë ˆê·¸ë¨ ì „ì†¡ì„ í•˜ëŠ” ë©”ì„œë“œ í˜¸ì¶œ
    result = service.generate_daily_report()

    if result and result.get("status") == "error":
        raise SchedulerError(
            message=f"ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {result.get('error', 'Unknown error')}",
            error_code=ErrorCode.TASK_EXECUTION_ERROR,
            details={"service": "daily_comprehensive_report", "result": result},
        )
    else:
        logger.info(
            "daily_comprehensive_report_completed",
            message_length=result.get("report_length", 0),
        )


@memory_monitor()
def cleanup_scheduler_memory():
    """
    ìŠ¤ì¼€ì¤„ëŸ¬ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‘ì—…
    """
    try:
        # ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰
        optimize_memory()
        logger.info("scheduler_memory_cleanup_completed")
    except Exception as e:
        logger.error("scheduler_memory_cleanup_failed", error=str(e))


@memory_monitor()
def monitor_scheduler_performance():
    """
    ìŠ¤ì¼€ì¤„ëŸ¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """
    try:
        import psutil
        import os

        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        cpu_percent = process.cpu_percent()

        logger.info(
            "scheduler_performance_metrics",
            memory_mb=memory_info.rss / 1024 / 1024,
            cpu_percent=cpu_percent,
            threads=process.num_threads(),
        )
    except Exception as e:
        logger.error("scheduler_performance_monitoring_failed", error=str(e))

"""
ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ì„œë¹„ìŠ¤

Phase 3ì˜ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ, ë¨¸ì‹ ëŸ¬ë‹ê³¼ í†µê³„ ê¸°ë²•ì„ í™œìš©í•œ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. íŒ¨í„´ ìœ ì‚¬ë„ ë¶„ì„ - ì½”ì‚¬ì¸ ìœ ì‚¬ë„, ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­
2. í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ íŒ¨í„´ ê·¸ë£¹í™” - ìœ ì‚¬í•œ íŒ¨í„´ë“¤ì„ ìë™ìœ¼ë¡œ ê·¸ë£¹í™”
3. ì‹œê³„ì—´ ë¶„ì„ - íŒ¨í„´ì˜ ì‹œê°„ì  íŠ¹ì„± ë¶„ì„
4. ì˜ˆì¸¡ ëª¨ë¸ - íŒ¨í„´ ê¸°ë°˜ ê°€ê²© ì˜ˆì¸¡
5. ë™ì  íŒ¨í„´ ë°œê²¬ - ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒˆë¡œìš´ íŒ¨í„´ íƒì§€

ê³ ê¸‰ ë¶„ì„ ê¸°ë²•:
- Dynamic Time Warping (DTW) - ì‹œê³„ì—´ íŒ¨í„´ ë§¤ì¹­
- K-means í´ëŸ¬ìŠ¤í„°ë§ - íŒ¨í„´ ê·¸ë£¹í™”
- ìƒê´€ê´€ê³„ ë¶„ì„ - íŒ¨í„´ ê°„ ì—°ê´€ì„± ë¶„ì„
- í†µê³„ì  ìœ ì˜ì„± ê²€ì • - íŒ¨í„´ì˜ ì‹ ë¢°ë„ ê²€ì¦
"""

import numpy as np
import pandas as pd
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from app.common.infra.database.config.database_config import SessionLocal
from app.technical_analysis.infra.model.repository.signal_pattern_repository import (
    SignalPatternRepository,
)
from app.technical_analysis.infra.model.repository.technical_signal_repository import (
    TechnicalSignalRepository,
)
from app.technical_analysis.infra.model.entity.signal_patterns import SignalPattern
from app.technical_analysis.infra.model.entity.technical_signals import TechnicalSignal


class AdvancedPatternService:
    """
    ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤

    ë¨¸ì‹ ëŸ¬ë‹ê³¼ í†µê³„ ê¸°ë²•ì„ í™œìš©í•œ íŒ¨í„´ ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.session: Optional[Session] = None
        self.pattern_repository: Optional[SignalPatternRepository] = None
        self.signal_repository: Optional[TechnicalSignalRepository] = None

    def _get_session_and_repositories(self):
        """ì„¸ì…˜ê³¼ ë¦¬í¬ì§€í† ë¦¬ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)"""
        if not self.session:
            self.session = SessionLocal()
            self.pattern_repository = SignalPatternRepository(self.session)
            self.signal_repository = TechnicalSignalRepository(self.session)
        return self.session, self.pattern_repository, self.signal_repository

    # =================================================================
    # íŒ¨í„´ ìœ ì‚¬ë„ ë¶„ì„
    # =================================================================

    def calculate_pattern_similarity_matrix(
        self, symbol: str, pattern_type: str = "sequential"
    ) -> Dict[str, Any]:
        """
        íŒ¨í„´ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°

        Args:
            symbol: ë¶„ì„í•  ì‹¬ë³¼
            pattern_type: íŒ¨í„´ íƒ€ì…

        Returns:
            ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ì™€ ë¶„ì„ ê²°ê³¼
        """
        session, pattern_repo, signal_repo = self._get_session_and_repositories()

        try:
            # í•´ë‹¹ ì‹¬ë³¼ì˜ ëª¨ë“  íŒ¨í„´ ì¡°íšŒ
            patterns = (
                session.query(SignalPattern)
                .filter(
                    SignalPattern.symbol == symbol,
                    SignalPattern.pattern_type == pattern_type,
                )
                .all()
            )

            if len(patterns) < 2:
                return {
                    "message": "ìœ ì‚¬ë„ ë¶„ì„ì„ ìœ„í•œ íŒ¨í„´ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 2ê°œ í•„ìš”)",
                    "pattern_count": len(patterns),
                }

            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            similarity_matrix = []
            pattern_info = []

            for i, pattern1 in enumerate(patterns):
                pattern_info.append(
                    {
                        "id": pattern1.id,
                        "name": pattern1.pattern_name,
                        "start": pattern1.pattern_start.isoformat(),
                        "duration": (
                            float(pattern1.pattern_duration_hours)
                            if pattern1.pattern_duration_hours
                            else 0.0
                        ),
                    }
                )

                similarity_row = []
                for j, pattern2 in enumerate(patterns):
                    if i == j:
                        similarity = 1.0
                    else:
                        similarity = self._calculate_advanced_similarity(
                            pattern1, pattern2
                        )
                    similarity_row.append(round(similarity, 3))

                similarity_matrix.append(similarity_row)

            # ê°€ì¥ ìœ ì‚¬í•œ íŒ¨í„´ ìŒ ì°¾ê¸°
            most_similar_pairs = []
            for i in range(len(patterns)):
                for j in range(i + 1, len(patterns)):
                    similarity = similarity_matrix[i][j]
                    if similarity > 0.7:  # 70% ì´ìƒ ìœ ì‚¬í•œ ê²½ìš°
                        most_similar_pairs.append(
                            {
                                "pattern1": pattern_info[i],
                                "pattern2": pattern_info[j],
                                "similarity": similarity,
                            }
                        )

            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            most_similar_pairs.sort(key=lambda x: x["similarity"], reverse=True)

            return {
                "symbol": symbol,
                "pattern_type": pattern_type,
                "total_patterns": len(patterns),
                "similarity_matrix": similarity_matrix,
                "pattern_info": pattern_info,
                "most_similar_pairs": most_similar_pairs[:10],  # ìƒìœ„ 10ê°œë§Œ
                "analysis_timestamp": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            print(f"âŒ íŒ¨í„´ ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
        finally:
            session.close()

    def _calculate_advanced_similarity(
        self, pattern1: SignalPattern, pattern2: SignalPattern
    ) -> float:
        """
        ê³ ê¸‰ ìœ ì‚¬ë„ ê³„ì‚° (ì—¬ëŸ¬ ê¸°ë²• ì¡°í•©)

        Args:
            pattern1: ì²« ë²ˆì§¸ íŒ¨í„´
            pattern2: ë‘ ë²ˆì§¸ íŒ¨í„´

        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0~1)
        """
        similarity_scores = []

        # 1. íŒ¨í„´ ì´ë¦„ ìœ ì‚¬ë„ (ê°„ë‹¨í•œ ë¬¸ìì—´ ë§¤ì¹­)
        name_similarity = self._calculate_name_similarity(
            pattern1.pattern_name, pattern2.pattern_name
        )
        similarity_scores.append(name_similarity * 0.3)  # 30% ê°€ì¤‘ì¹˜

        # 2. ì§€ì† ì‹œê°„ ìœ ì‚¬ë„
        duration_similarity = self._calculate_duration_similarity(
            pattern1.pattern_duration_hours, pattern2.pattern_duration_hours
        )
        similarity_scores.append(duration_similarity * 0.2)  # 20% ê°€ì¤‘ì¹˜

        # 3. ì‹œì¥ ìƒí™© ìœ ì‚¬ë„
        market_similarity = self._calculate_market_condition_similarity(
            pattern1.market_condition, pattern2.market_condition
        )
        similarity_scores.append(market_similarity * 0.2)  # 20% ê°€ì¤‘ì¹˜

        # 4. ì‹œê°„ì  ê·¼ì ‘ì„± (ë°œìƒ ì‹œì ì´ ë¹„ìŠ·í•œì§€)
        temporal_similarity = self._calculate_temporal_similarity(
            pattern1.pattern_start, pattern2.pattern_start
        )
        similarity_scores.append(temporal_similarity * 0.3)  # 30% ê°€ì¤‘ì¹˜

        # ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚°
        total_similarity = sum(similarity_scores)
        return min(max(total_similarity, 0.0), 1.0)  # 0~1 ë²”ìœ„ë¡œ ì œí•œ

    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """íŒ¨í„´ ì´ë¦„ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not name1 or not name2:
            return 0.0

        # ê°„ë‹¨í•œ ê³µí†µ ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„
        words1 = set(name1.lower().split("_"))
        words2 = set(name2.lower().split("_"))

        if not words1 or not words2:
            return 0.0

        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))

        return intersection / union if union > 0 else 0.0

    def _calculate_duration_similarity(
        self, duration1: Optional[float], duration2: Optional[float]
    ) -> float:
        """ì§€ì† ì‹œê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
        if duration1 is None or duration2 is None:
            return 0.5  # ì¤‘ë¦½ê°’

        d1, d2 = float(duration1), float(duration2)
        if d1 == 0 and d2 == 0:
            return 1.0

        max_duration = max(d1, d2)
        if max_duration == 0:
            return 1.0

        # ì°¨ì´ê°€ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ë„ ë†’ìŒ
        diff_ratio = abs(d1 - d2) / max_duration
        return max(0.0, 1.0 - diff_ratio)

    def _calculate_market_condition_similarity(
        self, condition1: Optional[str], condition2: Optional[str]
    ) -> float:
        """ì‹œì¥ ìƒí™© ìœ ì‚¬ë„ ê³„ì‚°"""
        if not condition1 or not condition2:
            return 0.5  # ì¤‘ë¦½ê°’

        if condition1.lower() == condition2.lower():
            return 1.0

        # ìœ ì‚¬í•œ ì‹œì¥ ìƒí™© ê·¸ë£¹í™”
        bullish_conditions = ["bullish", "bull", "up", "positive"]
        bearish_conditions = ["bearish", "bear", "down", "negative"]

        c1_lower = condition1.lower()
        c2_lower = condition2.lower()

        c1_bullish = any(term in c1_lower for term in bullish_conditions)
        c2_bullish = any(term in c2_lower for term in bullish_conditions)
        c1_bearish = any(term in c1_lower for term in bearish_conditions)
        c2_bearish = any(term in c2_lower for term in bearish_conditions)

        if (c1_bullish and c2_bullish) or (c1_bearish and c2_bearish):
            return 0.7  # ê°™ì€ ë°©í–¥ì„±
        elif (c1_bullish and c2_bearish) or (c1_bearish and c2_bullish):
            return 0.1  # ë°˜ëŒ€ ë°©í–¥ì„±
        else:
            return 0.5  # ì¤‘ë¦½

    def _calculate_temporal_similarity(self, time1: datetime, time2: datetime) -> float:
        """ì‹œê°„ì  ê·¼ì ‘ì„± ê³„ì‚°"""
        if not time1 or not time2:
            return 0.5

        # ì‹œê°„ ì°¨ì´ ê³„ì‚° (ì¼ ë‹¨ìœ„)
        time_diff = abs((time1 - time2).days)

        # 30ì¼ ì´ë‚´ë©´ ë†’ì€ ìœ ì‚¬ë„, ê·¸ ì´í›„ë¡œëŠ” ê°ì†Œ
        if time_diff <= 7:
            return 1.0
        elif time_diff <= 30:
            return 1.0 - (time_diff - 7) / 23 * 0.5  # 7ì¼ í›„ë¶€í„° ì„ í˜• ê°ì†Œ
        else:
            return max(0.1, 1.0 - time_diff / 365)  # 1ë…„ ê¸°ì¤€ìœ¼ë¡œ ê°ì†Œ

    # =================================================================
    # íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§
    # =================================================================

    def cluster_patterns(
        self, symbol: str, n_clusters: int = 5, min_patterns: int = 10
    ) -> Dict[str, Any]:
        """
        íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ê·¸ë£¹í™”

        Args:
            symbol: ë¶„ì„í•  ì‹¬ë³¼
            n_clusters: ìƒì„±í•  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
            min_patterns: í´ëŸ¬ìŠ¤í„°ë§ì— í•„ìš”í•œ ìµœì†Œ íŒ¨í„´ ê°œìˆ˜

        Returns:
            í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        session, pattern_repo, signal_repo = self._get_session_and_repositories()

        try:
            print(f"ğŸ” {symbol} íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (í´ëŸ¬ìŠ¤í„°: {n_clusters}ê°œ)")

            # í•´ë‹¹ ì‹¬ë³¼ì˜ ëª¨ë“  íŒ¨í„´ ì¡°íšŒ
            patterns = (
                session.query(SignalPattern)
                .filter(SignalPattern.symbol == symbol)
                .all()
            )

            if len(patterns) < min_patterns:
                return {
                    "message": f"í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ íŒ¨í„´ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ {min_patterns}ê°œ í•„ìš”, í˜„ì¬ {len(patterns)}ê°œ)",
                    "pattern_count": len(patterns),
                }

            # íŒ¨í„´ íŠ¹ì„± ë²¡í„° ìƒì„±
            feature_vectors = []
            pattern_info = []

            for pattern in patterns:
                # ê° íŒ¨í„´ì„ ìˆ˜ì¹˜ ë²¡í„°ë¡œ ë³€í™˜
                features = self._extract_pattern_features(pattern)
                if features:
                    feature_vectors.append(features)
                    pattern_info.append(
                        {
                            "id": pattern.id,
                            "name": pattern.pattern_name,
                            "start": pattern.pattern_start.isoformat(),
                            "duration": (
                                float(pattern.pattern_duration_hours)
                                if pattern.pattern_duration_hours
                                else 0.0
                            ),
                            "market_condition": pattern.market_condition,
                        }
                    )

            if len(feature_vectors) < n_clusters:
                n_clusters = max(2, len(feature_vectors) // 2)  # í´ëŸ¬ìŠ¤í„° ìˆ˜ ì¡°ì •

            # ê°„ë‹¨í•œ K-means í´ëŸ¬ìŠ¤í„°ë§ êµ¬í˜„
            clusters = self._simple_kmeans_clustering(feature_vectors, n_clusters)

            # í´ëŸ¬ìŠ¤í„°ë³„ ë¶„ì„
            cluster_analysis = []
            for i, cluster_indices in enumerate(clusters):
                if not cluster_indices:
                    continue

                cluster_patterns = [pattern_info[idx] for idx in cluster_indices]
                cluster_features = [feature_vectors[idx] for idx in cluster_indices]

                # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„
                cluster_stats = self._analyze_cluster_characteristics(
                    cluster_patterns, cluster_features
                )

                cluster_analysis.append(
                    {
                        "cluster_id": i,
                        "cluster_name": self._generate_cluster_name(cluster_stats),
                        "pattern_count": len(cluster_patterns),
                        "patterns": cluster_patterns[:5],  # ìƒìœ„ 5ê°œë§Œ
                        "characteristics": cluster_stats,
                        "avg_success_rate": cluster_stats.get("avg_success_rate", 0.5),
                    }
                )

            return {
                "symbol": symbol,
                "total_patterns": len(patterns),
                "clustered_patterns": len(feature_vectors),
                "n_clusters": len(cluster_analysis),
                "clusters": cluster_analysis,
                "analysis_quality": self._assess_clustering_quality(
                    clusters, feature_vectors
                ),
                "analysis_timestamp": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            print(f"âŒ íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
        finally:
            session.close()

    def _extract_pattern_features(
        self, pattern: SignalPattern
    ) -> Optional[List[float]]:
        """íŒ¨í„´ì—ì„œ ìˆ˜ì¹˜ íŠ¹ì„± ì¶”ì¶œ"""
        try:
            features = []

            # 1. ì§€ì† ì‹œê°„ (ì •ê·œí™”)
            duration = (
                float(pattern.pattern_duration_hours)
                if pattern.pattern_duration_hours
                else 0.0
            )
            features.append(min(duration / 24.0, 10.0))  # ìµœëŒ€ 10ì¼ë¡œ ì •ê·œí™”

            # 2. íŒ¨í„´ ì´ë¦„ ê¸°ë°˜ íŠ¹ì„±
            name = pattern.pattern_name.lower() if pattern.pattern_name else ""

            # ì‹ í˜¸ íƒ€ì…ë³„ ì›-í•« ì¸ì½”ë”©
            signal_types = ["rsi", "ma", "bb", "macd", "cross", "volume"]
            for signal_type in signal_types:
                features.append(1.0 if signal_type in name else 0.0)

            # ë°©í–¥ì„± (ìƒìŠ¹/í•˜ë½)
            bullish_terms = ["up", "bull", "breakout", "golden", "oversold"]
            bearish_terms = ["down", "bear", "breakdown", "dead", "overbought"]

            bullish_score = sum(1 for term in bullish_terms if term in name)
            bearish_score = sum(1 for term in bearish_terms if term in name)

            features.append(bullish_score / max(len(bullish_terms), 1))
            features.append(bearish_score / max(len(bearish_terms), 1))

            # 3. ì‹œì¥ ìƒí™©
            market_condition = (
                pattern.market_condition.lower() if pattern.market_condition else ""
            )
            features.append(1.0 if "bull" in market_condition else 0.0)
            features.append(1.0 if "bear" in market_condition else 0.0)

            # 4. ì‹œê°„ì  íŠ¹ì„± (ìš”ì¼, ì›”)
            if pattern.pattern_start:
                features.append(
                    pattern.pattern_start.weekday() / 6.0
                )  # 0-6ì„ 0-1ë¡œ ì •ê·œí™”
                features.append(
                    pattern.pattern_start.month / 12.0
                )  # 1-12ë¥¼ 0-1ë¡œ ì •ê·œí™”
            else:
                features.extend([0.5, 0.5])  # ì¤‘ë¦½ê°’

            return features

        except Exception as e:
            print(f"âš ï¸ íŒ¨í„´ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _simple_kmeans_clustering(
        self, feature_vectors: List[List[float]], n_clusters: int
    ) -> List[List[int]]:
        """ê°„ë‹¨í•œ K-means í´ëŸ¬ìŠ¤í„°ë§ êµ¬í˜„"""
        if not feature_vectors or n_clusters <= 0:
            return []

        n_features = len(feature_vectors[0])
        n_points = len(feature_vectors)

        # ì´ˆê¸° ì¤‘ì‹¬ì  ëœë¤ ì„ íƒ
        import random

        random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´

        centroids = []
        for _ in range(n_clusters):
            centroid = [random.random() for _ in range(n_features)]
            centroids.append(centroid)

        # K-means ë°˜ë³µ
        max_iterations = 50
        for iteration in range(max_iterations):
            # ê° ì ì„ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì— í• ë‹¹
            clusters = [[] for _ in range(n_clusters)]

            for i, point in enumerate(feature_vectors):
                distances = []
                for centroid in centroids:
                    # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
                    distance = sum((p - c) ** 2 for p, c in zip(point, centroid)) ** 0.5
                    distances.append(distance)

                closest_cluster = distances.index(min(distances))
                clusters[closest_cluster].append(i)

            # ìƒˆë¡œìš´ ì¤‘ì‹¬ì  ê³„ì‚°
            new_centroids = []
            for cluster_indices in clusters:
                if cluster_indices:
                    # í´ëŸ¬ìŠ¤í„° ë‚´ ì ë“¤ì˜ í‰ê· 
                    cluster_points = [feature_vectors[i] for i in cluster_indices]
                    centroid = []
                    for j in range(n_features):
                        avg = sum(point[j] for point in cluster_points) / len(
                            cluster_points
                        )
                        centroid.append(avg)
                    new_centroids.append(centroid)
                else:
                    # ë¹ˆ í´ëŸ¬ìŠ¤í„°ëŠ” ëœë¤ ì¤‘ì‹¬ì  ìœ ì§€
                    new_centroids.append(centroids[len(new_centroids)])

            # ìˆ˜ë ´ ì²´í¬ (ì¤‘ì‹¬ì  ë³€í™”ê°€ ì‘ìœ¼ë©´ ì¢…ë£Œ)
            converged = True
            for old, new in zip(centroids, new_centroids):
                distance = sum((o - n) ** 2 for o, n in zip(old, new)) ** 0.5
                if distance > 0.001:  # ì„ê³„ê°’
                    converged = False
                    break

            centroids = new_centroids

            if converged:
                break

        return clusters

    def _analyze_cluster_characteristics(
        self, cluster_patterns: List[Dict], cluster_features: List[List[float]]
    ) -> Dict[str, Any]:
        """í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„"""
        if not cluster_patterns:
            return {}

        # í‰ê·  ì§€ì† ì‹œê°„
        durations = [p["duration"] for p in cluster_patterns if p["duration"] > 0]
        avg_duration = sum(durations) / len(durations) if durations else 0

        # ì‹œì¥ ìƒí™© ë¶„í¬
        market_conditions = [
            p["market_condition"] for p in cluster_patterns if p["market_condition"]
        ]
        market_dist = {}
        for condition in market_conditions:
            market_dist[condition] = market_dist.get(condition, 0) + 1

        # íŒ¨í„´ ì´ë¦„ ë¶„ì„
        pattern_names = [p["name"] for p in cluster_patterns if p["name"]]
        common_terms = self._find_common_terms(pattern_names)

        # ì„±ê³µë¥  ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        success_rate = self._estimate_cluster_success_rate(
            cluster_patterns, cluster_features
        )

        return {
            "avg_duration_hours": avg_duration,
            "market_condition_distribution": market_dist,
            "common_terms": common_terms,
            "avg_success_rate": success_rate,
            "pattern_count": len(cluster_patterns),
        }

    def _find_common_terms(self, pattern_names: List[str]) -> List[str]:
        """íŒ¨í„´ ì´ë¦„ì—ì„œ ê³µí†µ ìš©ì–´ ì°¾ê¸°"""
        if not pattern_names:
            return []

        # ëª¨ë“  íŒ¨í„´ ì´ë¦„ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
        all_terms = []
        for name in pattern_names:
            terms = name.lower().split("_")
            all_terms.extend(terms)

        # ë¹ˆë„ ê³„ì‚°
        term_counts = {}
        for term in all_terms:
            term_counts[term] = term_counts.get(term, 0) + 1

        # ë¹ˆë„ê°€ ë†’ì€ ìƒìœ„ 3ê°œ ìš©ì–´ ë°˜í™˜
        sorted_terms = sorted(term_counts.items(), key=lambda x: x[1], reverse=True)
        return [term for term, count in sorted_terms[:3] if count > 1]

    def _estimate_cluster_success_rate(
        self, cluster_patterns: List[Dict], cluster_features: List[List[float]]
    ) -> float:
        """í´ëŸ¬ìŠ¤í„° ì„±ê³µë¥  ì¶”ì • (íœ´ë¦¬ìŠ¤í‹±)"""
        if not cluster_features:
            return 0.5

        # íŠ¹ì„± ê¸°ë°˜ ì„±ê³µë¥  ì¶”ì •
        avg_features = []
        n_features = len(cluster_features[0])

        for i in range(n_features):
            avg = sum(features[i] for features in cluster_features) / len(
                cluster_features
            )
            avg_features.append(avg)

        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: íŠ¹ì • íŠ¹ì„± ì¡°í•©ì´ ë†’ì€ ì„±ê³µë¥ ì„ ê°€ì§„ë‹¤ê³  ê°€ì •
        success_score = 0.5  # ê¸°ë³¸ê°’

        # ìƒìŠ¹ íŒ¨í„´ íŠ¹ì„±ì´ ê°•í•˜ë©´ ì„±ê³µë¥  ì¦ê°€
        if len(avg_features) > 7:  # bullish_score ì¸ë±ìŠ¤
            success_score += avg_features[7] * 0.2

        # ì§€ì† ì‹œê°„ì´ ì ì ˆí•˜ë©´ ì„±ê³µë¥  ì¦ê°€ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šê²Œ)
        if len(avg_features) > 0:
            duration_score = avg_features[0]
            if 0.1 <= duration_score <= 0.5:  # ì ì ˆí•œ ì§€ì† ì‹œê°„
                success_score += 0.1

        return min(max(success_score, 0.1), 0.9)  # 0.1~0.9 ë²”ìœ„ë¡œ ì œí•œ

    def _generate_cluster_name(self, cluster_stats: Dict[str, Any]) -> str:
        """í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„±"""
        common_terms = cluster_stats.get("common_terms", [])
        avg_success_rate = cluster_stats.get("avg_success_rate", 0.5)

        if common_terms:
            base_name = "_".join(common_terms[:2])
        else:
            base_name = "mixed_pattern"

        # ì„±ê³µë¥ ì— ë”°ë¥¸ ì ‘ë‘ì‚¬
        if avg_success_rate >= 0.7:
            prefix = "high_success"
        elif avg_success_rate >= 0.6:
            prefix = "good"
        elif avg_success_rate <= 0.4:
            prefix = "low_success"
        else:
            prefix = "moderate"

        return f"{prefix}_{base_name}_cluster"

    def _assess_clustering_quality(
        self, clusters: List[List[int]], feature_vectors: List[List[float]]
    ) -> str:
        """í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€"""
        if not clusters or not feature_vectors:
            return "Poor"

        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ê· í˜• ì²´í¬
        cluster_sizes = [len(cluster) for cluster in clusters if cluster]
        if not cluster_sizes:
            return "Poor"

        avg_size = sum(cluster_sizes) / len(cluster_sizes)
        size_variance = sum((size - avg_size) ** 2 for size in cluster_sizes) / len(
            cluster_sizes
        )

        # í´ëŸ¬ìŠ¤í„° ê°„ ë¶„ë¦¬ë„ ì²´í¬ (ê°„ë‹¨í•œ ë²„ì „)
        non_empty_clusters = [cluster for cluster in clusters if cluster]

        if len(non_empty_clusters) < 2:
            return "Poor"
        elif size_variance < avg_size * 0.5:  # í¬ê¸°ê°€ ë¹„êµì  ê· ë“±
            return "Good"
        else:
            return "Fair"

    # =================================================================
    # ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„
    # =================================================================

    def analyze_temporal_patterns(
        self, symbol: str, timeframe: str = "1h", days: int = 30
    ) -> Dict[str, Any]:
        """
        ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„

        Args:
            symbol: ë¶„ì„í•  ì‹¬ë³¼
            timeframe: ì‹œê°„ëŒ€
            days: ë¶„ì„ ê¸°ê°„

        Returns:
            ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        session, pattern_repo, signal_repo = self._get_session_and_repositories()

        try:
            print(f"ğŸ“Š {symbol} ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹œì‘ ({days}ì¼ê°„)")

            # ì§€ì •ëœ ê¸°ê°„ì˜ íŒ¨í„´ ì¡°íšŒ
            end_date = datetime.utcnow()
            start_date = end_date - timedelta(days=days)

            patterns = (
                session.query(SignalPattern)
                .filter(
                    SignalPattern.symbol == symbol,
                    SignalPattern.pattern_start >= start_date,
                    SignalPattern.pattern_start <= end_date,
                )
                .all()
            )

            if not patterns:
                return {
                    "message": f"ë¶„ì„ ê¸°ê°„ ë‚´ íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤ ({days}ì¼ê°„)",
                    "pattern_count": 0,
                }

            # ì‹œê°„ë³„ ë¶„ì„
            temporal_analysis = {
                "hourly_distribution": self._analyze_hourly_distribution(patterns),
                "daily_distribution": self._analyze_daily_distribution(patterns),
                "weekly_patterns": self._analyze_weekly_patterns(patterns),
                "seasonal_effects": self._analyze_seasonal_effects(patterns),
                "frequency_analysis": self._analyze_pattern_frequency(patterns),
            }

            return {
                "symbol": symbol,
                "timeframe": timeframe,
                "analysis_period_days": days,
                "total_patterns": len(patterns),
                "temporal_analysis": temporal_analysis,
                "insights": self._generate_temporal_insights(temporal_analysis),
                "analysis_timestamp": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            print(f"âŒ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
        finally:
            session.close()

    def _analyze_hourly_distribution(
        self, patterns: List[SignalPattern]
    ) -> Dict[str, Any]:
        """ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„í¬ ë¶„ì„"""
        hourly_counts = {}
        for pattern in patterns:
            if pattern.pattern_start:
                hour = pattern.pattern_start.hour
                hourly_counts[hour] = hourly_counts.get(hour, 0) + 1

        # ê°€ì¥ í™œë°œí•œ ì‹œê°„ëŒ€ ì°¾ê¸°
        if hourly_counts:
            peak_hour = max(hourly_counts.items(), key=lambda x: x[1])
            return {
                "distribution": hourly_counts,
                "peak_hour": peak_hour[0],
                "peak_count": peak_hour[1],
                "total_patterns": len(patterns),
            }
        return {"distribution": {}, "peak_hour": None, "peak_count": 0}

    def _analyze_daily_distribution(
        self, patterns: List[SignalPattern]
    ) -> Dict[str, Any]:
        """ìš”ì¼ë³„ íŒ¨í„´ ë¶„í¬ ë¶„ì„"""
        daily_counts = {}
        weekdays = [
            "Monday",
            "Tuesday",
            "Wednesday",
            "Thursday",
            "Friday",
            "Saturday",
            "Sunday",
        ]

        for pattern in patterns:
            if pattern.pattern_start:
                weekday = weekdays[pattern.pattern_start.weekday()]
                daily_counts[weekday] = daily_counts.get(weekday, 0) + 1

        if daily_counts:
            peak_day = max(daily_counts.items(), key=lambda x: x[1])
            return {
                "distribution": daily_counts,
                "peak_day": peak_day[0],
                "peak_count": peak_day[1],
            }
        return {"distribution": {}, "peak_day": None, "peak_count": 0}

    def _analyze_weekly_patterns(self, patterns: List[SignalPattern]) -> Dict[str, Any]:
        """ì£¼ê°„ íŒ¨í„´ ë¶„ì„"""
        # ì£¼ì˜ ì‹œì‘/ì¤‘ê°„/ë êµ¬ë¶„
        week_periods = {"week_start": 0, "week_middle": 0, "week_end": 0}

        for pattern in patterns:
            if pattern.pattern_start:
                weekday = pattern.pattern_start.weekday()
                if weekday in [0, 1]:  # ì›”, í™”
                    week_periods["week_start"] += 1
                elif weekday in [2, 3]:  # ìˆ˜, ëª©
                    week_periods["week_middle"] += 1
                else:  # ê¸ˆ, í† , ì¼
                    week_periods["week_end"] += 1

        return week_periods

    def _analyze_seasonal_effects(
        self, patterns: List[SignalPattern]
    ) -> Dict[str, Any]:
        """ê³„ì ˆì„± íš¨ê³¼ ë¶„ì„"""
        monthly_counts = {}

        for pattern in patterns:
            if pattern.pattern_start:
                month = pattern.pattern_start.month
                monthly_counts[month] = monthly_counts.get(month, 0) + 1

        # ê³„ì ˆë³„ ê·¸ë£¹í™”
        seasons = {
            "Spring": sum(monthly_counts.get(m, 0) for m in [3, 4, 5]),
            "Summer": sum(monthly_counts.get(m, 0) for m in [6, 7, 8]),
            "Fall": sum(monthly_counts.get(m, 0) for m in [9, 10, 11]),
            "Winter": sum(monthly_counts.get(m, 0) for m in [12, 1, 2]),
        }

        return {
            "monthly_distribution": monthly_counts,
            "seasonal_distribution": seasons,
        }

    def _analyze_pattern_frequency(
        self, patterns: List[SignalPattern]
    ) -> Dict[str, Any]:
        """íŒ¨í„´ ë°œìƒ ë¹ˆë„ ë¶„ì„"""
        if not patterns:
            return {}

        # íŒ¨í„´ ê°„ ì‹œê°„ ê°„ê²© ê³„ì‚°
        sorted_patterns = sorted(
            patterns, key=lambda p: p.pattern_start if p.pattern_start else datetime.min
        )

        intervals = []
        for i in range(1, len(sorted_patterns)):
            if (
                sorted_patterns[i].pattern_start
                and sorted_patterns[i - 1].pattern_start
            ):
                interval = (
                    sorted_patterns[i].pattern_start
                    - sorted_patterns[i - 1].pattern_start
                ).total_seconds() / 3600  # ì‹œê°„ ë‹¨ìœ„
                intervals.append(interval)

        if intervals:
            avg_interval = sum(intervals) / len(intervals)
            return {
                "average_interval_hours": avg_interval,
                "min_interval_hours": min(intervals),
                "max_interval_hours": max(intervals),
                "total_intervals": len(intervals),
            }

        return {"average_interval_hours": 0, "total_intervals": 0}

    def _generate_temporal_insights(
        self, temporal_analysis: Dict[str, Any]
    ) -> List[str]:
        """ì‹œê³„ì—´ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []

        # ì‹œê°„ëŒ€ë³„ ì¸ì‚¬ì´íŠ¸
        hourly = temporal_analysis.get("hourly_distribution", {})
        if hourly.get("peak_hour") is not None:
            peak_hour = hourly["peak_hour"]
            if 9 <= peak_hour <= 16:
                insights.append(f"ì¥ì¤‘ ì‹œê°„ëŒ€({peak_hour}ì‹œ)ì— íŒ¨í„´ ë°œìƒ ë¹ˆë„ ìµœê³ ")
            elif peak_hour >= 21 or peak_hour <= 6:
                insights.append(f"ì•¼ê°„ ì‹œê°„ëŒ€({peak_hour}ì‹œ)ì— íŒ¨í„´ ë°œìƒ ë¹ˆë„ ìµœê³ ")

        # ìš”ì¼ë³„ ì¸ì‚¬ì´íŠ¸
        daily = temporal_analysis.get("daily_distribution", {})
        if daily.get("peak_day"):
            peak_day = daily["peak_day"]
            if peak_day == "Monday":
                insights.append("ì›”ìš”ì¼ íš¨ê³¼: ì£¼ ì‹œì‘ì— íŒ¨í„´ ë°œìƒ ì¦ê°€")
            elif peak_day == "Friday":
                insights.append("ê¸ˆìš”ì¼ íš¨ê³¼: ì£¼ ë§ˆê°ì— íŒ¨í„´ ë°œìƒ ì¦ê°€")

        # ê³„ì ˆì„± ì¸ì‚¬ì´íŠ¸
        seasonal = temporal_analysis.get("seasonal_effects", {})
        if seasonal.get("seasonal_distribution"):
            seasons = seasonal["seasonal_distribution"]
            max_season = max(seasons.items(), key=lambda x: x[1])
            if max_season[1] > 0:
                insights.append(f"{max_season[0]} ê³„ì ˆì— íŒ¨í„´ ë°œìƒ ë¹ˆë„ ìµœê³ ")

        # ë¹ˆë„ ì¸ì‚¬ì´íŠ¸
        frequency = temporal_analysis.get("frequency_analysis", {})
        avg_interval = frequency.get("average_interval_hours", 0)
        if avg_interval > 0:
            if avg_interval < 24:
                insights.append(f"í‰ê·  {avg_interval:.1f}ì‹œê°„ë§ˆë‹¤ íŒ¨í„´ ë°œìƒ")
            else:
                insights.append(f"í‰ê·  {avg_interval/24:.1f}ì¼ë§ˆë‹¤ íŒ¨í„´ ë°œìƒ")

        return insights if insights else ["ë¶„ì„í•  ìˆ˜ ìˆëŠ” ì‹œê³„ì—´ íŒ¨í„´ì´ ë¶€ì¡±í•©ë‹ˆë‹¤"]

    def __del__(self):
        """ì†Œë©¸ì - ì„¸ì…˜ ì •ë¦¬"""
        if self.session:
            self.session.close()

        # 1. ê¸°ë³¸ ì†ì„± ìœ ì‚¬ë„ (30%)
        basic_similarity = 0.0
        if pattern1.symbol == pattern2.symbol:
            basic_similarity += 0.4
        if pattern1.timeframe == pattern2.timeframe:
            basic_similarity += 0.3
        if pattern1.market_condition == pattern2.market_condition:
            basic_similarity += 0.3

        similarity_scores.append(("basic", basic_similarity, 0.3))

        # 2. ì‹œê°„ì  ìœ ì‚¬ë„ (20%)
        temporal_similarity = self._calculate_temporal_similarity(pattern1, pattern2)
        similarity_scores.append(("temporal", temporal_similarity, 0.2))

        # 3. ì„±ê³¼ ìœ ì‚¬ë„ (30%)
        performance_similarity = self._calculate_performance_similarity(
            pattern1, pattern2
        )
        similarity_scores.append(("performance", performance_similarity, 0.3))

        # 4. êµ¬ì¡°ì  ìœ ì‚¬ë„ (20%)
        structural_similarity = self._calculate_structural_similarity(
            pattern1, pattern2
        )
        similarity_scores.append(("structural", structural_similarity, 0.2))

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_similarity = sum(score * weight for _, score, weight in similarity_scores)

        return min(total_similarity, 1.0)

    def _calculate_temporal_similarity(
        self, pattern1: SignalPattern, pattern2: SignalPattern
    ) -> float:
        """ì‹œê°„ì  íŠ¹ì„± ìœ ì‚¬ë„ ê³„ì‚°"""
        if not pattern1.pattern_duration_hours or not pattern2.pattern_duration_hours:
            return 0.0

        duration1 = float(pattern1.pattern_duration_hours)
        duration2 = float(pattern2.pattern_duration_hours)

        # ì§€ì† ì‹œê°„ ìœ ì‚¬ë„
        max_duration = max(duration1, duration2)
        if max_duration == 0:
            return 1.0

        duration_diff = abs(duration1 - duration2)
        duration_similarity = 1.0 - (duration_diff / max_duration)

        return max(duration_similarity, 0.0)

    def _calculate_performance_similarity(
        self, pattern1: SignalPattern, pattern2: SignalPattern
    ) -> float:
        """ì„±ê³¼ ìœ ì‚¬ë„ ê³„ì‚°"""
        similarity = 0.0
        comparisons = 0

        # ê° ì‹œê°„ëŒ€ë³„ ê²°ê³¼ ë¹„êµ
        for timeframe in ["1h", "1d", "1w"]:
            outcome1 = getattr(pattern1, f"pattern_outcome_{timeframe}")
            outcome2 = getattr(pattern2, f"pattern_outcome_{timeframe}")
            success1 = getattr(pattern1, f"is_successful_{timeframe}")
            success2 = getattr(pattern2, f"is_successful_{timeframe}")

            if outcome1 is not None and outcome2 is not None:
                # ìˆ˜ìµë¥  ë°©í–¥ ìœ ì‚¬ë„
                if (outcome1 > 0 and outcome2 > 0) or (outcome1 < 0 and outcome2 < 0):
                    similarity += 0.3

                # ìˆ˜ìµë¥  í¬ê¸° ìœ ì‚¬ë„
                max_outcome = max(abs(outcome1), abs(outcome2))
                if max_outcome > 0:
                    outcome_diff = abs(abs(outcome1) - abs(outcome2))
                    outcome_similarity = 1.0 - (outcome_diff / max_outcome)
                    similarity += outcome_similarity * 0.2

                comparisons += 1

            if success1 is not None and success2 is not None:
                if success1 == success2:
                    similarity += 0.2
                comparisons += 1

        return similarity / comparisons if comparisons > 0 else 0.0

    def _calculate_structural_similarity(
        self, pattern1: SignalPattern, pattern2: SignalPattern
    ) -> float:
        """êµ¬ì¡°ì  ìœ ì‚¬ë„ ê³„ì‚° (ì‹ í˜¸ êµ¬ì„± ë¹„êµ)"""
        # íŒ¨í„´ì„ êµ¬ì„±í•˜ëŠ” ì‹ í˜¸ IDë“¤ ì¶”ì¶œ
        signals1 = pattern1.get_signal_sequence()
        signals2 = pattern2.get_signal_sequence()

        if not signals1 or not signals2:
            return 0.0

        # ê¸¸ì´ ìœ ì‚¬ë„
        len_similarity = 1.0 - abs(len(signals1) - len(signals2)) / max(
            len(signals1), len(signals2)
        )

        # ì‹ í˜¸ íƒ€ì… ìœ ì‚¬ë„ (ì‹¤ì œ ì‹ í˜¸ íƒ€ì… ë¹„êµëŠ” ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨íˆ êµ¬í˜„)
        # ì‹¤ì œë¡œëŠ” ê° ì‹ í˜¸ì˜ íƒ€ì…ì„ ì¡°íšŒí•´ì„œ ë¹„êµí•´ì•¼ í•¨
        type_similarity = 0.5  # ê¸°ë³¸ê°’

        return (len_similarity + type_similarity) / 2

    # =================================================================
    # í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ íŒ¨í„´ ê·¸ë£¹í™”
    # =================================================================

    def cluster_patterns(
        self, symbol: str, n_clusters: int = 5, min_patterns: int = 10
    ) -> Dict[str, Any]:
        """
        íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ê·¸ë£¹í™”

        Args:
            symbol: ë¶„ì„í•  ì‹¬ë³¼
            n_clusters: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
            min_patterns: ìµœì†Œ íŒ¨í„´ ê°œìˆ˜

        Returns:
            í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        session, pattern_repo, signal_repo = self._get_session_and_repositories()

        try:
            # íŒ¨í„´ ë°ì´í„° ìˆ˜ì§‘
            patterns = (
                session.query(SignalPattern)
                .filter(SignalPattern.symbol == symbol)
                .all()
            )

            if len(patterns) < min_patterns:
                return {
                    "message": f"í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ íŒ¨í„´ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ {min_patterns}ê°œ í•„ìš”)",
                    "pattern_count": len(patterns),
                }

            # íŠ¹ì„± ë²¡í„° ìƒì„±
            feature_vectors = []
            pattern_info = []

            for pattern in patterns:
                features = self._extract_pattern_features(pattern)
                if features is not None:
                    feature_vectors.append(features)
                    pattern_info.append(
                        {
                            "id": pattern.id,
                            "name": pattern.pattern_name,
                            "start": pattern.pattern_start.isoformat(),
                            "duration": (
                                float(pattern.pattern_duration_hours)
                                if pattern.pattern_duration_hours
                                else 0.0
                            ),
                        }
                    )

            if len(feature_vectors) < n_clusters:
                n_clusters = len(feature_vectors)

            # ê°„ë‹¨í•œ K-means í´ëŸ¬ìŠ¤í„°ë§ êµ¬í˜„ (numpy ê¸°ë°˜)
            clusters = self._simple_kmeans_clustering(feature_vectors, n_clusters)

            # í´ëŸ¬ìŠ¤í„°ë³„ íŒ¨í„´ ê·¸ë£¹í™”
            clustered_patterns = {}
            for i, cluster_id in enumerate(clusters):
                if cluster_id not in clustered_patterns:
                    clustered_patterns[cluster_id] = []
                clustered_patterns[cluster_id].append(pattern_info[i])

            # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„
            cluster_analysis = {}
            for cluster_id, cluster_patterns in clustered_patterns.items():
                cluster_analysis[cluster_id] = {
                    "pattern_count": len(cluster_patterns),
                    "patterns": cluster_patterns,
                    "characteristics": self._analyze_cluster_characteristics(
                        cluster_patterns, patterns
                    ),
                }

            return {
                "symbol": symbol,
                "total_patterns": len(patterns),
                "n_clusters": n_clusters,
                "clusters": cluster_analysis,
                "analysis_timestamp": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            print(f"âŒ íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
        finally:
            session.close()

    def _extract_pattern_features(
        self, pattern: SignalPattern
    ) -> Optional[List[float]]:
        """íŒ¨í„´ì—ì„œ íŠ¹ì„± ë²¡í„° ì¶”ì¶œ"""
        try:
            features = []

            # 1. ì§€ì† ì‹œê°„ íŠ¹ì„±
            duration = (
                float(pattern.pattern_duration_hours)
                if pattern.pattern_duration_hours
                else 0.0
            )
            features.append(duration)

            # 2. ì„±ê³¼ íŠ¹ì„±
            for timeframe in ["1h", "1d", "1w"]:
                outcome = getattr(pattern, f"pattern_outcome_{timeframe}")
                features.append(float(outcome) if outcome is not None else 0.0)

                success = getattr(pattern, f"is_successful_{timeframe}")
                features.append(1.0 if success else 0.0)

            # 3. ì‹œì¥ ìƒí™© íŠ¹ì„± (ì›-í•« ì¸ì½”ë”©)
            market_conditions = ["bullish", "bearish", "sideways", "unknown"]
            for condition in market_conditions:
                features.append(1.0 if pattern.market_condition == condition else 0.0)

            # 4. ë³€ë™ì„± íŠ¹ì„±
            volatility_levels = ["low", "medium", "high"]
            for level in volatility_levels:
                features.append(1.0 if pattern.volatility_level == level else 0.0)

            return features

        except Exception as e:
            print(f"âš ï¸ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _simple_kmeans_clustering(
        self, feature_vectors: List[List[float]], n_clusters: int
    ) -> List[int]:
        """ê°„ë‹¨í•œ K-means í´ëŸ¬ìŠ¤í„°ë§ êµ¬í˜„"""
        if not feature_vectors or n_clusters <= 0:
            return []

        # numpy ë°°ì—´ë¡œ ë³€í™˜
        data = np.array(feature_vectors)
        n_samples, n_features = data.shape

        # ì´ˆê¸° ì¤‘ì‹¬ì  ëœë¤ ì„ íƒ
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´
        centroids = data[np.random.choice(n_samples, n_clusters, replace=False)]

        # K-means ë°˜ë³µ
        max_iterations = 100
        for iteration in range(max_iterations):
            # ê° ì ì„ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì— í• ë‹¹
            distances = np.sqrt(((data - centroids[:, np.newaxis]) ** 2).sum(axis=2))
            clusters = np.argmin(distances, axis=0)

            # ìƒˆë¡œìš´ ì¤‘ì‹¬ì  ê³„ì‚° (ë¹ˆ í´ëŸ¬ìŠ¤í„° ì²˜ë¦¬)
            new_centroids = []
            for i in range(n_clusters):
                cluster_data = data[clusters == i]
                if len(cluster_data) > 0:
                    new_centroids.append(cluster_data.mean(axis=0))
                else:
                    # ë¹ˆ í´ëŸ¬ìŠ¤í„°ëŠ” ê¸°ì¡´ ì¤‘ì‹¬ì  ìœ ì§€
                    new_centroids.append(centroids[i])
            new_centroids = np.array(new_centroids)

            # ìˆ˜ë ´ ì²´í¬
            if np.allclose(centroids, new_centroids):
                break

            centroids = new_centroids

        return clusters.tolist()

    def _analyze_cluster_characteristics(
        self, cluster_patterns: List[Dict], all_patterns: List[SignalPattern]
    ) -> Dict[str, Any]:
        """í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„"""
        if not cluster_patterns:
            return {}

        # íŒ¨í„´ ì´ë¦„ ë¶„í¬
        pattern_names = {}
        durations = []

        for pattern_info in cluster_patterns:
            # ì‹¤ì œ íŒ¨í„´ ê°ì²´ ì°¾ê¸°
            pattern = next(
                (p for p in all_patterns if p.id == pattern_info["id"]), None
            )
            if pattern:
                name = pattern.pattern_name
                pattern_names[name] = pattern_names.get(name, 0) + 1

                if pattern.pattern_duration_hours:
                    durations.append(float(pattern.pattern_duration_hours))

        # ê°€ì¥ ì¼ë°˜ì ì¸ íŒ¨í„´ ì´ë¦„
        most_common_pattern = (
            max(pattern_names.items(), key=lambda x: x[1])
            if pattern_names
            else ("unknown", 0)
        )

        return {
            "most_common_pattern": most_common_pattern[0],
            "pattern_name_distribution": pattern_names,
            "avg_duration": sum(durations) / len(durations) if durations else 0.0,
            "duration_range": {
                "min": min(durations) if durations else 0.0,
                "max": max(durations) if durations else 0.0,
            },
        }

    # =================================================================
    # ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„
    # =================================================================

    def analyze_temporal_patterns(
        self, symbol: str, timeframe: str, days: int = 30
    ) -> Dict[str, Any]:
        """
        ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„

        Args:
            symbol: ë¶„ì„í•  ì‹¬ë³¼
            timeframe: ì‹œê°„ëŒ€
            days: ë¶„ì„ ê¸°ê°„

        Returns:
            ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        session, pattern_repo, signal_repo = self._get_session_and_repositories()

        try:
            # ê¸°ê°„ ë‚´ íŒ¨í„´ë“¤ ì¡°íšŒ
            start_date = datetime.utcnow() - timedelta(days=days)
            patterns = (
                session.query(SignalPattern)
                .filter(
                    SignalPattern.symbol == symbol,
                    SignalPattern.timeframe == timeframe,
                    SignalPattern.pattern_start >= start_date,
                )
                .order_by(SignalPattern.pattern_start)
                .all()
            )

            if len(patterns) < 5:
                return {
                    "message": "ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ íŒ¨í„´ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 5ê°œ í•„ìš”)",
                    "pattern_count": len(patterns),
                }

            # ì‹œê°„ë³„ íŒ¨í„´ ë°œìƒ ë¹ˆë„
            hourly_distribution = self._analyze_hourly_distribution(patterns)

            # ìš”ì¼ë³„ íŒ¨í„´ ë°œìƒ ë¹ˆë„
            weekday_distribution = self._analyze_weekday_distribution(patterns)

            # íŒ¨í„´ ê°„ê²© ë¶„ì„
            interval_analysis = self._analyze_pattern_intervals(patterns)

            # ê³„ì ˆì„± ë¶„ì„
            seasonal_analysis = self._analyze_seasonal_patterns(patterns)

            return {
                "symbol": symbol,
                "timeframe": timeframe,
                "analysis_period_days": days,
                "total_patterns": len(patterns),
                "temporal_analysis": {
                    "hourly_distribution": hourly_distribution,
                    "weekday_distribution": weekday_distribution,
                    "interval_analysis": interval_analysis,
                    "seasonal_analysis": seasonal_analysis,
                },
                "analysis_timestamp": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            print(f"âŒ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
        finally:
            session.close()

    def _analyze_hourly_distribution(
        self, patterns: List[SignalPattern]
    ) -> Dict[str, Any]:
        """ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë°œìƒ ë¶„í¬ ë¶„ì„"""
        hourly_counts = {}

        for pattern in patterns:
            hour = pattern.pattern_start.hour
            hourly_counts[hour] = hourly_counts.get(hour, 0) + 1

        # ê°€ì¥ í™œë°œí•œ ì‹œê°„ëŒ€
        peak_hour = (
            max(hourly_counts.items(), key=lambda x: x[1]) if hourly_counts else (0, 0)
        )

        return {
            "distribution": hourly_counts,
            "peak_hour": peak_hour[0],
            "peak_count": peak_hour[1],
            "total_hours_active": len(hourly_counts),
        }

    def _analyze_weekday_distribution(
        self, patterns: List[SignalPattern]
    ) -> Dict[str, Any]:
        """ìš”ì¼ë³„ íŒ¨í„´ ë°œìƒ ë¶„í¬ ë¶„ì„"""
        weekday_names = [
            "Monday",
            "Tuesday",
            "Wednesday",
            "Thursday",
            "Friday",
            "Saturday",
            "Sunday",
        ]
        weekday_counts = {}

        for pattern in patterns:
            weekday = pattern.pattern_start.weekday()
            weekday_name = weekday_names[weekday]
            weekday_counts[weekday_name] = weekday_counts.get(weekday_name, 0) + 1

        # ê°€ì¥ í™œë°œí•œ ìš”ì¼
        peak_weekday = (
            max(weekday_counts.items(), key=lambda x: x[1])
            if weekday_counts
            else ("Monday", 0)
        )

        return {
            "distribution": weekday_counts,
            "peak_weekday": peak_weekday[0],
            "peak_count": peak_weekday[1],
            "weekday_activity": len(weekday_counts),
        }

    def _analyze_pattern_intervals(
        self, patterns: List[SignalPattern]
    ) -> Dict[str, Any]:
        """íŒ¨í„´ ê°„ ì‹œê°„ ê°„ê²© ë¶„ì„"""
        if len(patterns) < 2:
            return {"message": "ê°„ê²© ë¶„ì„ì„ ìœ„í•œ íŒ¨í„´ì´ ë¶€ì¡±í•©ë‹ˆë‹¤"}

        intervals = []
        for i in range(1, len(patterns)):
            interval = (
                patterns[i].pattern_start - patterns[i - 1].pattern_start
            ).total_seconds() / 3600
            intervals.append(interval)

        return {
            "avg_interval_hours": sum(intervals) / len(intervals),
            "min_interval_hours": min(intervals),
            "max_interval_hours": max(intervals),
            "total_intervals": len(intervals),
        }

    def _analyze_seasonal_patterns(
        self, patterns: List[SignalPattern]
    ) -> Dict[str, Any]:
        """ê³„ì ˆì„± íŒ¨í„´ ë¶„ì„"""
        monthly_counts = {}

        for pattern in patterns:
            month = pattern.pattern_start.month
            monthly_counts[month] = monthly_counts.get(month, 0) + 1

        # ê°€ì¥ í™œë°œí•œ ì›”
        peak_month = (
            max(monthly_counts.items(), key=lambda x: x[1])
            if monthly_counts
            else (1, 0)
        )

        return {
            "monthly_distribution": monthly_counts,
            "peak_month": peak_month[0],
            "peak_count": peak_month[1],
            "active_months": len(monthly_counts),
        }

    def __del__(self):
        """ì†Œë©¸ì - ì„¸ì…˜ ì •ë¦¬"""
        if self.session:
            self.session.close()

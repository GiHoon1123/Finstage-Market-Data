import requests
import xml.etree.ElementTree as ET
import random
import time
from typing import List, Dict, Optional
from dateutil import parser as date_parser
from app.news_crawler.service.base import BaseCrawler
from app.news_crawler.service.news_processor import NewsProcessor
from app.common.constants.investing_config import INVESTING_RSS_FEEDS
from app.common.utils.memory_cache import cache_result
from app.common.utils.memory_optimizer import memory_monitor


class InvestingNewsCrawler(BaseCrawler):
    # ë‹¤ì–‘í•œ ë¸Œë¼ìš°ì € User-Agent (ë” í˜„ì‹¤ì ì¸ ì‹œë®¬ë ˆì´ì…˜)
    USER_AGENTS = [
        # Chrome (Windows)
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
        
        # Chrome (Mac)
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        
        # Firefox (Windows)
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:119.0) Gecko/20100101 Firefox/119.0",
        
        # Firefox (Mac)
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:119.0) Gecko/20100101 Firefox/119.0",
        
        # Safari (Mac)
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        
        # Edge (Windows)
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
    ]
    
    # ë¸Œë¼ìš°ì €ê°€ ë³´ë‚´ëŠ” Accept í—¤ë”ë“¤
    ACCEPT_HEADERS = [
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "application/rss+xml, application/xml, text/xml, */*",
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    ]
    
    # ì–¸ì–´ ì„¤ì •
    ACCEPT_LANGUAGES = [
        "en-US,en;q=0.9",
        "en-GB,en;q=0.9",
        "en-CA,en;q=0.9",
        "ko-KR,ko;q=0.9,en;q=0.8",
        "ja-JP,ja;q=0.9,en;q=0.8",
    ]
    
    # Referer í—¤ë” (Investing.com ê´€ë ¨ í˜ì´ì§€ë“¤)
    REFERERS = [
        "https://www.investing.com/",
        "https://www.investing.com/news/",
        "https://www.investing.com/economic-calendar/",
        "https://www.investing.com/markets/",
        "https://www.google.com/",
        "https://www.bing.com/",
    ]

    def __init__(self, symbol: str):
        self.symbol = symbol
        self.rss_url = INVESTING_RSS_FEEDS.get(symbol)
        self.session = requests.Session()
        self._last_request_time = 0
        self._setup_session()

    def _setup_session(self):
        """ì„¸ì…˜ì„ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ì„¤ì •"""
        # ëœë¤ User-Agent ì„¤ì •
        user_agent = random.choice(self.USER_AGENTS)
        self.session.headers.update({
            "User-Agent": user_agent,
            "Accept": random.choice(self.ACCEPT_HEADERS),
            "Accept-Language": random.choice(self.ACCEPT_LANGUAGES),
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0",
        })

    def _get_random_headers(self) -> Dict[str, str]:
        """ìš”ì²­ë§ˆë‹¤ ëœë¤ í—¤ë” ìƒì„±"""
        headers = {}
        
        # User-Agent ëœë¤ ë³€ê²½
        headers["User-Agent"] = random.choice(self.USER_AGENTS)
        
        # Accept í—¤ë” ëœë¤ ë³€ê²½
        headers["Accept"] = random.choice(self.ACCEPT_HEADERS)
        
        # Accept-Language ëœë¤ ë³€ê²½
        headers["Accept-Language"] = random.choice(self.ACCEPT_LANGUAGES)
        
        # Referer ì„¤ì • (Investing.com ê´€ë ¨ í˜ì´ì§€ë¡œ)
        referer = random.choice(self.REFERERS)
        headers["Referer"] = referer
        
        return headers

    def _rate_limit_delay(self):
        """ìš”ì²­ ê°„ ëœë¤ ì§€ì—° (API ì œí•œ ë°©ì§€)"""
        current_time = time.time()
        time_since_last = current_time - self._last_request_time
        
        # ìµœì†Œ 1ì´ˆ, ìµœëŒ€ 3ì´ˆ ëœë¤ ì§€ì—° (ë‰´ìŠ¤ í¬ë¡¤ë§ì€ ë” ì‹ ì¤‘í•˜ê²Œ)
        min_delay = 1.0
        max_delay = 3.0
        
        if time_since_last < min_delay:
            delay = random.uniform(min_delay - time_since_last, max_delay)
            time.sleep(delay)
        
        self._last_request_time = time.time()

    def _make_request(self, url: str) -> Optional[requests.Response]:
        """ë¸Œë¼ìš°ì € ì‹œë®¬ë ˆì´ì…˜ ìš”ì²­ ìˆ˜í–‰"""
        try:
            # ìš”ì²­ ê°„ ì§€ì—°
            self._rate_limit_delay()
            
            # ëœë¤ í—¤ë” ì„¤ì •
            headers = self._get_random_headers()
            
            # ìš”ì²­ ìˆ˜í–‰
            response = self.session.get(url, headers=headers, timeout=15)
            
            # 429 ì—ëŸ¬ ì‹œ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
            retry_count = 0
            while response.status_code == 429 and retry_count < 3:
                print(f"âš ï¸ {self.symbol} ë‰´ìŠ¤ API ì œí•œ ê°ì§€, {retry_count + 1}íšŒ ì¬ì‹œë„ ì¤‘...")
                time.sleep(random.uniform(3, 8))  # ë” ê¸´ ì§€ì—°
                headers = self._get_random_headers()  # í—¤ë” ì¬ì„¤ì •
                response = self.session.get(url, headers=headers, timeout=15)
                retry_count += 1
            
            return response
            
        except Exception as e:
            print(f"âŒ {self.symbol} ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None

    @cache_result(cache_name="news", ttl=300)  # 5ë¶„ ìºì‹±
    @memory_monitor
    def crawl(self) -> List[Dict]:
        try:
            res = self._make_request(self.rss_url)
            if not res:
                return []
                
            if res.status_code != 200:
                print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: status {res.status_code}")
                return []

            root = ET.fromstring(res.content)
            items = root.find("channel").findall("item")
            news_list = []

            for item in items[:1]:
                title = item.findtext("title")
                url = item.findtext("link")
                summary = item.findtext("description")
                pub_date_raw = item.findtext("pubDate")

                if not title or not url:
                    continue

                content_hash = self.generate_hash(title)
                published_at = date_parser.parse(pub_date_raw) if pub_date_raw else None
                if published_at and published_at.tzinfo:
                    published_at = published_at.replace(tzinfo=None)

                news_list.append(
                    {
                        "title": title.strip(),
                        "url": url.strip(),
                        "source": "investing.com",
                        "summary": summary.strip() if summary else None,
                        "html": "",
                        "symbol": self.symbol,
                        "content_hash": content_hash,
                        "crawled_at": self.get_crawled_at(),
                        "published_at": published_at,
                    }
                )

            return news_list

        except Exception as e:
            print(f"âŒ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    @memory_monitor
    def process_all(self, telegram_enabled: bool = False):
        results = self.crawl()
        processor = NewsProcessor(results, telegram_enabled=telegram_enabled)
        processor.run()

    @memory_monitor
    def crawl_multiple_symbols_batch(self, symbols: list, batch_size: int = 5) -> dict:
        """
        ì—¬ëŸ¬ ì‹¬ë³¼ì˜ ë‰´ìŠ¤ë¥¼ ë°°ì¹˜ë¡œ í¬ë¡¤ë§í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ

        Args:
            symbols: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            ì‹¬ë³¼ë³„ í¬ë¡¤ë§ ê²°ê³¼
        """
        results = {}

        for i in range(0, len(symbols), batch_size):
            batch = symbols[i : i + batch_size]
            print(
                f"ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}-{min(i+batch_size, len(symbols))}/{len(symbols)}"
            )

            for symbol in batch:
                try:
                    # ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±í•˜ì—¬ ì‹¬ë³¼ë³„ í¬ë¡¤ë§
                    crawler = InvestingNewsCrawler(symbol)
                    news_list = crawler.crawl()
                    results[symbol] = {
                        "success": True,
                        "count": len(news_list),
                        "news": news_list,
                    }
                except Exception as e:
                    results[symbol] = {"success": False, "error": str(e), "count": 0}

            # ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            del batch

        return results

    @cache_result(cache_name="news", ttl=600)  # 10ë¶„ ìºì‹±
    @memory_monitor
    def get_cached_news_summary(self, symbol: str) -> dict:
        """
        ì‹¬ë³¼ì˜ ë‰´ìŠ¤ ìš”ì•½ ì •ë³´ ì¡°íšŒ (ìºì‹± ì ìš©)

        Args:
            symbol: ì‹¬ë³¼

        Returns:
            ë‰´ìŠ¤ ìš”ì•½ ì •ë³´
        """
        try:
            news_list = self.crawl()

            summary = {
                "symbol": symbol,
                "source": "investing.com",
                "total_news": len(news_list),
                "latest_news": news_list[0] if news_list else None,
                "crawled_at": self.get_crawled_at().isoformat(),
                "rss_url": self.rss_url,
            }

            return summary

        except Exception as e:
            return {"error": f"{symbol} ë‰´ìŠ¤ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}

    @memory_monitor
    def cleanup_old_cache(self):
        """
        ì˜¤ë˜ëœ ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ì •ë¦¬
        """
        try:
            # ìºì‹œ ì •ë¦¬ëŠ” memory_cache ëª¨ë“ˆì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ë§Œ
            # í•„ìš”ì‹œ ìˆ˜ë™ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆëŠ” ë©”ì„œë“œ ì œê³µ
            print(f"ğŸ§¹ {self.symbol} ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")

import requests
import xml.etree.ElementTree as ET
from typing import List, Dict
from dateutil import parser as date_parser
from app.news_crawler.service.base import BaseCrawler
from app.news_crawler.service.news_processor import NewsProcessor
from app.common.constants.rss_feeds import INVESTING_RSS_FEEDS
from app.common.utils.memory_cache import cache_result
from app.common.utils.memory_optimizer import memory_monitor


class InvestingNewsCrawler(BaseCrawler):
    def __init__(self, symbol: str):
        self.symbol = symbol
        self.rss_url = INVESTING_RSS_FEEDS.get(symbol)

    @cache_result(cache_name="news", ttl=300)  # 5ë¶„ ìºì‹±
    @memory_monitor
    def crawl(self) -> List[Dict]:
        try:
            res = requests.get(self.rss_url, headers={"User-Agent": "Mozilla/5.0"})
            if res.status_code != 200:
                print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: status {res.status_code}")
                return []

            root = ET.fromstring(res.content)
            items = root.find("channel").findall("item")
            news_list = []

            for item in items[:1]:
                title = item.findtext("title")
                url = item.findtext("link")
                summary = item.findtext("description")
                pub_date_raw = item.findtext("pubDate")

                if not title or not url:
                    continue

                content_hash = self.generate_hash(title)
                published_at = date_parser.parse(pub_date_raw) if pub_date_raw else None
                if published_at and published_at.tzinfo:
                    published_at = published_at.replace(tzinfo=None)

                news_list.append(
                    {
                        "title": title.strip(),
                        "url": url.strip(),
                        "source": "investing.com",
                        "summary": summary.strip() if summary else None,
                        "html": "",
                        "symbol": self.symbol,
                        "content_hash": content_hash,
                        "crawled_at": self.get_crawled_at(),
                        "published_at": published_at,
                    }
                )

            return news_list

        except Exception as e:
            print(f"âŒ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    @memory_monitor
    def process_all(self):
        results = self.crawl()
        processor = NewsProcessor(results)
        processor.run()

    @memory_monitor
    def crawl_multiple_symbols_batch(self, symbols: list, batch_size: int = 5) -> dict:
        """
        ì—¬ëŸ¬ ì‹¬ë³¼ì˜ ë‰´ìŠ¤ë¥¼ ë°°ì¹˜ë¡œ í¬ë¡¤ë§í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ

        Args:
            symbols: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            ì‹¬ë³¼ë³„ í¬ë¡¤ë§ ê²°ê³¼
        """
        results = {}

        for i in range(0, len(symbols), batch_size):
            batch = symbols[i : i + batch_size]
            print(
                f"ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}-{min(i+batch_size, len(symbols))}/{len(symbols)}"
            )

            for symbol in batch:
                try:
                    # ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±í•˜ì—¬ ì‹¬ë³¼ë³„ í¬ë¡¤ë§
                    crawler = InvestingNewsCrawler(symbol)
                    news_list = crawler.crawl()
                    results[symbol] = {
                        "success": True,
                        "count": len(news_list),
                        "news": news_list,
                    }
                except Exception as e:
                    results[symbol] = {"success": False, "error": str(e), "count": 0}

            # ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            del batch

        return results

    @cache_result(cache_name="news", ttl=600)  # 10ë¶„ ìºì‹±
    @memory_monitor
    def get_cached_news_summary(self, symbol: str) -> dict:
        """
        ì‹¬ë³¼ì˜ ë‰´ìŠ¤ ìš”ì•½ ì •ë³´ ì¡°íšŒ (ìºì‹± ì ìš©)

        Args:
            symbol: ì‹¬ë³¼

        Returns:
            ë‰´ìŠ¤ ìš”ì•½ ì •ë³´
        """
        try:
            news_list = self.crawl()

            summary = {
                "symbol": symbol,
                "source": "investing.com",
                "total_news": len(news_list),
                "latest_news": news_list[0] if news_list else None,
                "crawled_at": self.get_crawled_at().isoformat(),
                "rss_url": self.rss_url,
            }

            return summary

        except Exception as e:
            return {"error": f"{symbol} ë‰´ìŠ¤ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}

    @memory_monitor
    def cleanup_old_cache(self):
        """
        ì˜¤ë˜ëœ ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ì •ë¦¬
        """
        try:
            # ìºì‹œ ì •ë¦¬ëŠ” memory_cache ëª¨ë“ˆì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ë§Œ
            # í•„ìš”ì‹œ ìˆ˜ë™ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆëŠ” ë©”ì„œë“œ ì œê³µ
            print(f"ğŸ§¹ {self.symbol} ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")

"""
ë°°í¬ í›„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° íŠœë‹

ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì„±ëŠ¥ ì§€í‘œë¥¼ ìˆ˜ì§‘í•˜ê³  í•„ìš”ì‹œ ì¶”ê°€ íŠœë‹ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""

import asyncio
import time
import json
import statistics
from typing import Dict, Any, List
from datetime import datetime, timedelta
from dataclasses import dataclass

from app.common.utils.logging_config import get_logger
from app.common.utils.memory_optimizer import memory_monitor
from app.common.monitoring.performance_metrics_collector import get_metrics_collector
from app.common.monitoring.alert_system import get_alert_system
from app.common.optimization.optimization_manager import get_optimization_manager

logger = get_logger(__name__)


@dataclass
class PerformanceThreshold:
    """ì„±ëŠ¥ ì„ê³„ê°’"""

    metric_name: str
    warning_threshold: float
    critical_threshold: float
    unit: str
    description: str


class PostDeploymentMonitor:
    """ë°°í¬ í›„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.metrics_collector = get_metrics_collector()
        self.alert_system = get_alert_system()
        self.optimization_manager = get_optimization_manager()

        # ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì •
        self.performance_thresholds = [
            PerformanceThreshold("cpu_percent", 70.0, 85.0, "%", "CPU ì‚¬ìš©ë¥ "),
            PerformanceThreshold("memory_percent", 75.0, 90.0, "%", "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ "),
            PerformanceThreshold(
                "api_response_time_ms", 500.0, 1000.0, "ms", "API ì‘ë‹µ ì‹œê°„"
            ),
            PerformanceThreshold("error_rate_percent", 2.0, 5.0, "%", "ì—ëŸ¬ìœ¨"),
            PerformanceThreshold(
                "cache_hit_rate", 80.0, 70.0, "%", "ìºì‹œ íˆíŠ¸ìœ¨ (ë‚®ì„ìˆ˜ë¡ ë‚˜ì¨)"
            ),
        ]

        self.monitoring_data = []
        self.tuning_actions = []

    @memory_monitor
    async def start_post_deployment_monitoring(
        self, duration_hours: int = 24
    ) -> Dict[str, Any]:
        """ë°°í¬ í›„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        logger.info("post_deployment_monitoring_started", duration_hours=duration_hours)

        monitoring_result = {
            "monitoring_id": f"monitor_{int(time.time())}",
            "started_at": datetime.now().isoformat(),
            "duration_hours": duration_hours,
            "status": "running",
        }

        try:
            # 1. ì´ˆê¸° ì„±ëŠ¥ ê¸°ì¤€ì„  ì„¤ì •
            baseline_result = await self._establish_baseline()
            monitoring_result["baseline"] = baseline_result

            # 2. ì—°ì† ëª¨ë‹ˆí„°ë§ ì‹œì‘
            monitoring_task = asyncio.create_task(
                self._continuous_monitoring(duration_hours)
            )

            # 3. ìë™ íŠœë‹ ì‹œìŠ¤í…œ ì‹œì‘
            tuning_task = asyncio.create_task(self._auto_tuning_system())

            # 4. ëª¨ë‹ˆí„°ë§ ì™„ë£Œ ëŒ€ê¸°
            monitoring_data, tuning_data = await asyncio.gather(
                monitoring_task, tuning_task, return_exceptions=True
            )

            # 5. ê²°ê³¼ ë¶„ì„
            analysis_result = await self._analyze_monitoring_results()

            monitoring_result.update(
                {
                    "status": "completed",
                    "completed_at": datetime.now().isoformat(),
                    "monitoring_data": (
                        monitoring_data
                        if not isinstance(monitoring_data, Exception)
                        else str(monitoring_data)
                    ),
                    "tuning_data": (
                        tuning_data
                        if not isinstance(tuning_data, Exception)
                        else str(tuning_data)
                    ),
                    "analysis": analysis_result,
                }
            )

            logger.info("post_deployment_monitoring_completed")

            return monitoring_result

        except Exception as e:
            logger.error("post_deployment_monitoring_failed", error=str(e))
            monitoring_result.update(
                {
                    "status": "failed",
                    "error": str(e),
                    "completed_at": datetime.now().isoformat(),
                }
            )
            return monitoring_result

    async def _establish_baseline(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ê¸°ì¤€ì„  ì„¤ì •"""
        logger.info("establishing_performance_baseline")

        try:
            # í˜„ì¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            current_metrics = self.metrics_collector._get_current_metrics_summary()

            if current_metrics:
                # ìµœì í™” ë§¤ë‹ˆì €ì— ê¸°ì¤€ì„  ì„¤ì •
                success = self.optimization_manager.set_performance_baseline()

                return {
                    "success": success,
                    "baseline_metrics": current_metrics,
                    "timestamp": datetime.now().isoformat(),
                }
            else:
                return {
                    "success": False,
                    "error": "No metrics available for baseline",
                    "timestamp": datetime.now().isoformat(),
                }

        except Exception as e:
            logger.error("baseline_establishment_failed", error=str(e))
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    async def _continuous_monitoring(self, duration_hours: int) -> Dict[str, Any]:
        """ì—°ì† ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        logger.info("starting_continuous_monitoring")

        end_time = datetime.now() + timedelta(hours=duration_hours)
        monitoring_interval = 60  # 1ë¶„ ê°„ê²©

        monitoring_data = []

        while datetime.now() < end_time:
            try:
                # í˜„ì¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                current_metrics = self.metrics_collector._get_current_metrics_summary()

                if current_metrics:
                    # ì„ê³„ê°’ í™•ì¸
                    threshold_violations = self._check_thresholds(current_metrics)

                    monitoring_point = {
                        "timestamp": datetime.now().isoformat(),
                        "metrics": current_metrics,
                        "threshold_violations": threshold_violations,
                    }

                    monitoring_data.append(monitoring_point)
                    self.monitoring_data.append(monitoring_point)

                    # ì„ê³„ê°’ ìœ„ë°˜ ì‹œ ë¡œê·¸ ê¸°ë¡
                    if threshold_violations:
                        logger.warning(
                            "performance_threshold_violations",
                            violations=threshold_violations,
                        )

                await asyncio.sleep(monitoring_interval)

            except Exception as e:
                logger.error("monitoring_cycle_error", error=str(e))
                await asyncio.sleep(monitoring_interval)

        return {
            "monitoring_completed": True,
            "data_points": len(monitoring_data),
            "monitoring_data": monitoring_data[-10:],  # ìµœê·¼ 10ê°œë§Œ ë°˜í™˜
            "total_violations": sum(
                len(point.get("threshold_violations", [])) for point in monitoring_data
            ),
        }

    def _check_thresholds(self, metrics: Dict[str, float]) -> List[Dict[str, Any]]:
        """ì„±ëŠ¥ ì„ê³„ê°’ í™•ì¸"""
        violations = []

        for threshold in self.performance_thresholds:
            metric_value = metrics.get(threshold.metric_name)

            if metric_value is None:
                continue

            violation_level = None

            # ìºì‹œ íˆíŠ¸ìœ¨ì€ ë‚®ì„ìˆ˜ë¡ ë‚˜ì¨ (ì—­ë°©í–¥ ì²´í¬)
            if threshold.metric_name == "cache_hit_rate":
                if metric_value < threshold.critical_threshold:
                    violation_level = "critical"
                elif metric_value < threshold.warning_threshold:
                    violation_level = "warning"
            else:
                # ì¼ë°˜ì ì¸ ë©”íŠ¸ë¦­ (ë†’ì„ìˆ˜ë¡ ë‚˜ì¨)
                if metric_value > threshold.critical_threshold:
                    violation_level = "critical"
                elif metric_value > threshold.warning_threshold:
                    violation_level = "warning"

            if violation_level:
                violations.append(
                    {
                        "metric_name": threshold.metric_name,
                        "current_value": metric_value,
                        "threshold_type": violation_level,
                        "warning_threshold": threshold.warning_threshold,
                        "critical_threshold": threshold.critical_threshold,
                        "unit": threshold.unit,
                        "description": threshold.description,
                    }
                )

        return violations

    async def _auto_tuning_system(self) -> Dict[str, Any]:
        """ìë™ íŠœë‹ ì‹œìŠ¤í…œ"""
        logger.info("starting_auto_tuning_system")

        tuning_actions = []
        check_interval = 300  # 5ë¶„ ê°„ê²©ìœ¼ë¡œ íŠœë‹ ê¸°íšŒ í™•ì¸

        while True:
            try:
                # ìµœê·¼ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ë¶„ì„
                if len(self.monitoring_data) >= 5:  # ìµœì†Œ 5ê°œ ë°ì´í„° í¬ì¸íŠ¸ í•„ìš”
                    tuning_recommendations = await self._analyze_tuning_opportunities()

                    for recommendation in tuning_recommendations:
                        if (
                            recommendation["confidence"] > 0.8
                        ):  # 80% ì´ìƒ í™•ì‹ í•  ë•Œë§Œ ìë™ ì ìš©
                            action_result = await self._apply_tuning_action(
                                recommendation
                            )
                            tuning_actions.append(action_result)
                            self.tuning_actions.append(action_result)

                await asyncio.sleep(check_interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error("auto_tuning_error", error=str(e))
                await asyncio.sleep(check_interval)

        return {
            "tuning_completed": True,
            "actions_taken": len(tuning_actions),
            "tuning_actions": tuning_actions,
        }

    async def _analyze_tuning_opportunities(self) -> List[Dict[str, Any]]:
        """íŠœë‹ ê¸°íšŒ ë¶„ì„"""
        recommendations = []

        try:
            # ìµœê·¼ ë°ì´í„° ë¶„ì„
            recent_data = self.monitoring_data[-10:]  # ìµœê·¼ 10ê°œ ë°ì´í„° í¬ì¸íŠ¸

            if not recent_data:
                return recommendations

            # CPU ì‚¬ìš©ë¥  ë¶„ì„
            cpu_values = [
                point["metrics"].get("cpu_percent", 0) for point in recent_data
            ]
            avg_cpu = statistics.mean(cpu_values)

            if avg_cpu > 75:
                recommendations.append(
                    {
                        "type": "enable_optimization",
                        "optimization_rule": "async_api_endpoints",
                        "reason": f"High CPU usage detected: {avg_cpu:.1f}%",
                        "confidence": 0.9,
                        "expected_improvement": "25% CPU reduction",
                    }
                )

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë¶„ì„
            memory_values = [
                point["metrics"].get("memory_percent", 0) for point in recent_data
            ]
            avg_memory = statistics.mean(memory_values)

            if avg_memory > 80:
                recommendations.append(
                    {
                        "type": "enable_optimization",
                        "optimization_rule": "memory_optimization_basic",
                        "reason": f"High memory usage detected: {avg_memory:.1f}%",
                        "confidence": 0.85,
                        "expected_improvement": "15% memory reduction",
                    }
                )

            # ì‘ë‹µ ì‹œê°„ ë¶„ì„
            response_time_values = [
                point["metrics"].get("api_response_time_ms", 0) for point in recent_data
            ]
            avg_response_time = statistics.mean(response_time_values)

            if avg_response_time > 600:
                recommendations.append(
                    {
                        "type": "enable_optimization",
                        "optimization_rule": "caching_technical_analysis",
                        "reason": f"Slow response times detected: {avg_response_time:.0f}ms",
                        "confidence": 0.95,
                        "expected_improvement": "40% response time improvement",
                    }
                )

            # ìºì‹œ íˆíŠ¸ìœ¨ ë¶„ì„
            cache_hit_values = [
                point["metrics"].get("cache_hit_rate", 100) for point in recent_data
            ]
            avg_cache_hit = statistics.mean(cache_hit_values)

            if avg_cache_hit < 75:
                recommendations.append(
                    {
                        "type": "tune_cache_settings",
                        "reason": f"Low cache hit rate: {avg_cache_hit:.1f}%",
                        "confidence": 0.8,
                        "expected_improvement": "Improved cache efficiency",
                    }
                )

            return recommendations

        except Exception as e:
            logger.error("tuning_analysis_failed", error=str(e))
            return recommendations

    async def _apply_tuning_action(
        self, recommendation: Dict[str, Any]
    ) -> Dict[str, Any]:
        """íŠœë‹ ì•¡ì…˜ ì ìš©"""
        logger.info("applying_tuning_action", recommendation=recommendation)

        action_result = {
            "recommendation": recommendation,
            "applied_at": datetime.now().isoformat(),
            "success": False,
        }

        try:
            if recommendation["type"] == "enable_optimization":
                rule_id = recommendation["optimization_rule"]
                result = self.optimization_manager.enable_optimization(rule_id)

                action_result.update(
                    {
                        "success": result.status.value == "enabled",
                        "result": {
                            "rule_id": rule_id,
                            "status": result.status.value,
                            "error": result.error_message,
                        },
                    }
                )

            elif recommendation["type"] == "tune_cache_settings":
                # ìºì‹œ ì„¤ì • íŠœë‹ (ì‹¤ì œë¡œëŠ” ìºì‹œ ì‹œìŠ¤í…œì— ë”°ë¼ êµ¬í˜„)
                action_result.update(
                    {
                        "success": True,
                        "result": {
                            "action": "cache_settings_tuned",
                            "message": "Cache TTL and size optimized",
                        },
                    }
                )

            if action_result["success"]:
                logger.info(
                    "tuning_action_applied_successfully",
                    action_type=recommendation["type"],
                )
            else:
                logger.warning(
                    "tuning_action_failed", action_type=recommendation["type"]
                )

            return action_result

        except Exception as e:
            logger.error("tuning_action_application_failed", error=str(e))
            action_result["error"] = str(e)
            return action_result

    async def _analyze_monitoring_results(self) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ë¶„ì„"""
        logger.info("analyzing_monitoring_results")

        try:
            if not self.monitoring_data:
                return {"error": "No monitoring data available"}

            # ì „ì²´ ê¸°ê°„ í†µê³„
            all_metrics = {}
            for point in self.monitoring_data:
                for metric_name, value in point["metrics"].items():
                    if metric_name not in all_metrics:
                        all_metrics[metric_name] = []
                    all_metrics[metric_name].append(value)

            # í†µê³„ ê³„ì‚°
            statistics_summary = {}
            for metric_name, values in all_metrics.items():
                if values:
                    statistics_summary[metric_name] = {
                        "avg": statistics.mean(values),
                        "min": min(values),
                        "max": max(values),
                        "median": statistics.median(values),
                        "std_dev": statistics.stdev(values) if len(values) > 1 else 0,
                    }

            # ì„ê³„ê°’ ìœ„ë°˜ ë¶„ì„
            total_violations = sum(
                len(point.get("threshold_violations", []))
                for point in self.monitoring_data
            )

            violation_by_metric = {}
            for point in self.monitoring_data:
                for violation in point.get("threshold_violations", []):
                    metric = violation["metric_name"]
                    if metric not in violation_by_metric:
                        violation_by_metric[metric] = {"warning": 0, "critical": 0}
                    violation_by_metric[metric][violation["threshold_type"]] += 1

            # íŠœë‹ íš¨ê³¼ ë¶„ì„
            tuning_effectiveness = self._analyze_tuning_effectiveness()

            # ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            performance_score = self._calculate_overall_performance_score(
                statistics_summary
            )

            return {
                "monitoring_period": {
                    "start_time": self.monitoring_data[0]["timestamp"],
                    "end_time": self.monitoring_data[-1]["timestamp"],
                    "data_points": len(self.monitoring_data),
                },
                "performance_statistics": statistics_summary,
                "threshold_violations": {
                    "total_violations": total_violations,
                    "violations_by_metric": violation_by_metric,
                },
                "tuning_actions": {
                    "total_actions": len(self.tuning_actions),
                    "effectiveness": tuning_effectiveness,
                },
                "overall_performance_score": performance_score,
                "recommendations": self._generate_final_recommendations(
                    statistics_summary, violation_by_metric
                ),
            }

        except Exception as e:
            logger.error("monitoring_results_analysis_failed", error=str(e))
            return {"error": str(e)}

    def _analyze_tuning_effectiveness(self) -> Dict[str, Any]:
        """íŠœë‹ íš¨ê³¼ ë¶„ì„"""
        if not self.tuning_actions:
            return {"message": "No tuning actions were taken"}

        successful_actions = [
            action for action in self.tuning_actions if action.get("success", False)
        ]

        return {
            "total_actions": len(self.tuning_actions),
            "successful_actions": len(successful_actions),
            "success_rate": len(successful_actions) / len(self.tuning_actions) * 100,
            "actions_summary": [
                {
                    "type": action["recommendation"]["type"],
                    "success": action.get("success", False),
                    "applied_at": action["applied_at"],
                }
                for action in self.tuning_actions
            ],
        }

    def _calculate_overall_performance_score(
        self, statistics_summary: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = {}

            # CPU ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            cpu_avg = statistics_summary.get("cpu_percent", {}).get("avg", 50)
            cpu_score = max(0, min(100, 100 - cpu_avg))
            scores["cpu"] = cpu_score

            # ë©”ëª¨ë¦¬ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            memory_avg = statistics_summary.get("memory_percent", {}).get("avg", 50)
            memory_score = max(0, min(100, 100 - memory_avg))
            scores["memory"] = memory_score

            # ì‘ë‹µ ì‹œê°„ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            response_time_avg = statistics_summary.get("api_response_time_ms", {}).get(
                "avg", 500
            )
            response_time_score = max(0, min(100, 100 - (response_time_avg - 100) / 10))
            scores["response_time"] = response_time_score

            # ìºì‹œ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
            cache_hit_avg = statistics_summary.get("cache_hit_rate", {}).get("avg", 80)
            scores["cache"] = cache_hit_avg

            # ì „ì²´ ì ìˆ˜
            overall_score = statistics.mean(scores.values())

            # ë“±ê¸‰ ê²°ì •
            if overall_score >= 90:
                grade = "A"
            elif overall_score >= 80:
                grade = "B"
            elif overall_score >= 70:
                grade = "C"
            elif overall_score >= 60:
                grade = "D"
            else:
                grade = "F"

            return {
                "overall_score": round(overall_score, 1),
                "grade": grade,
                "component_scores": {k: round(v, 1) for k, v in scores.items()},
            }

        except Exception as e:
            logger.error("performance_score_calculation_failed", error=str(e))
            return {"error": str(e)}

    def _generate_final_recommendations(
        self, statistics_summary: Dict[str, Any], violations_by_metric: Dict[str, Any]
    ) -> List[str]:
        """ìµœì¢… ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # CPU ì‚¬ìš©ë¥  ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        cpu_avg = statistics_summary.get("cpu_percent", {}).get("avg", 0)
        if cpu_avg > 70:
            recommendations.append(
                f"CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({cpu_avg:.1f}%). "
                "ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            )

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        memory_avg = statistics_summary.get("memory_percent", {}).get("avg", 0)
        if memory_avg > 75:
            recommendations.append(
                f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({memory_avg:.1f}%). "
                "ë©”ëª¨ë¦¬ ìµœì í™” ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ íŠœë‹ì„ ê³ ë ¤í•˜ì„¸ìš”."
            )

        # ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        response_time_avg = statistics_summary.get("api_response_time_ms", {}).get(
            "avg", 0
        )
        if response_time_avg > 500:
            recommendations.append(
                f"API ì‘ë‹µ ì‹œê°„ì´ ëŠë¦½ë‹ˆë‹¤ ({response_time_avg:.0f}ms). "
                "ìºì‹± ì‹œìŠ¤í…œ ê°•í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            )

        # ìºì‹œ íˆíŠ¸ìœ¨ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        cache_hit_avg = statistics_summary.get("cache_hit_rate", {}).get("avg", 100)
        if cache_hit_avg < 80:
            recommendations.append(
                f"ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤ ({cache_hit_avg:.1f}%). "
                "ìºì‹œ ì „ëµ ë° TTL ì„¤ì •ì„ ê²€í† í•˜ì„¸ìš”."
            )

        # ì„ê³„ê°’ ìœ„ë°˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if violations_by_metric:
            most_violated_metric = max(
                violations_by_metric.items(),
                key=lambda x: x[1]["warning"] + x[1]["critical"],
            )
            recommendations.append(
                f"{most_violated_metric[0]} ë©”íŠ¸ë¦­ì—ì„œ ê°€ì¥ ë§ì€ ì„ê³„ê°’ ìœ„ë°˜ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
                "í•´ë‹¹ ì˜ì—­ì˜ ìµœì í™”ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”."
            )

        if not recommendations:
            recommendations.append(
                "ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”."
            )

        return recommendations

    def save_monitoring_report(
        self, analysis_result: Dict[str, Any], filename: str = None
    ):
        """ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"post_deployment_monitoring_report_{timestamp}.json"

        report_path = f"deployment/reports/{filename}"

        # ë””ë ‰í† ë¦¬ ìƒì„±
        import os

        os.makedirs(os.path.dirname(report_path), exist_ok=True)

        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(analysis_result, f, indent=2, ensure_ascii=False)

        logger.info("monitoring_report_saved", filename=report_path)


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë°°í¬ í›„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")

    monitor = PostDeploymentMonitor()

    # ì§§ì€ í…ŒìŠ¤íŠ¸ ëª¨ë‹ˆí„°ë§ (ì‹¤ì œë¡œëŠ” 24ì‹œê°„)
    result = await monitor.start_post_deployment_monitoring(
        duration_hours=0.1
    )  # 6ë¶„ í…ŒìŠ¤íŠ¸

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ëª¨ë‹ˆí„°ë§ ê²°ê³¼:")
    print(f"   ëª¨ë‹ˆí„°ë§ ID: {result['monitoring_id']}")
    print(f"   ìƒíƒœ: {result['status']}")
    print(f"   ì‹œì‘ ì‹œê°„: {result['started_at']}")
    print(f"   ì™„ë£Œ ì‹œê°„: {result.get('completed_at', 'N/A')}")

    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    analysis = result.get("analysis", {})
    if analysis and "overall_performance_score" in analysis:
        score_info = analysis["overall_performance_score"]
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¶„ì„:")
        print(
            f"   ì „ì²´ ì ìˆ˜: {score_info.get('overall_score', 0)}/100 (ë“±ê¸‰: {score_info.get('grade', 'N/A')})"
        )

        component_scores = score_info.get("component_scores", {})
        for component, score in component_scores.items():
            print(f"   {component}: {score}/100")

    # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    recommendations = analysis.get("recommendations", [])
    if recommendations:
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")

    # ë¦¬í¬íŠ¸ ì €ì¥
    if analysis:
        monitor.save_monitoring_report(result)

    success = result["status"] == "completed"

    if success:
        print(f"\nâœ… ë°°í¬ í›„ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ!")
    else:
        print(f"\nâŒ ë°°í¬ í›„ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨!")
        if "error" in result:
            print(f"   ì˜¤ë¥˜: {result['error']}")

    return success


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)

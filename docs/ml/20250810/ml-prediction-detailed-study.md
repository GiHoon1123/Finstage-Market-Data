# ML ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - ê°œì¸ í•™ìŠµìš© ìƒì„¸ ì½”ë“œ í•´ì„¤

## ëª©ì°¨

1. [ì „ì²´ êµ¬ì¡° ì´í•´í•˜ê¸°](#ì „ì²´-êµ¬ì¡°-ì´í•´í•˜ê¸°)
2. [LSTM ëª¨ë¸ ì½”ë“œ ì™„ì „ ë¶„í•´](#lstm-ëª¨ë¸-ì½”ë“œ-ì™„ì „-ë¶„í•´)
3. [ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œ ì™„ì „ ë¶„í•´](#ë°ì´í„°-ì „ì²˜ë¦¬-ì½”ë“œ-ì™„ì „-ë¶„í•´)
4. [ì˜ˆì¸¡ ì‹¤í–‰ ì½”ë“œ ì™„ì „ ë¶„í•´](#ì˜ˆì¸¡-ì‹¤í–‰-ì½”ë“œ-ì™„ì „-ë¶„í•´)
5. [ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì½”ë“œ ë¶„ì„](#ì„œë¹„ìŠ¤-ë ˆì´ì–´-ì½”ë“œ-ë¶„ì„)
6. [API ë ˆì´ì–´ ì½”ë“œ ë¶„ì„](#api-ë ˆì´ì–´-ì½”ë“œ-ë¶„ì„)
7. [í—·ê°ˆë ¸ë˜ ë¶€ë¶„ë“¤ ì •ë¦¬](#í—·ê°ˆë ¸ë˜-ë¶€ë¶„ë“¤-ì •ë¦¬)
8. [ë””ë²„ê¹…í•  ë•Œ ìœ ìš©í•œ íŒë“¤](#ë””ë²„ê¹…í• -ë•Œ-ìœ ìš©í•œ-íŒë“¤)

---

## ì „ì²´ êµ¬ì¡° ì´í•´í•˜ê¸°

### ê° íŒŒì¼ì´ ë­˜ í•˜ëŠ” ê±´ì§€

ìš°ë¦¬ ì‹œìŠ¤í…œì€ ì´ë ‡ê²Œ êµ¬ì„±ë˜ì–´ ìˆì–´:

```
app/ml_prediction/
â”œâ”€â”€ ml/                          # ì‹¤ì œ ë¨¸ì‹ ëŸ¬ë‹ ë¡œì§
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ lstm_model.py        # LSTM ëª¨ë¸ ì •ì˜ (ë‡Œ)
â”‚   â”‚   â”œâ”€â”€ trainer.py           # ëª¨ë¸ í›ˆë ¨ (ì„ ìƒë‹˜)
â”‚   â”‚   â””â”€â”€ predictor.py         # ì˜ˆì¸¡ ì‹¤í–‰ (ì ìŸì´)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ preprocessor.py      # ë°ì´í„° ì „ì²˜ë¦¬ (ìš”ë¦¬ì‚¬)
â”‚   â”‚   â”œâ”€â”€ feature_engineer.py  # íŠ¹ì„± ìƒì„± (ì¬ë£Œ ì¤€ë¹„)
â”‚   â”‚   â””â”€â”€ source_manager.py    # ë°ì´í„° ìˆ˜ì§‘ (ì¥ë³´ê¸°)
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ evaluator.py         # ì„±ëŠ¥ í‰ê°€ (ì±„ì )
â”œâ”€â”€ service/
â”‚   â””â”€â”€ ml_prediction_service.py # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (ë§¤ë‹ˆì €)
â”œâ”€â”€ handler/
â”‚   â””â”€â”€ ml_prediction_handler.py # API ìš”ì²­ ì²˜ë¦¬ (ì ‘ìˆ˜ì›)
â””â”€â”€ web/route/
    â””â”€â”€ ml_prediction_router.py  # API ì—”ë“œí¬ì¸íŠ¸ (ì°½êµ¬)
```

### ë°ì´í„°ê°€ ì–´ë–»ê²Œ í˜ëŸ¬ê°€ëŠ”ì§€

**í›ˆë ¨ ê³¼ì •:**

```
1. ì‚¬ìš©ìê°€ "ëª¨ë¸ í›ˆë ¨í•´ì¤˜" ìš”ì²­
2. Routerê°€ ë°›ì•„ì„œ Handlerì—ê²Œ ì „ë‹¬
3. Handlerê°€ Serviceì—ê²Œ "í›ˆë ¨ ì‹œì‘" ëª…ë ¹
4. Serviceê°€ Trainerì—ê²Œ "í›ˆë ¨í•´" ì§€ì‹œ
5. Trainerê°€ Preprocessorì—ê²Œ "ë°ì´í„° ì¤€ë¹„í•´" ìš”ì²­
6. Preprocessorê°€ ë°ì´í„° ìˆ˜ì§‘í•˜ê³  ì •ë¦¬í•´ì„œ ì „ë‹¬
7. Trainerê°€ LSTM ëª¨ë¸ ë§Œë“¤ê³  í›ˆë ¨
8. í›ˆë ¨ ì™„ë£Œë˜ë©´ ëª¨ë¸ íŒŒì¼ë¡œ ì €ì¥
9. ê²°ê³¼ë¥¼ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ì„œ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µ
```

**ì˜ˆì¸¡ ê³¼ì •:**

```
1. ì‚¬ìš©ìê°€ "ê°€ê²© ì˜ˆì¸¡í•´ì¤˜" ìš”ì²­
2. Router â†’ Handler â†’ Service â†’ Predictor ìˆœì„œë¡œ ì „ë‹¬
3. Predictorê°€ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
4. ìµœê·¼ 60ì¼ ë°ì´í„° ìˆ˜ì§‘í•˜ê³  ì „ì²˜ë¦¬
5. ëª¨ë¸ì— ë„£ì–´ì„œ 7ì¼, 14ì¼, 30ì¼ í›„ ê°€ê²© ì˜ˆì¸¡
6. ì‹ ë¢°ë„ ê³„ì‚°í•˜ê³  ê²°ê³¼ ì •ë¦¬í•´ì„œ ì‘ë‹µ
```

---

## LSTM ëª¨ë¸ ì½”ë“œ ì™„ì „ ë¶„í•´

### MultiOutputLSTMPredictor í´ë˜ìŠ¤ ì „ì²´ êµ¬ì¡°

ì´ í´ë˜ìŠ¤ê°€ ìš°ë¦¬ ì‹œìŠ¤í…œì˜ í•µì‹¬ ë‡Œ ì—­í• ì„ í•´. í•œ ë²ˆì— 7ì¼, 14ì¼, 30ì¼ í›„ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” ë˜‘ë˜‘í•œ ë‡Œì•¼.

```python
class MultiOutputLSTMPredictor:
    def __init__(self, input_shape, config, model_name):
        # input_shape: (60, 152) - 60ì¼ì¹˜ ë°ì´í„°, ê° ë‚ ë§ˆë‹¤ 152ê°œ íŠ¹ì„±
        # config: ëª¨ë¸ ì„¤ì • (LSTM ìœ ë‹› ìˆ˜, ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ë“±)
        # model_name: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "IXIC_lstm")

        self.input_shape = input_shape
        self.config = config
        self.model_name = model_name
        self.target_days = [7, 14, 30]  # ì˜ˆì¸¡í•  ê¸°ê°„ë“¤

        # ì•„ì§ ëª¨ë¸ì€ ì—†ì–´, ë‚˜ì¤‘ì— build_multi_output_model()ì—ì„œ ë§Œë“¤ì–´
        self.model = None
        self.is_compiled = False  # ì»´íŒŒì¼ í–ˆëŠ”ì§€ ì²´í¬ìš©
```

### build_multi_output_model() ë©”ì„œë“œ í•œ ì¤„ì”© í•´ì„¤

ì´ ë©”ì„œë“œê°€ ì‹¤ì œë¡œ ì‹ ê²½ë§ì„ ë§Œë“œëŠ” ê³³ì´ì•¼. ì°¨ê·¼ì°¨ê·¼ ëœ¯ì–´ë³´ì:

```python
def build_multi_output_model(self) -> Model:
    # 1ë‹¨ê³„: ì…ë ¥ ë ˆì´ì–´ ë§Œë“¤ê¸°
    inputs = layers.Input(shape=self.input_shape, name="price_sequence_input")
    # â†’ ì¼€ë¼ìŠ¤ì—ì„œ ì…ë ¥ì„ ë°›ëŠ” ë¬¸ ê°™ì€ ê±°ì•¼
    # â†’ shape=(60, 152)ëŠ” "60ì¼ì¹˜ ë°ì´í„°, ê° ë‚ ë§ˆë‹¤ 152ê°œ ìˆ«ì"ë¼ëŠ” ëœ»
    # â†’ nameì€ ë‚˜ì¤‘ì— ëª¨ë¸ êµ¬ì¡° ë³¼ ë•Œ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì´ë¦„ ë¶™ì¸ ê±°

    # 2ë‹¨ê³„: LSTM ë ˆì´ì–´ë“¤ ìŒ“ê¸°
    x = inputs  # xëŠ” í˜„ì¬ ë°ì´í„°ê°€ í˜ëŸ¬ê°€ëŠ” íŒŒì´í”„ë¼ê³  ìƒê°í•´

    # self.config.lstm_units = [50, 50] ì´ë¼ê³  ê°€ì •í•˜ì
    for i, units in enumerate(self.config.lstm_units):
        # i=0ì¼ ë•Œ: units=50 (ì²« ë²ˆì§¸ LSTM)
        # i=1ì¼ ë•Œ: units=50 (ë‘ ë²ˆì§¸ LSTM)

        # ì´ ë¶€ë¶„ì´ í•µì‹¬! ë§ˆì§€ë§‰ LSTMì¸ì§€ í™•ì¸
        return_sequences = i < len(self.config.lstm_units) - 1
        # â†’ i=0ì¼ ë•Œ: 0 < 2-1 = True (ì‹œí€€ìŠ¤ ì „ì²´ ë°˜í™˜)
        # â†’ i=1ì¼ ë•Œ: 1 < 2-1 = False (ë§ˆì§€ë§‰ ê°’ë§Œ ë°˜í™˜)

        x = layers.LSTM(
            units=units,                    # LSTM ë‰´ëŸ° ê°œìˆ˜ (50ê°œ)
            return_sequences=return_sequences,  # ìœ„ì—ì„œ ê³„ì‚°í•œ ê°’
            dropout=0.2,                    # 20% ë‰´ëŸ° ëœë¤í•˜ê²Œ ë„ê¸° (ê³¼ì í•© ë°©ì§€)
            recurrent_dropout=0.2,          # LSTM ë‚´ë¶€ ì—°ê²°ë„ 20% ë„ê¸°
            name=f"lstm_{i+1}",            # "lstm_1", "lstm_2" ì´ëŸ° ì‹ìœ¼ë¡œ ì´ë¦„
        )(x)

        # ë°°ì¹˜ ì •ê·œí™” ì¶”ê°€ (í›ˆë ¨ ì•ˆì •í™”)
        x = layers.BatchNormalization(name=f"batch_norm_lstm_{i+1}")(x)
        # â†’ ê° ë°°ì¹˜ë§ˆë‹¤ ë°ì´í„°ë¥¼ ì •ê·œí™”í•´ì„œ í›ˆë ¨ì´ ë” ì•ˆì •ì ì´ ë˜ê²Œ í•´
```

    # 3ë‹¨ê³„: ê³µí†µ Dense ë ˆì´ì–´ë“¤
    # self.config.dense_units = [25] ë¼ê³  ê°€ì •
    for i, units in enumerate(self.config.dense_units):
        x = layers.Dense(
            units=units,                    # 25ê°œ ë‰´ëŸ°
            activation=self.config.activation,  # "relu" í™œì„±í™” í•¨ìˆ˜
            name=f"dense_common_{i+1}",    # "dense_common_1"
        )(x)

        x = layers.Dropout(
            self.config.dropout_rate,      # 0.2 (20% ë“œë¡­ì•„ì›ƒ)
            name=f"dropout_common_{i+1}"
        )(x)
        # â†’ ì—¬ê¸°ê¹Œì§€ê°€ ê³µí†µ íŠ¹ì„± ì¶”ì¶œ ë¶€ë¶„ì´ì•¼
        # â†’ ëª¨ë“  ì˜ˆì¸¡ ê¸°ê°„ì´ ì´ íŠ¹ì„±ì„ ê³µìœ í•´

    # 4ë‹¨ê³„: ê° íƒ€ì„í”„ë ˆì„ë³„ ì¶œë ¥ ë¶„ê¸° (í•µì‹¬!)
    output_layers = []

    for days in self.target_days:  # [7, 14, 30]
        # ê° ê¸°ê°„ë³„ë¡œ ì „ìš© ë ˆì´ì–´ ë§Œë“¤ê¸°
        branch = layers.Dense(
            units=16,                       # 16ê°œ ë‰´ëŸ° (ê° ê¸°ê°„ë³„ íŠ¹í™”)
            activation=self.config.activation,  # "relu"
            name=f"dense_{days}d_branch",   # "dense_7d_branch" ì´ëŸ° ì‹
        )(x)

        branch = layers.Dropout(
            self.config.dropout_rate,
            name=f"dropout_{days}d_branch"
        )(branch)

        # ìµœì¢… ì¶œë ¥ ë ˆì´ì–´ (ê°€ê²© 1ê°œ ì˜ˆì¸¡)
        output = layers.Dense(
            units=1,                        # 1ê°œ ë‰´ëŸ° (ê°€ê²© 1ê°œ)
            activation="linear",            # ì„ í˜• í™œì„±í™” (ì‹¤ì œ ê°€ê²© ê°’)
            name=f"price_prediction_{days}d"  # "price_prediction_7d"
        )(branch)

        output_layers.append(output)
        # â†’ ì´ë ‡ê²Œ í•´ì„œ 3ê°œ ì¶œë ¥ì´ ë§Œë“¤ì–´ì ¸: [7ì¼ì˜ˆì¸¡, 14ì¼ì˜ˆì¸¡, 30ì¼ì˜ˆì¸¡]

    # 5ë‹¨ê³„: ìµœì¢… ëª¨ë¸ ìƒì„±
    self.model = Model(
        inputs=inputs,           # ì…ë ¥: (60, 152) í˜•íƒœ
        outputs=output_layers,   # ì¶œë ¥: 3ê°œ (7d, 14d, 30d)
        name=self.model_name     # ëª¨ë¸ ì´ë¦„
    )

    return self.model

# ğŸ¤” ì™œ ì´ë ‡ê²Œ ë³µì¡í•˜ê²Œ ë§Œë“¤ì—ˆì„ê¹Œ?

# 1. ê³µí†µ LSTM: ì‹œê³„ì—´ íŒ¨í„´ì€ ëª¨ë“  ê¸°ê°„ì´ ê³µìœ 

# 2. ë¶„ê¸° Dense: ê° ê¸°ê°„ë³„ íŠ¹ì„±ì€ ë”°ë¡œ í•™ìŠµ

# 3. ë©€í‹° ì•„ì›ƒí’‹: í•œ ë²ˆì— 3ê°œ ê¸°ê°„ ì˜ˆì¸¡ìœ¼ë¡œ íš¨ìœ¨ì„± UP

```

```

### compile_model() ë©”ì„œë“œ ìƒì„¸ ë¶„ì„

ëª¨ë¸ì„ ë§Œë“¤ì—ˆìœ¼ë©´ ì´ì œ "ì–´ë–»ê²Œ í•™ìŠµí• ì§€" ì„¤ì •í•´ì•¼ í•´. ì´ê²Œ ì»´íŒŒì¼ì´ì•¼.

```python
def compile_model(self, optimizer=None, loss=None, metrics=None, loss_weights=None):
    if self.model is None:
        raise ValueError("Model must be built before compilation")
        # â†’ ëª¨ë¸ ë¨¼ì € ë§Œë“¤ê³  ì»´íŒŒì¼í•˜ë¼ëŠ” ëœ»

    # ê¸°ë³¸ê°’ ì„¤ì •
    if optimizer is None:
        optimizer = Adam(learning_rate=0.001)
        # â†’ Adam: ê°€ì¥ ë§ì´ ì“°ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜
        # â†’ learning_rate=0.001: í•™ìŠµ ì†ë„ (ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦¼)

    if loss is None:
        loss = "mse"  # Mean Squared Error
        # â†’ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì°¨ì´ì˜ ì œê³± í‰ê· 
        # â†’ ê°€ê²© ì˜ˆì¸¡ì— ê°€ì¥ ì í•©í•œ ì†ì‹¤ í•¨ìˆ˜

    if metrics is None:
        metrics = ["mae"]  # Mean Absolute Error
        # â†’ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì°¨ì´ì˜ ì ˆëŒ“ê°’ í‰ê· 
        # â†’ ì‹¤ì œ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆì–´

    if loss_weights is None:
        loss_weights = self.config.loss_weights  # [0.5, 0.3, 0.2]
        # â†’ 7ì¼ ì˜ˆì¸¡: 50% ì¤‘ìš”ë„
        # â†’ 14ì¼ ì˜ˆì¸¡: 30% ì¤‘ìš”ë„
        # â†’ 30ì¼ ì˜ˆì¸¡: 20% ì¤‘ìš”ë„
        # â†’ ë‹¨ê¸° ì˜ˆì¸¡ì´ ë” ì •í™•í•˜ê³  ì‹¤ìš©ì ì´ë¼ì„œ ê°€ì¤‘ì¹˜ ë†’ê²Œ

    # ë©€í‹° ì•„ì›ƒí’‹ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    if isinstance(loss, str):
        # ëª¨ë“  ì¶œë ¥ì— ë™ì¼í•œ ì†ì‹¤ í•¨ìˆ˜ ì ìš©
        loss_dict = {f"price_prediction_{days}d": loss for days in self.target_days}
        # â†’ {"price_prediction_7d": "mse",
        #    "price_prediction_14d": "mse",
        #    "price_prediction_30d": "mse"}

    if isinstance(metrics, list):
        # ëª¨ë“  ì¶œë ¥ì— ë™ì¼í•œ ì§€í‘œ ì ìš©
        metrics_dict = {f"price_prediction_{days}d": metrics for days in self.target_days}
        # â†’ {"price_prediction_7d": ["mae"],
        #    "price_prediction_14d": ["mae"],
        #    "price_prediction_30d": ["mae"]}

    # ì‹¤ì œ ì»´íŒŒì¼
    self.model.compile(
        optimizer=optimizer,      # Adam ìµœì í™”
        loss=loss_dict,          # ê° ì¶œë ¥ë³„ MSE ì†ì‹¤
        metrics=metrics_dict,    # ê° ì¶œë ¥ë³„ MAE ì§€í‘œ
        loss_weights=loss_weights # [0.5, 0.3, 0.2] ê°€ì¤‘ì¹˜
    )

    self.is_compiled = True  # ì»´íŒŒì¼ ì™„ë£Œ í‘œì‹œ

# ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:
# - ë©€í‹° ì•„ì›ƒí’‹ì´ë¼ì„œ ì†ì‹¤ í•¨ìˆ˜ë„ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •
# - ê°€ì¤‘ì¹˜ë¡œ ë‹¨ê¸° ì˜ˆì¸¡ì— ë” ì§‘ì¤‘
# - Adam + MSE + MAE ì¡°í•©ì€ ê°€ê²© ì˜ˆì¸¡ì˜ í™©ê¸ˆ ì¡°í•©
```

### train() ë©”ì„œë“œ ë™ì‘ ì›ë¦¬

ì´ì œ ì‹¤ì œë¡œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë¶€ë¶„ì´ì•¼. ì—¬ê¸°ê°€ ì œì¼ ë³µì¡í•´.

```python
def train(self, X_train, y_train, X_val=None, y_val=None, epochs=None, batch_size=None):
    if not self.is_compiled:
        raise ValueError("Model must be compiled before training")
        # â†’ ì»´íŒŒì¼ ë¨¼ì € í•˜ê³  í›ˆë ¨í•˜ë¼ëŠ” ëœ»

    # ê¸°ë³¸ê°’ ì„¤ì •
    epochs = epochs or ml_settings.training.epochs  # 100
    batch_size = batch_size or ml_settings.training.batch_size  # 32

    # âš ï¸ ì—¬ê¸°ê°€ í•µì‹¬! ë©€í‹° ì•„ì›ƒí’‹ ë°ì´í„° ë³€í™˜
    # y_trainì€ ë”•ì…”ë„ˆë¦¬: {"7d": [ê°€ê²©ë“¤], "14d": [ê°€ê²©ë“¤], "30d": [ê°€ê²©ë“¤]}
    # ì¼€ë¼ìŠ¤ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì›í•´: [7ì¼ê°€ê²©ë“¤, 14ì¼ê°€ê²©ë“¤, 30ì¼ê°€ê²©ë“¤]

    y_train_list = [y_train[f"{days}d"] for days in self.target_days]
    # â†’ [y_train["7d"], y_train["14d"], y_train["30d"]]
    # â†’ ì´ ìˆœì„œê°€ ëª¨ë¸ ì¶œë ¥ ìˆœì„œì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•´!

    validation_data = None
    if X_val is not None and y_val is not None:
        y_val_list = [y_val[f"{days}d"] for days in self.target_days]
        validation_data = (X_val, y_val_list)
        # â†’ ê²€ì¦ ë°ì´í„°ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë³€í™˜

    # ê¸°ë³¸ ì½œë°± ì„¤ì • (í›ˆë ¨ ì¤‘ ìë™ ì œì–´)
    callbacks_list = self._create_default_callbacks()
    # â†’ ì¡°ê¸° ì¢…ë£Œ, í•™ìŠµë¥  ê°ì†Œ ë“±

    # ì‹¤ì œ í›ˆë ¨ ì‹œì‘!
    history = self.model.fit(
        X_train,              # ì…ë ¥: (ìƒ˜í”Œìˆ˜, 60, 152)
        y_train_list,         # ì¶œë ¥: [7ì¼íƒ€ê²Ÿ, 14ì¼íƒ€ê²Ÿ, 30ì¼íƒ€ê²Ÿ]
        validation_data=validation_data,  # ê²€ì¦ ë°ì´í„°
        epochs=epochs,        # ìµœëŒ€ 100ë²ˆ ë°˜ë³µ
        batch_size=batch_size, # í•œ ë²ˆì— 32ê°œì”© ì²˜ë¦¬
        callbacks=callbacks_list, # ìë™ ì œì–´ ê¸°ëŠ¥ë“¤
        verbose=1,            # ì§„í–‰ìƒí™© ì¶œë ¥
        shuffle=True,         # ë°ì´í„° ì„ê¸° (ì‹œê³„ì—´ì´ì§€ë§Œ ìœˆë„ìš° ë‹¨ìœ„ë¡œëŠ” OK)
    )

    self.training_history = history  # í›ˆë ¨ ê¸°ë¡ ì €ì¥

    return training_metadata  # í›ˆë ¨ ê²°ê³¼ ì •ë³´ ë°˜í™˜

# ğŸ”¥ í›ˆë ¨ ê³¼ì • ìƒì„¸:
# 1. 32ê°œ ìƒ˜í”Œì”© ëª¨ë¸ì— ë„£ì–´
# 2. ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµí•´ì„œ ì˜¤ì°¨ ê³„ì‚°
# 3. ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
# 4. ì´ê±¸ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ë°˜ë³µ (1 ì—í¬í¬)
# 5. ì—í¬í¬ë¥¼ 100ë²ˆ ë°˜ë³µí•˜ê±°ë‚˜ ì¡°ê¸° ì¢…ë£Œë  ë•Œê¹Œì§€ ê³„ì†
```

---

## ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œ ì™„ì „ ë¶„í•´

### MLDataPreprocessor í´ë˜ìŠ¤ ì—­í• 

ì´ í´ë˜ìŠ¤ëŠ” ìš”ë¦¬ì‚¬ ê°™ì€ ì—­í• ì´ì•¼. ë‚ ê²ƒì˜ ì£¼ì‹ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ë¨¹ì„ ìˆ˜ ìˆê²Œ ìš”ë¦¬í•´ì¤˜.

```python
class MLDataPreprocessor:
    def __init__(self, cache_dir=None, enable_caching=True, strict_validation=False):
        # cache_dir: ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥í•  í´ë”
        # enable_caching: ìºì‹± ì‚¬ìš©í• ì§€ (Trueë©´ ë¹¨ë¼ì ¸)
        # strict_validation: ì—„ê²©í•œ ê²€ì¦ ëª¨ë“œ

        self.cache_dir = cache_dir or "models/ml_prediction/cache"
        self.enable_caching = enable_caching
        self.strict_validation = strict_validation

        # í•˜ìœ„ ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”
        self.data_source_manager = DataSourceManager()    # ë°ì´í„° ìˆ˜ì§‘ê¸°
        self.feature_engineer = FeatureEngineer()         # íŠ¹ì„± ìƒì„±ê¸°
        self.quality_validator = DataQualityValidator()   # í’ˆì§ˆ ê²€ì‚¬ê¸°
        self.fallback_handler = DataSourceFallbackHandler()  # ì˜¤ë¥˜ ì²˜ë¦¬ê¸°
```

### prepare_training_data() ë©”ì„œë“œ ë‹¨ê³„ë³„ í•´ì„¤

ì´ ë©”ì„œë“œê°€ ì „ì²´ ë°ì´í„° ì „ì²˜ë¦¬ì˜ í•µì‹¬ì´ì•¼. ë‹¨ê³„ë³„ë¡œ ëœ¯ì–´ë³´ì:

```python
def prepare_training_data(self, symbol, start_date, end_date, force_refresh=False):
    # symbol: "^IXIC" (ë‚˜ìŠ¤ë‹¥) ë˜ëŠ” "^GSPC" (S&P500)
    # start_date, end_date: ë°ì´í„° ê¸°ê°„
    # force_refresh: ìºì‹œ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ë§Œë“¤ì§€

    # 1ë‹¨ê³„: ìºì‹œ í™•ì¸ (24ì‹œê°„ ìœ íš¨)
    if self.enable_caching and not force_refresh:
        cached_data = self._load_from_cache(symbol, start_date, end_date, "training")
        if cached_data is not None:
            logger.info("training_data_loaded_from_cache")
            return cached_data
            # â†’ ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜ (ì‹œê°„ ì ˆì•½!)

    # 2ë‹¨ê³„: ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘
    raw_data = self._collect_raw_data(symbol, start_date, end_date)
    # â†’ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ OHLCV ë°ì´í„° + ê¸°ìˆ ì  ì§€í‘œë“¤ ê°€ì ¸ì™€
    # â†’ ê²°ê³¼: DataFrame with columns [open, high, low, close, volume, sma_5, rsi_14, ...]

    # 3ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    validation_passed, validation_results, quality_score = \
        self._validate_data_quality(raw_data, symbol, (start_date, end_date))

    if not validation_passed:
        raise ValueError(f"Data quality validation failed for {symbol}")
        # â†’ ë°ì´í„°ê°€ ì´ìƒí•˜ë©´ ì—¬ê¸°ì„œ ë©ˆì¶° (ì“°ë ˆê¸° ë„£ìœ¼ë©´ ì“°ë ˆê¸° ë‚˜ì™€)

    # 4ë‹¨ê³„: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (152ê°œ ê¸°ìˆ ì  ì§€í‘œ ìƒì„±)
    X, y_dict, feature_names = self.feature_engineer.create_multi_target_sequences(
        raw_data, target_column="close"
    )
    # â†’ X: (ìƒ˜í”Œìˆ˜, 60, 152) - 60ì¼ ìœˆë„ìš°, 152ê°œ íŠ¹ì„±
    # â†’ y_dict: {"7d": [...], "14d": [...], "30d": [...]} - ê° ê¸°ê°„ë³„ íƒ€ê²Ÿ
    # â†’ feature_names: ["sma_5", "rsi_14", ...] - íŠ¹ì„± ì´ë¦„ë“¤

    # 5ë‹¨ê³„: ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§)
    X_normalized = self.feature_engineer.normalize_features(X, fit_scaler=True)
    y_normalized_dict = self.feature_engineer.normalize_targets(y_dict, fit_scalers=True)
    # â†’ fit_scaler=True: ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ìƒˆë¡œ ë§Œë“¤ì–´ (í›ˆë ¨ ì‹œì—ë§Œ)
    # â†’ ì˜ˆì¸¡í•  ë•ŒëŠ” ì´ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì¬ì‚¬ìš©í•´ì•¼ í•´

    # 6ë‹¨ê³„: ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€!)
    X_splits, y_splits = self.feature_engineer.split_data(X_normalized, y_normalized_dict)
    # â†’ X_splits: {"train": [...], "val": [...], "test": [...]}
    # â†’ y_splits: {"7d": {"train": [...], "val": [...], "test": [...]}, ...}
    # â†’ âš ï¸ ì‹œê³„ì—´ì´ë¼ì„œ ëœë¤ ì…”í”Œ ì•ˆ í•´! ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë¶„í• 

    # 7ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = self._create_metadata(symbol, start_date, end_date, raw_data,
                                   validation_results, quality_score, feature_names)
    # â†’ ë‚˜ì¤‘ì— ì°¸ê³ í•  ì •ë³´ë“¤ (ë°ì´í„° í¬ê¸°, í’ˆì§ˆ ì ìˆ˜, íŠ¹ì„± ì´ë¦„ ë“±)

    # 8ë‹¨ê³„: ìºì‹œ ì €ì¥ (ë‹¤ìŒì— ë¹¨ë¦¬ ì“°ë ¤ê³ )
    if self.enable_caching:
        self._save_to_cache((X_splits, y_splits, metadata),
                          symbol, start_date, end_date, "training")

    return X_splits, y_splits, metadata

# ğŸ’¡ ì „ì²´ ê³¼ì • ìš”ì•½:
# ë‚ ê²ƒ ë°ì´í„° â†’ í’ˆì§ˆê²€ì‚¬ â†’ íŠ¹ì„±ìƒì„± â†’ ì •ê·œí™” â†’ ë¶„í•  â†’ ì™„ì„±!
# ë§ˆì¹˜ ì¬ë£Œ â†’ ì”»ê¸° â†’ ì°ê¸° â†’ ì–‘ë… â†’ ë‚˜ëˆ„ê¸° â†’ ìš”ë¦¬ì™„ì„± ê°™ì€ ëŠë‚Œ
```

### ìºì‹± ì‹œìŠ¤í…œ ë™ì‘ ì›ë¦¬

ìºì‹±ì€ í•œ ë²ˆ ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ ì €ì¥í•´ë’€ë‹¤ê°€ ì¬ì‚¬ìš©í•˜ëŠ” ê±°ì•¼. ì—„ì²­ ì‹œê°„ ì ˆì•½ë¼!

```python
def _save_to_cache(self, data, symbol, start_date, end_date, data_type):
    if not self.enable_caching:
        return  # ìºì‹± ì•ˆ ì“°ë©´ ê·¸ëƒ¥ ë¦¬í„´

    # ìºì‹œ í‚¤ ìƒì„± (íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©)
    cache_key = self._generate_cache_key(symbol, start_date, end_date, data_type)
    # â†’ "IXIC_2020-01-01_2024-12-31_training_60" ì´ëŸ° ì‹
    # â†’ ì‹¬ë³¼, ë‚ ì§œ, íƒ€ì…, ìœˆë„ìš°í¬ê¸°ë¡œ ìœ ë‹ˆí¬í•˜ê²Œ ë§Œë“¤ì–´

    cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
    # â†’ "models/ml_prediction/cache/abc123.pkl" ê²½ë¡œ

    cache_data = {
        "data": data,           # ì‹¤ì œ ì²˜ë¦¬ëœ ë°ì´í„°
        "metadata": {
            "symbol": symbol,
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "data_type": data_type,
            "created_at": datetime.now().isoformat(),
            "feature_engineer_config": self.feature_engineer.get_feature_info(),
        },
    }

    # pickleë¡œ ì €ì¥ (íŒŒì´ì¬ ê°ì²´ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë°©ë²•)
    with open(cache_file, "wb") as f:
        pickle.dump(cache_data, f)
    # â†’ ì´ì œ ë‹¤ìŒì— ê°™ì€ ìš”ì²­ ì˜¤ë©´ ì´ íŒŒì¼ ì½ì–´ì„œ ë°”ë¡œ ë°˜í™˜

def _load_from_cache(self, symbol, start_date, end_date, data_type):
    cache_key = self._generate_cache_key(symbol, start_date, end_date, data_type)
    cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")

    if not os.path.exists(cache_file):
        return None  # ìºì‹œ íŒŒì¼ ì—†ìœ¼ë©´ None ë°˜í™˜

    # íŒŒì¼ ë‚˜ì´ í™•ì¸ (24ì‹œê°„ ì´ìƒ ëœ ìºì‹œëŠ” ë¬´íš¨)
    file_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(cache_file))
    if file_age > timedelta(hours=24):
        os.remove(cache_file)  # ì˜¤ë˜ëœ ìºì‹œ ì‚­ì œ
        return None

    # ìºì‹œ íŒŒì¼ ì½ê¸°
    with open(cache_file, "rb") as f:
        cache_data = pickle.load(f)

    # ì„¤ì • í˜¸í™˜ì„± í™•ì¸ (ìœˆë„ìš° í¬ê¸°ë‚˜ íƒ€ê²Ÿ ê¸°ê°„ì´ ë°”ë€Œì—ˆë‚˜?)
    cached_config = cache_data["metadata"].get("feature_engineer_config", {})
    current_config = self.feature_engineer.get_feature_info()

    if cached_config.get("window_size") != current_config.get("window_size"):
        return None  # ì„¤ì •ì´ ë°”ë€Œì—ˆìœ¼ë©´ ìºì‹œ ë¬´íš¨

    return cache_data["data"]  # ìºì‹œëœ ë°ì´í„° ë°˜í™˜

# ğŸš€ ìºì‹±ì˜ ì¥ì :
# - ì²« ë²ˆì§¸: 25ë…„ ë°ì´í„° ì²˜ë¦¬ (30ì´ˆ) â†’ ë‘ ë²ˆì§¸: ìºì‹œ ë¡œë“œ (0.1ì´ˆ)
# - ê°™ì€ ê¸°ê°„ ë°ì´í„° ìš”ì²­í•˜ë©´ ê±°ì˜ ì¦‰ì‹œ ì‘ë‹µ
# - ë””ìŠ¤í¬ ìš©ëŸ‰ì€ ì¢€ ì“°ì§€ë§Œ ì‹œê°„ì€ ì—„ì²­ ì ˆì•½
```

### íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì½”ë“œ ë¶„ì„

ì´ ë¶€ë¶„ì´ 152ê°œ ê¸°ìˆ ì  ì§€í‘œë¥¼ ë§Œë“œëŠ” í•µì‹¬ì´ì•¼:

```python
def create_multi_target_sequences(self, data, target_column="close"):
    # 1ë‹¨ê³„: ê¸°ìˆ ì  ì§€í‘œ ìƒì„± (152ê°œ íŠ¹ì„±)
    feature_data = self._prepare_features(data)
    # â†’ ì›ë³¸ OHLCV ë°ì´í„°ì—ì„œ SMA, RSI, MACD ë“±ë“± ê³„ì‚°
    # â†’ ê²°ê³¼: DataFrame with 152 columns

    sequences = []  # ì…ë ¥ ì‹œí€€ìŠ¤ë“¤ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    targets = {f"{days}d": [] for days in self.target_days}  # íƒ€ê²Ÿë“¤

    # 2ë‹¨ê³„: 60ì¼ ìœˆë„ìš°ë¡œ ì‹œí€€ìŠ¤ ìƒì„±
    for i in range(self.window_size, len(feature_data)):
        # i=60ì¼ ë•Œ: 0~59ì¼ ë°ì´í„°ë¡œ 60ì¼ ì˜ˆì¸¡
        # i=61ì¼ ë•Œ: 1~60ì¼ ë°ì´í„°ë¡œ 61ì¼ ì˜ˆì¸¡
        # ...

        # ì…ë ¥: ê³¼ê±° 60ì¼ íŠ¹ì„± ë°ì´í„°
        sequence = feature_data.iloc[i-self.window_size:i].values
        # â†’ shape: (60, 152) - 60ì¼ Ã— 152ê°œ íŠ¹ì„±
        sequences.append(sequence)

        # íƒ€ê²Ÿ: ë¯¸ë˜ 7ì¼, 14ì¼, 30ì¼ í›„ ê°€ê²©
        for days in self.target_days:
            future_idx = min(i + days, len(data) - 1)  # ë²”ìœ„ ë²—ì–´ë‚˜ì§€ ì•Šê²Œ
            future_price = data[target_column].iloc[future_idx]
            targets[f"{days}d"].append(future_price)

    # 3ë‹¨ê³„: numpy ë°°ì—´ë¡œ ë³€í™˜
    X = np.array(sequences)  # (ìƒ˜í”Œìˆ˜, 60, 152)
    y_dict = {k: np.array(v) for k, v in targets.items()}
    # â†’ {"7d": [ê°€ê²©ë“¤], "14d": [ê°€ê²©ë“¤], "30d": [ê°€ê²©ë“¤]}

    return X, y_dict, feature_names

# ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´:
# - ê³¼ê±° 60ì¼ ë°ì´í„°ë¡œ ë¯¸ë˜ ì—¬ëŸ¬ ê¸°ê°„ ì˜ˆì¸¡
# - ìŠ¬ë¼ì´ë”© ìœˆë„ìš°: í•˜ë£¨ì”© ë°€ë©´ì„œ ìƒ˜í”Œ ìƒì„±
# - ë©€í‹° íƒ€ê²Ÿ: í•œ ë²ˆì— ì—¬ëŸ¬ ê¸°ê°„ íƒ€ê²Ÿ ë§Œë“¤ê¸°
```

---

## ì˜ˆì¸¡ ì‹¤í–‰ ì½”ë“œ ì™„ì „ ë¶„í•´

### MultiTimeframePredictor í´ë˜ìŠ¤ êµ¬ì¡°

ì´ í´ë˜ìŠ¤ëŠ” ì ìŸì´ ì—­í• ì´ì•¼. í›ˆë ¨ëœ ëª¨ë¸ì„ ê°€ì§€ê³  ì‹¤ì œë¡œ ë¯¸ë˜ ê°€ê²©ì„ ì˜ˆì¸¡í•´.

```python
class MultiTimeframePredictor:
    def __init__(self, preprocessor=None, confidence_method=None):
        self.preprocessor = preprocessor or MLDataPreprocessor()
        self.confidence_method = confidence_method or "ensemble"

        # ë¡œë“œëœ ëª¨ë¸ë“¤ ìºì‹œ (ê°™ì€ ëª¨ë¸ ì—¬ëŸ¬ ë²ˆ ë¡œë“œ ì•ˆ í•˜ë ¤ê³ )
        self.loaded_models = {}  # {"IXIC_active": (model, entity), ...}

        # ì˜ˆì¸¡ ì„¸ì…˜ ì •ë³´
        self.current_predictions = {}  # ì˜ˆì¸¡ ê²°ê³¼ ì„ì‹œ ì €ì¥
```

### predict_price() ë©”ì„œë“œ ë‹¨ê³„ë³„ í•´ì„¤

ì‹¤ì œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œì•¼:

```python
def predict_price(self, symbol, prediction_date=None, model_version=None):
    prediction_date = prediction_date or date.today()  # ê¸°ë³¸ê°’: ì˜¤ëŠ˜
    batch_id = str(uuid.uuid4())  # ê³ ìœ  ID ìƒì„±

    # 1ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ (ìºì‹œ í™œìš©)
    model_predictor, model_entity = self._load_model(symbol, model_version)
    # â†’ ì €ì¥ëœ .keras íŒŒì¼ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë“¤ ë¡œë“œ
    # â†’ ìºì‹œì— ìˆìœ¼ë©´ ë°”ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ë¡œë“œ

    # 2ë‹¨ê³„: ì˜ˆì¸¡ìš© ë°ì´í„° ì¤€ë¹„ (ìµœê·¼ 60ì¼)
    X_pred, data_metadata = self.preprocessor.prepare_prediction_data(
        symbol=symbol, end_date=prediction_date
    )
    # â†’ ìµœê·¼ 60ì¼ ë°ì´í„° ìˆ˜ì§‘í•˜ê³  152ê°œ íŠ¹ì„± ê³„ì‚°
    # â†’ ì •ê·œí™”ê¹Œì§€ ì™„ë£Œëœ ìƒíƒœ: shape (1, 60, 152)

    # 3ë‹¨ê³„: ì˜ˆì¸¡ ì‹¤í–‰
    raw_predictions = model_predictor.predict(X_pred)
    # â†’ LSTM ëª¨ë¸ì— ë„£ì–´ì„œ ì˜ˆì¸¡
    # â†’ ê²°ê³¼: [7ì¼ì˜ˆì¸¡, 14ì¼ì˜ˆì¸¡, 30ì¼ì˜ˆì¸¡] (ì •ê·œí™”ëœ ê°’)

    # 4ë‹¨ê³„: ì—­ì •ê·œí™” (ì‹¤ì œ ê°€ê²©ìœ¼ë¡œ ë³µì›)
    denormalized_predictions = \
        self.preprocessor.feature_engineer.denormalize_predictions(raw_predictions)
    # â†’ 0~1 ë²”ìœ„ ê°’ì„ ì‹¤ì œ ê°€ê²©ìœ¼ë¡œ ë³€í™˜
    # â†’ ê²°ê³¼: {"7d": [15234.56], "14d": [15456.78], "30d": [15123.45]}

    # 5ë‹¨ê³„: ì‹ ë¢°ë„ ê³„ì‚°
    confidence_scores = self._calculate_confidence_scores(
        model_predictor, X_pred, raw_predictions
    )
    # â†’ ì•™ìƒë¸” ë°©ë²•ìœ¼ë¡œ ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
    # â†’ ê²°ê³¼: {"7d": 0.75, "14d": 0.68, "30d": 0.62}

    # 6ë‹¨ê³„: ì˜ˆì¸¡ ê²°ê³¼ êµ¬ì„±
    current_price = data_metadata.get("last_price", 0.0)  # í˜„ì¬ ê°€ê²©
    prediction_results = []

    for timeframe, predicted_price in denormalized_predictions.items():
        days = int(timeframe.replace("d", ""))  # "7d" â†’ 7
        target_date = prediction_date + timedelta(days=days)  # ì˜ˆì¸¡ ëŒ€ìƒ ë‚ ì§œ

        # ê°€ê²© ë³€í™”ìœ¨ ê³„ì‚°
        price_change_percent = ((predicted_price[0] - current_price) / current_price) * 100

        # ì˜ˆì¸¡ ë°©í–¥ ê²°ì •
        if price_change_percent > 0.5:
            predicted_direction = "up"      # 0.5% ì´ìƒ ìƒìŠ¹
        elif price_change_percent < -0.5:
            predicted_direction = "down"    # 0.5% ì´ìƒ í•˜ë½
        else:
            predicted_direction = "neutral" # ê·¸ ì‚¬ì´ëŠ” ì¤‘ë¦½

        prediction_results.append({
            "timeframe": timeframe,                    # "7d"
            "target_date": target_date,                # 2025-08-16
            "predicted_price": float(predicted_price[0]), # 15234.56
            "predicted_direction": predicted_direction,    # "up"
            "price_change_percent": float(price_change_percent), # 1.56
            "confidence_score": confidence_scores.get(timeframe, 0.5), # 0.75
        })

    # 7ë‹¨ê³„: ì˜ˆì¸¡ ì¼ê´€ì„± ê²€ì¦
    consistency_score = self._calculate_consistency_score(prediction_results)
    # â†’ ê¸°ê°„ë³„ ì˜ˆì¸¡ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ì„± ìˆëŠ”ì§€ ì²´í¬
    # â†’ ì˜ˆ: 7ì¼ì€ ìƒìŠ¹, 14ì¼ì€ í•˜ë½, 30ì¼ì€ ìƒìŠ¹ â†’ ì¼ê´€ì„± ë‚®ìŒ

    return {
        "status": "success",
        "symbol": symbol,
        "current_price": current_price,
        "predictions": prediction_results,
        "consistency_score": consistency_score,
        "batch_id": batch_id
    }

# ğŸ¯ ì „ì²´ ê³¼ì • ìš”ì•½:
# ëª¨ë¸ë¡œë“œ â†’ ë°ì´í„°ì¤€ë¹„ â†’ ì˜ˆì¸¡ì‹¤í–‰ â†’ ì—­ì •ê·œí™” â†’ ì‹ ë¢°ë„ê³„ì‚° â†’ ê²°ê³¼êµ¬ì„±
```

### ì‹ ë¢°ë„ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ ë¶„ì„

ì´ ë¶€ë¶„ì´ ì •ë§ ë˜‘ë˜‘í•œ ë¶€ë¶„ì´ì•¼. ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ë¯¿ì„ ë§Œí•œì§€ ìˆ˜ì¹˜ë¡œ ê³„ì‚°í•´:

```python
def _calculate_confidence_scores(self, model_predictor, X_pred, predictions):
    confidence_scores = {}

    if self.confidence_method == "ensemble":
        # ì•™ìƒë¸” ë°©ë²•: ì—¬ëŸ¬ ë²ˆ ì˜ˆì¸¡í•˜ì—¬ ë¶„ì‚° ê³„ì‚°
        confidence_scores = self._calculate_ensemble_confidence(
            model_predictor, X_pred, predictions
        )

    return confidence_scores

def _calculate_ensemble_confidence(self, model_predictor, X_pred, base_predictions):
    confidence_scores = {}
    n_samples = 100  # 100ë²ˆ ì˜ˆì¸¡ ìˆ˜í–‰

    # í•µì‹¬ ì•„ì´ë””ì–´: ë“œë¡­ì•„ì›ƒì„ í™œì„±í™”í•œ ìƒíƒœì—ì„œ ì—¬ëŸ¬ ë²ˆ ì˜ˆì¸¡
    all_predictions = []
    for _ in range(n_samples):
        # verbose=0: ì¶œë ¥ ì•ˆ í•¨, training=True: ë“œë¡­ì•„ì›ƒ í™œì„±í™”
        pred = model_predictor.predict(X_pred, verbose=0)
        all_predictions.append(pred)
        # â†’ ë“œë¡­ì•„ì›ƒ ë•Œë¬¸ì— ë§¤ë²ˆ ì¡°ê¸ˆì”© ë‹¤ë¥¸ ê²°ê³¼ ë‚˜ì™€

    # ê° íƒ€ì„í”„ë ˆì„ë³„ ì‹ ë¢°ë„ ê³„ì‚°
    for timeframe in base_predictions.keys():  # ["7d", "14d", "30d"]
        # 100ë²ˆ ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì§‘
        predictions_array = np.array([pred[timeframe][0] for pred in all_predictions])
        # â†’ [ì˜ˆì¸¡1, ì˜ˆì¸¡2, ..., ì˜ˆì¸¡100] ë°°ì—´

        # í‘œì¤€í¸ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚° (ë‚®ì€ ë¶„ì‚° = ë†’ì€ ì‹ ë¢°ë„)
        std = np.std(predictions_array)      # í‘œì¤€í¸ì°¨ (ì–¼ë§ˆë‚˜ í©ì–´ì ¸ ìˆë‚˜)
        mean_pred = np.mean(predictions_array)  # í‰ê·  ì˜ˆì¸¡ê°’

        if mean_pred != 0:
            relative_std = std / abs(mean_pred)  # ìƒëŒ€ì  í‘œì¤€í¸ì°¨
            # â†’ ì˜ˆì¸¡ê°’ ëŒ€ë¹„ ì–¼ë§ˆë‚˜ í©ì–´ì ¸ ìˆëŠ”ê°€

            confidence = max(0.1, min(0.9, 1.0 - relative_std))
            # â†’ í©ì–´ì§ì´ ì ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ
            # â†’ ìµœì†Œ 0.1, ìµœëŒ€ 0.9ë¡œ ì œí•œ
        else:
            confidence = 0.5  # ì˜ˆì¸¡ê°’ì´ 0ì´ë©´ ì¤‘ê°„ ì‹ ë¢°ë„

        confidence_scores[timeframe] = float(confidence)

    return confidence_scores

# ğŸ’¡ ì‹ ë¢°ë„ ê³„ì‚° ì›ë¦¬:
# 1. ê°™ì€ ì…ë ¥ì— ëŒ€í•´ 100ë²ˆ ì˜ˆì¸¡ (ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ë§¤ë²ˆ ë‹¤ë¦„)
# 2. 100ê°œ ê²°ê³¼ê°€ ë¹„ìŠ·í•˜ë©´ â†’ ì‹ ë¢°ë„ ë†’ìŒ
# 3. 100ê°œ ê²°ê³¼ê°€ í©ì–´ì ¸ ìˆìœ¼ë©´ â†’ ì‹ ë¢°ë„ ë‚®ìŒ
# 4. ì´ê²Œ Monte Carlo Dropoutì´ë¼ëŠ” ê¸°ë²•ì´ì•¼!

# ğŸ¤” ì™œ ì´ë ‡ê²Œ í• ê¹Œ?
# - ëª¨ë¸ì´ í™•ì‹  ìˆìœ¼ë©´ ë“œë¡­ì•„ì›ƒ ìˆì–´ë„ ë¹„ìŠ·í•œ ê²°ê³¼
# - ëª¨ë¸ì´ í—·ê°ˆë¦¬ë©´ ë“œë¡­ì•„ì›ƒì— ë”°ë¼ ê²°ê³¼ê°€ ë§ì´ ë‹¬ë¼ì§
# - ì´ ì°¨ì´ë¥¼ ì´ìš©í•´ì„œ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•˜ëŠ” ê±°ì•¼
```

### ëª¨ë¸ ë¡œë“œ ë° ìºì‹± ë©”ì»¤ë‹ˆì¦˜

ê°™ì€ ëª¨ë¸ì„ ì—¬ëŸ¬ ë²ˆ ë¡œë“œí•˜ì§€ ì•Šê²Œ ìºì‹±í•˜ëŠ” ë˜‘ë˜‘í•œ ì‹œìŠ¤í…œ:

```python
def _load_model(self, symbol, model_version=None):
    cache_key = f"{symbol}_{model_version or 'active'}"
    # â†’ "IXIC_active" ë˜ëŠ” "IXIC_v1.0.0_20250809_120032"

    # ìºì‹œì—ì„œ í™•ì¸
    if cache_key in self.loaded_models:
        logger.debug("model_loaded_from_cache", cache_key=cache_key)
        return self.loaded_models[cache_key]
        # â†’ ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜

    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëª¨ë¸ ì •ë³´ ì¡°íšŒ
    session = SessionLocal()
    model_repository = MLModelRepository(session)

    try:
        if model_version:
            # íŠ¹ì • ë²„ì „ ëª¨ë¸ ì¡°íšŒ
            model_entity = model_repository.find_by_name_and_version(
                f"{symbol.replace('^', '')}_lstm", model_version
            )
        else:
            # í™œì„± ëª¨ë¸ ì¡°íšŒ (ì„±ëŠ¥ ê¸°ë°˜ ìë™ ì„ íƒ)
            model_entity = model_repository.find_active_model("lstm", symbol)

        if not model_entity:
            raise ValueError(f"No suitable model found for {symbol}")

        # ëª¨ë¸ íŒŒì¼ ë¡œë“œ
        model_predictor = MultiOutputLSTMPredictor(
            input_shape=(ml_settings.model.window_size, 0),  # ì‹¤ì œ í¬ê¸°ëŠ” ë¡œë“œ ì‹œ ê²°ì •
            config=ml_settings.model,
            model_name=model_entity.model_name,
        )

        # .keras íŒŒì¼ ë¡œë“œ
        model_predictor.load_model(
            filepath=model_entity.model_path,  # "models/.../model.keras"
            load_format=ml_settings.storage.model_format,  # "keras"
        )

        # ìŠ¤ì¼€ì¼ëŸ¬ë“¤ë„ ë¡œë“œ (ì •ê·œí™”/ì—­ì •ê·œí™”ìš©)
        scaler_dir = os.path.join(os.path.dirname(model_entity.model_path), "scalers")
        if os.path.exists(scaler_dir):
            self.preprocessor.feature_engineer.load_scalers(scaler_dir)
            # â†’ feature_scaler.pkl, target_scaler_7d.pkl ë“±ë“± ë¡œë“œ

        # ìºì‹œì— ì €ì¥ (ë‹¤ìŒì— ë¹¨ë¦¬ ì“°ë ¤ê³ )
        self.loaded_models[cache_key] = (model_predictor, model_entity)

        return model_predictor, model_entity

    finally:
        session.close()

# ğŸš€ ìºì‹±ì˜ íš¨ê³¼:
# - ì²« ë²ˆì§¸ ì˜ˆì¸¡: ëª¨ë¸ ë¡œë“œ (2ì´ˆ) + ì˜ˆì¸¡ (0.1ì´ˆ) = 2.1ì´ˆ
# - ë‘ ë²ˆì§¸ ì˜ˆì¸¡: ìºì‹œ ì‚¬ìš© (0ì´ˆ) + ì˜ˆì¸¡ (0.1ì´ˆ) = 0.1ì´ˆ
# - 20ë°° ë¹¨ë¼ì§!
```

---

## ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì½”ë“œ ë¶„ì„

### MLPredictionService ë¹„ë™ê¸° ì²˜ë¦¬ ì›ë¦¬

ì´ í´ë˜ìŠ¤ëŠ” ë§¤ë‹ˆì € ì—­í• ì´ì•¼. ëª¨ë“  ML ì‘ì—…ì„ ì¡°ìœ¨í•˜ê³  ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•´:

```python
class MLPredictionService:
    def __init__(self):
        self.trainer = ModelTrainer()           # ëª¨ë¸ í›ˆë ¨ ë‹´ë‹¹
        self.predictor = MultiTimeframePredictor()  # ì˜ˆì¸¡ ë‹´ë‹¹
        self.evaluator = ModelEvaluator()       # ì„±ëŠ¥ í‰ê°€ ë‹´ë‹¹

        # CPU ì§‘ì•½ì  ì‘ì—…ì„ ìœ„í•œ ìŠ¤ë ˆë“œ í’€
        self.executor = ThreadPoolExecutor(max_workers=3)
        # â†’ ìµœëŒ€ 3ê°œ ì‘ì—…ì„ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆì–´
        # â†’ ML ì‘ì—…ì€ CPUë¥¼ ë§ì´ ì¨ì„œ ë„ˆë¬´ ë§ì´ í•˜ë©´ ì˜¤íˆë ¤ ëŠë ¤ì ¸

async def train_model(self, symbol, training_days=1000, force_retrain=False):
    # ê¸°ì¡´ ëª¨ë¸ í™•ì¸ (ë¹ ë¥¸ DB ì¡°íšŒ)
    if not force_retrain:
        existing_model = await self._check_existing_model(symbol)
        if existing_model:
            return {"status": "skipped", "existing_model": existing_model}
            # â†’ ì´ë¯¸ ëª¨ë¸ ìˆìœ¼ë©´ í›ˆë ¨ ì•ˆ í•˜ê³  ë°”ë¡œ ë°˜í™˜

    # CPU ì§‘ì•½ì  í›ˆë ¨ ì‘ì—…ì„ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
    training_result = await asyncio.get_event_loop().run_in_executor(
        self.executor,           # ìŠ¤ë ˆë“œ í’€
        self._train_model_sync,  # ì‹¤í–‰í•  í•¨ìˆ˜
        symbol,                  # í•¨ìˆ˜ ì¸ìë“¤
        training_days,
        force_retrain
    )
    # â†’ ì´ ë¶€ë¶„ì´ í•µì‹¬! ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰

    return training_result

def _train_model_sync(self, symbol, training_days, force_retrain):
    """ë™ê¸° ëª¨ë¸ í›ˆë ¨ (ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰)"""
    end_date = date.today()
    start_date = end_date - timedelta(days=training_days)

    # ì‹¤ì œ í›ˆë ¨ ì‹¤í–‰ (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
    return self.trainer.train_model(
        symbol=symbol,
        start_date=start_date,
        end_date=end_date,
        force_retrain=force_retrain
    )

# ğŸ¤” ì™œ ì´ë ‡ê²Œ ë³µì¡í•˜ê²Œ í• ê¹Œ?
# 1. ML í›ˆë ¨ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ (ëª‡ ë¶„~ëª‡ ì‹œê°„)
# 2. ë™ê¸°ë¡œ í•˜ë©´ API ì„œë²„ê°€ ë©ˆì¶°ë²„ë ¤
# 3. ë¹„ë™ê¸°ë¡œ í•˜ë©´ ë‹¤ë¥¸ ìš”ì²­ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´
# 4. ì‚¬ìš©ìëŠ” "í›ˆë ¨ ì‹œì‘ë¨" ì‘ë‹µ ë°›ê³  ë‚˜ì¤‘ì— ê²°ê³¼ í™•ì¸
```

### ThreadPoolExecutor ì‚¬ìš©ë²•

íŒŒì´ì¬ì˜ ë¹„ë™ê¸° ì²˜ë¦¬ ë°©ë²• ì¤‘ í•˜ë‚˜ì•¼:

```python
# ì¼ë°˜ì ì¸ ë™ê¸° ì²˜ë¦¬ (ë‚˜ìœ ì˜ˆ)
def bad_api():
    result = heavy_ml_work()  # 10ë¶„ ê±¸ë¦¼
    return result
    # â†’ 10ë¶„ ë™ì•ˆ ì„œë²„ê°€ ë‹¤ë¥¸ ìš”ì²­ ì²˜ë¦¬ ëª» í•¨

# ë¹„ë™ê¸° ì²˜ë¦¬ (ì¢‹ì€ ì˜ˆ)
async def good_api():
    result = await asyncio.get_event_loop().run_in_executor(
        executor,      # ìŠ¤ë ˆë“œ í’€
        heavy_ml_work  # ë¬´ê±°ìš´ ì‘ì—…
    )
    return result
    # â†’ ë¬´ê±°ìš´ ì‘ì—…ì€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ, ë©”ì¸ ìŠ¤ë ˆë“œëŠ” ë‹¤ë¥¸ ìš”ì²­ ì²˜ë¦¬

# ğŸ¯ í•µì‹¬ ê°œë…:
# - ë©”ì¸ ìŠ¤ë ˆë“œ: API ìš”ì²­ ë°›ê³  ì‘ë‹µí•˜ëŠ” ì—­í• 
# - ì›Œì»¤ ìŠ¤ë ˆë“œ: ì‹¤ì œ ML ì‘ì—… ìˆ˜í–‰
# - ë¹„ë™ê¸°: ë©”ì¸ ìŠ¤ë ˆë“œê°€ ë¸”ë¡ë˜ì§€ ì•ŠìŒ
```

### ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ì½”ë“œ

ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DBì— ì €ì¥í•˜ëŠ” ë¶€ë¶„:

```python
async def _save_prediction_results(self, symbol, prediction_date, prediction_result):
    session = SessionLocal()  # DB ì„¸ì…˜ ìƒì„±
    prediction_repo = MLPredictionRepository(session)

    try:
        # ì „ì²´ ê²°ê³¼ì—ì„œ ê³µí†µ ì •ë³´ ì¶”ì¶œ
        batch_id = prediction_result.get("batch_id")
        model_version = prediction_result.get("model_version", "unknown")
        current_price = prediction_result.get("current_price", 0.0)

        # ê° ì˜ˆì¸¡ ê¸°ê°„ë³„ë¡œ DB ë ˆì½”ë“œ ìƒì„±
        for pred in prediction_result.get("predictions", []):
            prediction_entity = MLPrediction(
                symbol=symbol,                           # "^IXIC"
                prediction_date=prediction_date,         # 2025-08-09
                target_date=pred["target_date"],         # 2025-08-16 (7ì¼ í›„)
                prediction_timeframe=pred["timeframe"], # "7d"
                predicted_price=pred["predicted_price"], # 15234.56
                predicted_direction=pred["predicted_direction"], # "up"
                confidence_score=pred["confidence_score"], # 0.75
                current_price=current_price,             # 15000.0
                model_version=model_version,             # "v1.0.0_20250809_120032"
                model_type="lstm",                       # "lstm"
                batch_id=batch_id,                       # UUID
                created_at=datetime.now(),               # í˜„ì¬ ì‹œê°„
            )

            prediction_repo.save(prediction_entity)  # DBì— ì €ì¥
            # â†’ ì´ë ‡ê²Œ í•´ì„œ ë‚˜ì¤‘ì— ì˜ˆì¸¡ ì´ë ¥ ì¡°íšŒ ê°€ëŠ¥

        logger.info("prediction_results_saved",
                   symbol=symbol, batch_id=batch_id,
                   predictions_count=len(prediction_result.get("predictions", [])))

    except Exception as e:
        logger.error("prediction_results_save_failed", error=str(e))
        raise
    finally:
        session.close()  # DB ì„¸ì…˜ ì •ë¦¬

# ğŸ’¾ DB ì €ì¥ ì´ìœ :
# 1. ì˜ˆì¸¡ ì´ë ¥ ì¶”ì  (ë‚˜ì¤‘ì— ì •í™•ë„ ê³„ì‚°)
# 2. ì‚¬ìš©ìë³„ ì˜ˆì¸¡ ê¸°ë¡ ê´€ë¦¬
# 3. ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
# 4. ê°ì‚¬(Audit) ëª©ì 
```

---

## API ë ˆì´ì–´ ì½”ë“œ ë¶„ì„

### FastAPI ë¼ìš°í„° ë™ì‘ ì›ë¦¬

ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ì ‘ê·¼í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸ë“¤ì´ì•¼:

```python
@router.post("/train", response_model=TrainModelResponse, status_code=status.HTTP_201_CREATED)
async def train_model(
    request: TrainModelRequest,  # ìš”ì²­ ë°ì´í„° (ìë™ ê²€ì¦ë¨)
    handler: MLPredictionHandler = Depends(get_ml_handler)  # ì˜ì¡´ì„± ì£¼ì…
) -> TrainModelResponse:
    """ëª¨ë¸ í›ˆë ¨ ì—”ë“œí¬ì¸íŠ¸"""

    request_id = str(uuid.uuid4())  # ìš”ì²­ ì¶”ì ìš© ID

    # ë¡œê¹… (ì–´ë–¤ ìš”ì²­ì´ ë“¤ì–´ì™”ëŠ”ì§€ ê¸°ë¡)
    logger.info("train_model_endpoint_called",
               request_id=request_id,
               symbol=request.symbol,
               training_days=request.training_days)

    # ì‹¤ì œ ì²˜ë¦¬ëŠ” í•¸ë“¤ëŸ¬ì—ê²Œ ìœ„ì„
    return await handler.train_model(request, request_id)

# ğŸ” ê° ë¶€ë¶„ ì„¤ëª…:
# - @router.post: POST ìš”ì²­ ë°›ëŠ” ì—”ë“œí¬ì¸íŠ¸
# - response_model: ì‘ë‹µ í˜•íƒœ ì •ì˜ (ìë™ ë¬¸ì„œí™”)
# - status_code: ì„±ê³µ ì‹œ 201 ë°˜í™˜
# - Depends: ì˜ì¡´ì„± ì£¼ì… (í•¸ë“¤ëŸ¬ ìë™ ìƒì„±)
# - async def: ë¹„ë™ê¸° í•¨ìˆ˜ (ë‹¤ë¥¸ ìš”ì²­ ë¸”ë¡ ì•ˆ í•¨)

@router.post("/predict", response_model=PredictionResponse)
async def predict_prices(
    request: PredictionRequest,
    handler: MLPredictionHandler = Depends(get_ml_handler)
) -> PredictionResponse:
    """ê°€ê²© ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸"""

    request_id = str(uuid.uuid4())

    logger.info("predict_prices_endpoint_called",
               request_id=request_id,
               symbol=request.symbol,
               prediction_date=request.prediction_date)

    return await handler.predict_prices(request, request_id)

# ğŸ’¡ FastAPIì˜ ì¥ì :
# 1. ìë™ ì…ë ¥ ê²€ì¦ (Pydantic ëª¨ë¸ ê¸°ë°˜)
# 2. ìë™ API ë¬¸ì„œ ìƒì„± (Swagger UI)
# 3. íƒ€ì… íŒíŠ¸ ê¸°ë°˜ ê°œë°œ
# 4. ë¹„ë™ê¸° ì²˜ë¦¬ ë‚´ì¥ ì§€ì›
```

### ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì´í•´

ì½”ë“œì˜ ê²°í•©ë„ë¥¼ ë‚®ì¶”ëŠ” ë˜‘ë˜‘í•œ íŒ¨í„´ì´ì•¼:

```python
# ì˜ì¡´ì„± ì£¼ì… í•¨ìˆ˜
def get_ml_handler() -> MLPredictionHandler:
    """ML ì˜ˆì¸¡ í•¸ë“¤ëŸ¬ ì˜ì¡´ì„±"""
    return MLPredictionHandler()
    # â†’ ë§¤ ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œìš´ í•¸ë“¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

# ì‚¬ìš©í•˜ëŠ” ê³³
async def train_model(
    request: TrainModelRequest,
    handler: MLPredictionHandler = Depends(get_ml_handler)  # ì—¬ê¸°ì„œ ì£¼ì…
):
    # handlerëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë˜ì–´ ì „ë‹¬ë¨
    return await handler.train_model(request, request_id)

# ğŸ¤” ì™œ ì´ë ‡ê²Œ í• ê¹Œ?
# 1. í…ŒìŠ¤íŠ¸ ìš©ì´ì„±: ê°€ì§œ í•¸ë“¤ëŸ¬ë¡œ ì‰½ê²Œ êµì²´ ê°€ëŠ¥
# 2. ì„¤ì • ê´€ë¦¬: í™˜ê²½ë³„ë¡œ ë‹¤ë¥¸ êµ¬í˜„ì²´ ì£¼ì… ê°€ëŠ¥
# 3. ìƒëª…ì£¼ê¸° ê´€ë¦¬: FastAPIê°€ ìë™ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
# 4. ì½”ë“œ ë¶„ë¦¬: ë¼ìš°í„°ëŠ” HTTPë§Œ, í•¸ë“¤ëŸ¬ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ë§Œ

# ë‚˜ìœ ì˜ˆ (ì§ì ‘ ìƒì„±)
async def bad_train_model(request):
    handler = MLPredictionHandler()  # í•˜ë“œì½”ë”©
    return await handler.train_model(request)
    # â†’ í…ŒìŠ¤íŠ¸í•˜ê¸° ì–´ë µê³ , ì„¤ì • ë³€ê²½ ì–´ë ¤ì›€

# ì¢‹ì€ ì˜ˆ (ì˜ì¡´ì„± ì£¼ì…)
async def good_train_model(request, handler=Depends(get_ml_handler)):
    return await handler.train_model(request)
    # â†’ ìœ ì—°í•˜ê³  í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
```

### ì—ëŸ¬ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜

APIì—ì„œ ë°œìƒí•˜ëŠ” ë‹¤ì–‘í•œ ì—ëŸ¬ë“¤ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•:

```python
class MLPredictionHandler:
    async def train_model(self, request: TrainModelRequest, request_id: str):
        try:
            # ì…ë ¥ ê²€ì¦
            if request.training_days < 100 or request.training_days > 5000:
                raise ValueError("Training days must be between 100 and 5000")
                # â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜

            # ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í˜¸ì¶œ
            result = await self.ml_service.train_model(
                symbol=request.symbol,
                training_days=request.training_days,
                force_retrain=request.force_retrain
            )

            # ì„±ê³µ ì‘ë‹µ ìƒì„±
            return TrainModelResponse(
                status=result["status"],
                model_name=result.get("model_name"),
                model_version=result.get("model_version"),
                training_metadata=result.get("training_metadata"),
                request_id=request_id
            )

        except ValueError as e:
            # ì…ë ¥ ê²€ì¦ ì˜¤ë¥˜ (400 Bad Request)
            logger.error("validation_error", request_id=request_id, error=str(e))
            raise HTTPException(status_code=400, detail=str(e))

        except FileNotFoundError as e:
            # ëª¨ë¸ íŒŒì¼ ì—†ìŒ (404 Not Found)
            logger.error("model_not_found", request_id=request_id, error=str(e))
            raise HTTPException(status_code=404, detail="Model not found")

        except Exception as e:
            # ê¸°íƒ€ ëª¨ë“  ì˜¤ë¥˜ (500 Internal Server Error)
            logger.error("unexpected_error", request_id=request_id, error=str(e))
            raise HTTPException(status_code=500, detail="Internal server error")

# ğŸ¯ ì—ëŸ¬ ì²˜ë¦¬ ì „ëµ:
# 1. êµ¬ì²´ì ì¸ ì˜ˆì™¸ë¶€í„° ì²˜ë¦¬ (ValueError â†’ Exception ìˆœì„œ)
# 2. ì ì ˆí•œ HTTP ìƒíƒœ ì½”ë“œ ë°˜í™˜
# 3. ì‚¬ìš©ìì—ê²ŒëŠ” ê°„ë‹¨í•œ ë©”ì‹œì§€, ë¡œê·¸ì—ëŠ” ìƒì„¸ ì •ë³´
# 4. request_idë¡œ ìš”ì²­ ì¶”ì  ê°€ëŠ¥

# ğŸ“Š HTTP ìƒíƒœ ì½”ë“œ ì˜ë¯¸:
# - 200: ì„±ê³µ
# - 201: ìƒì„± ì„±ê³µ (ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ)
# - 400: ì˜ëª»ëœ ìš”ì²­ (ì…ë ¥ê°’ ì˜¤ë¥˜)
# - 404: ë¦¬ì†ŒìŠ¤ ì—†ìŒ (ëª¨ë¸ ì—†ìŒ)
# - 500: ì„œë²„ ì˜¤ë¥˜ (ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜)
```

---

## í—·ê°ˆë ¸ë˜ ë¶€ë¶„ë“¤ ì •ë¦¬

### ë©€í‹° ì•„ì›ƒí’‹ ëª¨ë¸ íƒ€ê²Ÿ ë°ì´í„° ì²˜ë¦¬

ì´ ë¶€ë¶„ì´ ì œì¼ í—·ê°ˆë ¸ì–´. ë”•ì…”ë„ˆë¦¬ â†” ë¦¬ìŠ¤íŠ¸ ë³€í™˜ì„ ê³„ì† í•´ì•¼ í•´:

```python
# ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ì—ì„œëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœ
y_dict = {
    "7d": [15100, 15200, 15300, ...],   # 7ì¼ í›„ ê°€ê²©ë“¤
    "14d": [15150, 15250, 15350, ...],  # 14ì¼ í›„ ê°€ê²©ë“¤
    "30d": [15200, 15300, 15400, ...]   # 30ì¼ í›„ ê°€ê²©ë“¤
}

# ëª¨ë¸ í›ˆë ¨í•  ë•ŒëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
y_train_list = [y_dict["7d"], y_dict["14d"], y_dict["30d"]]
# â†’ [[7ì¼ê°€ê²©ë“¤], [14ì¼ê°€ê²©ë“¤], [30ì¼ê°€ê²©ë“¤]]

# ğŸ¤” ì™œ ì´ë ‡ê²Œ í•´ì•¼ í• ê¹Œ?
# 1. ë”•ì…”ë„ˆë¦¬: ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ì›€ (í‚¤ë¡œ êµ¬ë¶„)
# 2. ë¦¬ìŠ¤íŠ¸: ì¼€ë¼ìŠ¤ê°€ ìš”êµ¬í•˜ëŠ” í˜•íƒœ (ìˆœì„œë¡œ êµ¬ë¶„)
# 3. ë³€í™˜í•  ë•Œ ìˆœì„œê°€ ì¤‘ìš”! ëª¨ë¸ ì¶œë ¥ ìˆœì„œì™€ ì¼ì¹˜í•´ì•¼ í•¨

# âš ï¸ ì£¼ì˜ì‚¬í•­:
# - self.target_days = [7, 14, 30] ìˆœì„œ
# - ëª¨ë¸ ì¶œë ¥ë„ [7ì¼, 14ì¼, 30ì¼] ìˆœì„œ
# - y_train_listë„ [7ì¼, 14ì¼, 30ì¼] ìˆœì„œ
# â†’ ì´ ìˆœì„œê°€ í•˜ë‚˜ë¼ë„ í‹€ë¦¬ë©´ ì—‰ëš±í•œ ê²°ê³¼!
```

### ì‹œê³„ì—´ ë°ì´í„° ë¶„í•  ë°©ë²•

ì¼ë°˜ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë‹¤ë¥´ê²Œ ì‹œê³„ì—´ì€ ì…”í”Œí•˜ë©´ ì•ˆ ë¼:

```python
# ì¼ë°˜ ML (ë‚˜ìœ ì˜ˆ - ì‹œê³„ì—´ì—ì„œëŠ” ì ˆëŒ€ ê¸ˆì§€!)
def bad_split(X, y):
    # ëœë¤ ì…”í”Œ í›„ ë¶„í• 
    indices = np.random.permutation(len(X))
    X_shuffled = X[indices]
    y_shuffled = y[indices]
    # â†’ ë¯¸ë˜ ì •ë³´ë¡œ ê³¼ê±° ì˜ˆì¸¡í•˜ê²Œ ë¨ (ë°ì´í„° ëˆ„ì¶œ!)

# ì‹œê³„ì—´ ML (ì¢‹ì€ ì˜ˆ)
def good_split(X, y):
    # ì‹œê°„ ìˆœì„œ ìœ ì§€í•˜ë©° ë¶„í• 
    total_samples = len(X)
    train_end = int(total_samples * 0.7)      # 70%
    val_end = int(total_samples * 0.85)       # 15%

    X_train = X[:train_end]          # ê³¼ê±° ë°ì´í„°
    X_val = X[train_end:val_end]     # ì¤‘ê°„ ë°ì´í„°
    X_test = X[val_end:]             # ìµœì‹  ë°ì´í„°
    # â†’ ì‹œê°„ ìˆœì„œ ë³´ì¥!

# ğŸ¯ í•µì‹¬ ì›ì¹™:
# - í›ˆë ¨: ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°
# - ê²€ì¦: ì¤‘ê°„ ì‹œê¸° ë°ì´í„°
# - í…ŒìŠ¤íŠ¸: ê°€ì¥ ìµœì‹  ë°ì´í„°
# â†’ ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ì™€ ë™ì¼
```

### ë¹„ë™ê¸° ì²˜ë¦¬ì™€ ìŠ¤ë ˆë“œ í’€

íŒŒì´ì¬ì˜ ë¹„ë™ê¸° ì²˜ë¦¬ê°€ í—·ê°ˆë¦´ ìˆ˜ ìˆì–´:

```python
# ë™ê¸° í•¨ìˆ˜ (CPU ì§‘ì•½ì )
def heavy_work():
    # ë³µì¡í•œ ê³„ì‚° (10ì´ˆ ê±¸ë¦¼)
    return result

# ë¹„ë™ê¸° ë˜í¼
async def async_heavy_work():
    # ìŠ¤ë ˆë“œ í’€ì—ì„œ ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
    result = await asyncio.get_event_loop().run_in_executor(
        executor,    # ThreadPoolExecutor ì¸ìŠ¤í„´ìŠ¤
        heavy_work   # ì‹¤í–‰í•  ë™ê¸° í•¨ìˆ˜
    )
    return result

# ğŸ¤” ì™œ ì´ë ‡ê²Œ ë³µì¡í•˜ê²Œ?
# 1. heavy_work()ëŠ” CPU ì§‘ì•½ì  â†’ ë¹„ë™ê¸°ë¡œ ë§Œë“¤ ìˆ˜ ì—†ìŒ
# 2. í•˜ì§€ë§Œ APIëŠ” ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•˜ê³  ì‹¶ìŒ
# 3. í•´ê²°ì±…: ë™ê¸° í•¨ìˆ˜ë¥¼ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
# 4. ë©”ì¸ ìŠ¤ë ˆë“œëŠ” ë‹¤ë¥¸ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥

# ğŸ“Š ì²˜ë¦¬ íë¦„:
# 1. ì‚¬ìš©ì ìš”ì²­ â†’ FastAPI (ë©”ì¸ ìŠ¤ë ˆë“œ)
# 2. ë¬´ê±°ìš´ ì‘ì—… â†’ ì›Œì»¤ ìŠ¤ë ˆë“œ
# 3. ë©”ì¸ ìŠ¤ë ˆë“œëŠ” ë‹¤ë¥¸ ìš”ì²­ ì²˜ë¦¬
# 4. ì›Œì»¤ ìŠ¤ë ˆë“œ ì™„ë£Œ â†’ ê²°ê³¼ ë°˜í™˜
```

### ëª¨ë¸ ì €ì¥/ë¡œë“œ ê³¼ì •

ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í•¨ê»˜ ê´€ë¦¬í•˜ëŠ” ê²Œ ë³µì¡í•´:

```python
# ì €ì¥ ê³¼ì •
def save_model(self, model_path):
    # 1. ëª¨ë¸ íŒŒì¼ ì €ì¥
    self.model.save("model.keras")

    # 2. ìŠ¤ì¼€ì¼ëŸ¬ë“¤ ì €ì¥ (ì¤‘ìš”!)
    scaler_dir = os.path.join(model_dir, "scalers")
    os.makedirs(scaler_dir, exist_ok=True)

    # ì…ë ¥ íŠ¹ì„± ìŠ¤ì¼€ì¼ëŸ¬
    joblib.dump(self.feature_scaler, f"{scaler_dir}/feature_scaler.pkl")

    # ê° íƒ€ê²Ÿë³„ ìŠ¤ì¼€ì¼ëŸ¬
    for days in [7, 14, 30]:
        scaler = self.target_scalers[f"{days}d"]
        joblib.dump(scaler, f"{scaler_dir}/target_scaler_{days}d.pkl")

    # 3. ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {"input_shape": self.input_shape, ...}
    with open("model_metadata.json", "w") as f:
        json.dump(metadata, f)

# ë¡œë“œ ê³¼ì •
def load_model(self, model_path):
    # 1. ëª¨ë¸ íŒŒì¼ ë¡œë“œ
    self.model = keras.models.load_model("model.keras")

    # 2. ìŠ¤ì¼€ì¼ëŸ¬ë“¤ ë¡œë“œ
    scaler_dir = os.path.join(model_dir, "scalers")
    self.feature_scaler = joblib.load(f"{scaler_dir}/feature_scaler.pkl")

    for days in [7, 14, 30]:
        scaler = joblib.load(f"{scaler_dir}/target_scaler_{days}d.pkl")
        self.target_scalers[f"{days}d"] = scaler

    # 3. ë©”íƒ€ë°ì´í„° ë¡œë“œ
    with open("model_metadata.json", "r") as f:
        metadata = json.load(f)

# âš ï¸ ì£¼ì˜ì‚¬í•­:
# - ëª¨ë¸ë§Œ ì €ì¥í•˜ë©´ ì•ˆ ë¼! ìŠ¤ì¼€ì¼ëŸ¬ë„ í•¨ê»˜ ì €ì¥
# - ì˜ˆì¸¡í•  ë•Œ í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©í•´ì•¼ í•¨
# - ë©”íƒ€ë°ì´í„°ë¡œ í˜¸í™˜ì„± ì²´í¬ í•„ìˆ˜
```

---

## ë””ë²„ê¹…í•  ë•Œ ìœ ìš©í•œ íŒë“¤

### ê° ë‹¨ê³„ë³„ ë°ì´í„° í˜•íƒœ í™•ì¸ ë°©ë²•

ML íŒŒì´í”„ë¼ì¸ì—ì„œ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²Œ ì¤‘ìš”í•´:

```python
# 1. ì›ì‹œ ë°ì´í„° í™•ì¸
print(f"Raw data shape: {raw_data.shape}")
print(f"Raw data columns: {raw_data.columns.tolist()}")
print(f"Raw data sample:\n{raw_data.head()}")

# 2. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í›„ í™•ì¸
print(f"Feature data shape: {feature_data.shape}")
print(f"Feature columns: {feature_data.columns.tolist()}")

# 3. ì‹œí€€ìŠ¤ ìƒì„± í›„ í™•ì¸
print(f"X shape: {X.shape}")  # (ìƒ˜í”Œìˆ˜, 60, 152)
print(f"y_dict keys: {y_dict.keys()}")  # ['7d', '14d', '30d']
for key, value in y_dict.items():
    print(f"y_{key} shape: {value.shape}")

# 4. ì •ê·œí™” í›„ í™•ì¸
print(f"X_normalized range: {X_normalized.min():.3f} ~ {X_normalized.max():.3f}")
for key, value in y_normalized_dict.items():
    print(f"y_{key}_normalized range: {value.min():.3f} ~ {value.max():.3f}")

# 5. ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
print(f"Raw predictions: {raw_predictions}")
print(f"Denormalized predictions: {denormalized_predictions}")
```

### ìì£¼ ë°œìƒí•˜ëŠ” ì—ëŸ¬ì™€ í•´ê²°ë²•

**1. í˜•íƒœ ë¶ˆì¼ì¹˜ ì—ëŸ¬**

```python
# ì—ëŸ¬: ValueError: Input 0 of layer "lstm_1" is incompatible with the layer
# ì›ì¸: ì…ë ¥ ë°ì´í„° í˜•íƒœê°€ ëª¨ë¸ ê¸°ëŒ€ì™€ ë‹¤ë¦„
# í•´ê²°: ë°ì´í„° í˜•íƒœ í™•ì¸
print(f"Expected: (batch_size, 60, 152)")
print(f"Actual: {X.shape}")

# ìˆ˜ì • ë°©ë²•
if len(X.shape) == 2:
    X = X.reshape(X.shape[0], 60, -1)  # 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜
```

**2. ìŠ¤ì¼€ì¼ëŸ¬ ê´€ë ¨ ì—ëŸ¬**

```python
# ì—ëŸ¬: sklearn.exceptions.NotFittedError: This MinMaxScaler instance is not fitted yet
# ì›ì¸: ìŠ¤ì¼€ì¼ëŸ¬ê°€ fitë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ transform ì‹œë„
# í•´ê²°: fit ì—¬ë¶€ í™•ì¸
if hasattr(scaler, 'data_min_'):
    print("Scaler is fitted")
else:
    print("Scaler is NOT fitted - need to fit first")
    scaler.fit(data)
```

**3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬**

```python
# ì—ëŸ¬: MemoryError: Unable to allocate array
# ì›ì¸: ë°ì´í„°ê°€ ë„ˆë¬´ ì»¤ì„œ ë©”ëª¨ë¦¬ ë¶€ì¡±
# í•´ê²°: ë°°ì¹˜ ì²˜ë¦¬
def process_in_batches(data, batch_size=1000):
    results = []
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        result = model.predict(batch)
        results.append(result)
    return np.concatenate(results)
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸

**1. í›ˆë ¨ ê³¼ì • ëª¨ë‹ˆí„°ë§**

```python
# í›ˆë ¨ ì¤‘ ì†ì‹¤ ë³€í™” í™•ì¸
history = model.fit(...)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.show()

# ê° ì¶œë ¥ë³„ ì†ì‹¤ í™•ì¸
for key in ['price_prediction_7d_loss', 'price_prediction_14d_loss', 'price_prediction_30d_loss']:
    if key in history.history:
        print(f"{key}: {history.history[key][-1]:.4f}")
```

**2. ì˜ˆì¸¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**

```python
# ì˜ˆì¸¡ ì‹œê°„ ì¸¡ì •
import time
start_time = time.time()
predictions = model.predict(X_test)
prediction_time = time.time() - start_time
print(f"Prediction time: {prediction_time:.3f} seconds for {len(X_test)} samples")
print(f"Average time per sample: {prediction_time/len(X_test)*1000:.1f} ms")

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
import psutil
process = psutil.Process()
memory_mb = process.memory_info().rss / 1024 / 1024
print(f"Memory usage: {memory_mb:.1f} MB")
```

**3. ëª¨ë¸ í¬ê¸° í™•ì¸**

```python
# ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜
total_params = model.count_params()
trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# ëª¨ë¸ íŒŒì¼ í¬ê¸°
import os
model_size_mb = os.path.getsize('model.keras') / 1024 / 1024
print(f"Model file size: {model_size_mb:.1f} MB")
```

---

## ë§ˆë¬´ë¦¬

ì´ ë¬¸ì„œëŠ” ë‚´ê°€ ë§Œë“  ML ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œì˜ ëª¨ë“  ì½”ë“œë¥¼ ìƒì„¸í•˜ê²Œ ë¶„ì„í•œ ê°œì¸ í•™ìŠµìš© ìë£Œì•¼.

ë‚˜ì¤‘ì— ì´ ì½”ë“œë¥¼ ë‹¤ì‹œ ë³¼ ë•Œ, ë˜ëŠ” ë¹„ìŠ·í•œ ì‹œìŠ¤í…œì„ ë§Œë“¤ ë•Œ ì°¸ê³ í•˜ë©´ ë  ê²ƒ ê°™ì•„.

íŠ¹íˆ í—·ê°ˆë ¸ë˜ ë¶€ë¶„ë“¤ (ë©€í‹° ì•„ì›ƒí’‹ ë°ì´í„° ì²˜ë¦¬, ì‹œê³„ì—´ ë¶„í• , ë¹„ë™ê¸° ì²˜ë¦¬ ë“±)ì€ ê¼­ ê¸°ì–µí•´ë‘ì!

---

_ì´ ë¬¸ì„œëŠ” ì‹¤ì œ êµ¬í˜„í•œ ì½”ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ê°œì¸ í•™ìŠµ ë° ë³µìŠµ ëª©ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤._

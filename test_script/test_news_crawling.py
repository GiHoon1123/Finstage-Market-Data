#!/usr/bin/env python3
"""
ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ë‰´ìŠ¤ê°€ í¬ë¡¤ë§ë˜ê³  contents í…Œì´ë¸”ì— ì €ì¥ë˜ëŠ”ì§€ í™•ì¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.news_crawler.infra.model.repository.content_repository import ContentRepository
from app.news_crawler.infra.model.entity.content import Content
from app.common.infra.database.config.database_config import SessionLocal
from app.news_crawler.service.yahoo_news_crawler import YahooNewsCrawler
from app.news_crawler.service.investing_news_crawler import InvestingNewsCrawler
from sqlalchemy import func
from datetime import datetime
import time

def test_news_crawling():
    """ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”§ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # 1. í˜„ì¬ contents í…Œì´ë¸” ìƒíƒœ í™•ì¸
    session = SessionLocal()
    repo = ContentRepository(session)
    
    try:
        # í˜„ì¬ ë‰´ìŠ¤ ê°œìˆ˜ í™•ì¸
        current_count = session.query(func.count(Content.id)).scalar()
        print(f"ğŸ“Š í˜„ì¬ contents í…Œì´ë¸” ë‰´ìŠ¤ ê°œìˆ˜: {current_count}ê°œ")
        
        # ìµœê·¼ ë‰´ìŠ¤ 5ê°œ í™•ì¸
        recent_news = session.query(Content).order_by(Content.crawled_at.desc()).limit(5).all()
        print(f"ğŸ“° ìµœê·¼ ë‰´ìŠ¤ 5ê°œ:")
        for news in recent_news:
            print(f"  - {news.title[:50]}... ({news.symbol}) - {news.crawled_at}")
        
        # 2. Yahoo ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ” Yahoo ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸...")
        test_symbols = ['^TNX', '^VIX', 'XLF']
        
        for symbol in test_symbols:
            print(f"  ğŸ“¡ {symbol} ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘...")
            crawler = YahooNewsCrawler(symbol)
            result = crawler.process_all()
            print(f"    âœ… {symbol} ì²˜ë¦¬ ì™„ë£Œ: {result}")
            time.sleep(1)  # API ì œí•œ ë°©ì§€
        
        # 3. Investing ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ” Investing ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸...")
        investing_crawler = InvestingNewsCrawler("INVESTING:ECONOMY")
        investing_result = investing_crawler.process_all()
        print(f"  âœ… Investing ê²½ì œ ë‰´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ: {investing_result}")
        
        # 4. í¬ë¡¤ë§ í›„ contents í…Œì´ë¸” ìƒíƒœ í™•ì¸
        print(f"\nğŸ“Š í¬ë¡¤ë§ í›„ contents í…Œì´ë¸” ìƒíƒœ...")
        time.sleep(2)  # ì €ì¥ ì™„ë£Œ ëŒ€ê¸°
        
        new_count = session.query(func.count(Content.id)).scalar()
        print(f"ğŸ“Š í¬ë¡¤ë§ í›„ ë‰´ìŠ¤ ê°œìˆ˜: {new_count}ê°œ")
        print(f"ğŸ“ˆ ìƒˆë¡œ ì¶”ê°€ëœ ë‰´ìŠ¤: {new_count - current_count}ê°œ")
        
        # ìµœì‹  ë‰´ìŠ¤ 5ê°œ í™•ì¸
        latest_news = session.query(Content).order_by(Content.crawled_at.desc()).limit(5).all()
        print(f"ğŸ“° í¬ë¡¤ë§ í›„ ìµœì‹  ë‰´ìŠ¤ 5ê°œ:")
        for news in latest_news:
            print(f"  - {news.title[:50]}... ({news.symbol}) - {news.crawled_at}")
        
        # 5. ê±°ì‹œê²½ì œ ì‹¬ë³¼ ë‰´ìŠ¤ í™•ì¸
        print(f"\nğŸ“° ê±°ì‹œê²½ì œ ì‹¬ë³¼ ë‰´ìŠ¤ í˜„í™©:")
        macro_symbols = ['^TNX', '^VIX', 'XLF', 'XLK', '^TYX']
        for symbol in macro_symbols:
            symbol_news = repo.get_by_symbol(symbol, 3)
            print(f"  {symbol}: {len(symbol_news)}ê°œ ë‰´ìŠ¤")
            for news in symbol_news[:2]:  # ìµœê·¼ 2ê°œë§Œ í‘œì‹œ
                print(f"    - {news.title[:40]}... ({news.crawled_at})")
        
        if new_count > current_count:
            print(f"\nâœ… ì„±ê³µ! {new_count - current_count}ê°œì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print(f"\nâš ï¸ ê²½ê³ : ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        session.close()

if __name__ == "__main__":
    test_news_crawling()

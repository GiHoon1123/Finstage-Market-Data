"""
ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ì¸¡ì • ë° ë¹„êµ

ìµœì í™” ì „í›„ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ë° ê°œì„  íš¨ê³¼ ë¬¸ì„œí™”ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

import asyncio
import json
import time
import statistics
from typing import Dict, Any, List
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import pandas as pd

from app.common.utils.logging_config import get_logger
from app.common.monitoring.performance_metrics_collector import get_metrics_collector
from app.common.optimization.optimization_manager import get_optimization_manager

logger = get_logger(__name__)


class PerformanceComparison:
    """ì„±ëŠ¥ ë¹„êµ ë¶„ì„ê¸°"""

    def __init__(self):
        self.metrics_collector = get_metrics_collector()
        self.optimization_manager = get_optimization_manager()
        self.comparison_results = {}

    async def run_performance_comparison(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹¤í–‰"""
        logger.info("performance_comparison_started")

        # 1. í˜„ì¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        current_metrics = await self._collect_current_metrics()

        # 2. ê¸°ì¤€ì„ ê³¼ ë¹„êµ
        baseline_comparison = await self._compare_with_baseline()

        # 3. ìµœì í™”ë³„ íš¨ê³¼ ë¶„ì„
        optimization_effects = await self._analyze_optimization_effects()

        # 4. ì‹œê°„ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        performance_trends = await self._analyze_performance_trends()

        # 5. ì¢…í•© ê°œì„  íš¨ê³¼ ê³„ì‚°
        improvement_summary = self._calculate_improvement_summary(
            baseline_comparison, optimization_effects
        )

        self.comparison_results = {
            "current_metrics": current_metrics,
            "baseline_comparison": baseline_comparison,
            "optimization_effects": optimization_effects,
            "performance_trends": performance_trends,
            "improvement_summary": improvement_summary,
            "analysis_timestamp": datetime.now().isoformat(),
        }

        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        report = await self._generate_performance_report()

        logger.info("performance_comparison_completed")

        return {
            "comparison_results": self.comparison_results,
            "performance_report": report,
        }

    async def _collect_current_metrics(self) -> Dict[str, Any]:
        """í˜„ì¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°ì—ì„œ í˜„ì¬ ìƒíƒœ ì¡°íšŒ
            current_summary = self.metrics_collector.get_metrics_summary(hours=1)

            if "error" in current_summary:
                logger.warning(
                    "current_metrics_collection_failed", error=current_summary["error"]
                )
                return {}

            return current_summary

        except Exception as e:
            logger.error("current_metrics_collection_error", error=str(e))
            return {}

    async def _compare_with_baseline(self) -> Dict[str, Any]:
        """ê¸°ì¤€ì„ ê³¼ ì„±ëŠ¥ ë¹„êµ"""
        try:
            # ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì¡°íšŒ
            comparison = self.optimization_manager.get_performance_comparison()

            if "error" in comparison:
                logger.warning("baseline_comparison_failed", error=comparison["error"])
                return {}

            return comparison

        except Exception as e:
            logger.error("baseline_comparison_error", error=str(e))
            return {}

    async def _analyze_optimization_effects(self) -> Dict[str, Any]:
        """ìµœì í™”ë³„ íš¨ê³¼ ë¶„ì„"""
        try:
            # ìµœì í™” ìƒíƒœ ì¡°íšŒ
            optimization_status = self.optimization_manager.get_optimization_status()

            if "error" in optimization_status:
                return {}

            effects = {}

            # í™œì„±í™”ëœ ìµœì í™”ë“¤ì˜ íš¨ê³¼ ë¶„ì„
            for rule in optimization_status.get("rules", []):
                if rule["status"] == "enabled":
                    rule_id = rule["id"]

                    # ê° ìµœì í™”ì˜ ì˜ˆìƒ íš¨ê³¼ ê³„ì‚°
                    effect = self._estimate_optimization_effect(rule)
                    effects[rule_id] = effect

            return effects

        except Exception as e:
            logger.error("optimization_effects_analysis_error", error=str(e))
            return {}

    def _estimate_optimization_effect(self, rule: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œë³„ ìµœì í™” íš¨ê³¼ ì¶”ì •"""
        rule_type = rule.get("type", "")

        # ìµœì í™” ìœ í˜•ë³„ ì˜ˆìƒ íš¨ê³¼ (ì‹¤ì œ ì¸¡ì • ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì¡°ì • í•„ìš”)
        effect_estimates = {
            "memory_optimization": {
                "memory_reduction_percent": 15,
                "response_time_improvement_percent": 10,
                "cpu_usage_reduction_percent": 8,
            },
            "caching": {
                "response_time_improvement_percent": 40,
                "cache_hit_rate_percent": 85,
                "database_load_reduction_percent": 60,
            },
            "async_processing": {
                "concurrent_request_improvement_percent": 200,
                "response_time_improvement_percent": 25,
                "throughput_improvement_percent": 150,
            },
            "background_tasks": {
                "api_response_time_improvement_percent": 50,
                "system_load_reduction_percent": 30,
                "user_experience_improvement": "significant",
            },
            "websocket_streaming": {
                "real_time_latency_ms": 50,
                "polling_reduction_percent": 90,
                "bandwidth_efficiency_improvement_percent": 70,
            },
        }

        return effect_estimates.get(
            rule_type,
            {
                "estimated_improvement": "unknown",
                "note": f"No specific estimates available for {rule_type}",
            },
        )

    async def _analyze_performance_trends(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            # ìµœê·¼ 24ì‹œê°„ ë©”íŠ¸ë¦­ ìš”ì•½
            recent_summary = self.metrics_collector.get_metrics_summary(hours=24)

            if "error" in recent_summary:
                return {}

            # íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ ì‹œê°„ë³„ ë°ì´í„° ìˆ˜ì§‘
            trends = {
                "cpu_usage_trend": "stable",  # ì‹¤ì œë¡œëŠ” ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ í•„ìš”
                "memory_usage_trend": "improving",
                "response_time_trend": "improving",
                "error_rate_trend": "stable",
                "cache_hit_rate_trend": "improving",
            }

            return {
                "recent_summary": recent_summary,
                "trends": trends,
                "analysis_period_hours": 24,
            }

        except Exception as e:
            logger.error("performance_trends_analysis_error", error=str(e))
            return {}

    def _calculate_improvement_summary(
        self, baseline_comparison: Dict[str, Any], optimization_effects: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì¢…í•© ê°œì„  íš¨ê³¼ ê³„ì‚°"""
        try:
            summary = {
                "overall_improvement_score": 0,
                "key_improvements": [],
                "performance_gains": {},
                "optimization_count": len(optimization_effects),
            }

            # ê¸°ì¤€ì„  ë¹„êµì—ì„œ ê°œì„  ì‚¬í•­ ì¶”ì¶œ
            comparison_data = baseline_comparison.get("comparison", {})

            total_improvement = 0
            improvement_count = 0

            for metric, data in comparison_data.items():
                if data.get("status") == "improved":
                    improvement_percent = data.get("improvement_percent", 0)

                    if improvement_percent > 0:
                        summary["key_improvements"].append(
                            {
                                "metric": metric,
                                "improvement_percent": improvement_percent,
                                "baseline_value": data.get("baseline", 0),
                                "current_value": data.get("current", 0),
                            }
                        )

                        total_improvement += improvement_percent
                        improvement_count += 1

            # ì „ì²´ ê°œì„  ì ìˆ˜ ê³„ì‚°
            if improvement_count > 0:
                summary["overall_improvement_score"] = (
                    total_improvement / improvement_count
                )

            # ì£¼ìš” ì„±ëŠ¥ ì§€í‘œë³„ ê°œì„  íš¨ê³¼
            summary["performance_gains"] = {
                "response_time": self._extract_improvement(
                    comparison_data, "api_response_time_ms"
                ),
                "memory_usage": self._extract_improvement(
                    comparison_data, "memory_used_mb"
                ),
                "cpu_usage": self._extract_improvement(comparison_data, "cpu_percent"),
                "cache_efficiency": self._extract_improvement(
                    comparison_data, "cache_hit_rate"
                ),
            }

            return summary

        except Exception as e:
            logger.error("improvement_summary_calculation_error", error=str(e))
            return {}

    def _extract_improvement(
        self, comparison_data: Dict[str, Any], metric_name: str
    ) -> Dict[str, Any]:
        """íŠ¹ì • ë©”íŠ¸ë¦­ì˜ ê°œì„  íš¨ê³¼ ì¶”ì¶œ"""
        metric_data = comparison_data.get(metric_name, {})

        return {
            "improved": metric_data.get("status") == "improved",
            "improvement_percent": metric_data.get("improvement_percent", 0),
            "baseline": metric_data.get("baseline", 0),
            "current": metric_data.get("current", 0),
        }

    async def _generate_performance_report(self) -> str:
        """ì„±ëŠ¥ ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            report_lines = []

            # ë¦¬í¬íŠ¸ í—¤ë”
            report_lines.extend(
                [
                    "# ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸",
                    f"ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                    "",
                    "## ğŸ“Š ì¢…í•© ê°œì„  íš¨ê³¼",
                ]
            )

            # ì¢…í•© ê°œì„  íš¨ê³¼
            improvement_summary = self.comparison_results.get("improvement_summary", {})
            overall_score = improvement_summary.get("overall_improvement_score", 0)

            report_lines.extend(
                [
                    f"- **ì „ì²´ ê°œì„  ì ìˆ˜**: {overall_score:.1f}%",
                    f"- **ì ìš©ëœ ìµœì í™” ìˆ˜**: {improvement_summary.get('optimization_count', 0)}ê°œ",
                    "",
                ]
            )

            # ì£¼ìš” ê°œì„  ì‚¬í•­
            key_improvements = improvement_summary.get("key_improvements", [])
            if key_improvements:
                report_lines.append("### ğŸ¯ ì£¼ìš” ê°œì„  ì‚¬í•­")

                for improvement in key_improvements[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                    metric = improvement["metric"]
                    percent = improvement["improvement_percent"]
                    baseline = improvement["baseline_value"]
                    current = improvement["current_value"]

                    report_lines.append(
                        f"- **{metric}**: {percent:.1f}% ê°œì„  "
                        f"({baseline:.2f} â†’ {current:.2f})"
                    )

                report_lines.append("")

            # ì„±ëŠ¥ ì§€í‘œë³„ ìƒì„¸ ë¶„ì„
            performance_gains = improvement_summary.get("performance_gains", {})

            report_lines.extend(["### ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œë³„ ë¶„ì„", ""])

            for metric, data in performance_gains.items():
                if data.get("improved"):
                    improvement = data.get("improvement_percent", 0)
                    baseline = data.get("baseline", 0)
                    current = data.get("current", 0)

                    report_lines.extend(
                        [
                            f"#### {metric.replace('_', ' ').title()}",
                            f"- ê°œì„ ìœ¨: **{improvement:.1f}%**",
                            f"- ê¸°ì¤€ì„ : {baseline:.2f}",
                            f"- í˜„ì¬ê°’: {current:.2f}",
                            "",
                        ]
                    )

            # ìµœì í™”ë³„ íš¨ê³¼
            optimization_effects = self.comparison_results.get(
                "optimization_effects", {}
            )

            if optimization_effects:
                report_lines.extend(["### âš¡ ìµœì í™”ë³„ íš¨ê³¼", ""])

                for opt_id, effect in optimization_effects.items():
                    report_lines.extend(
                        [f"#### {opt_id.replace('_', ' ').title()}", ""]
                    )

                    for key, value in effect.items():
                        if isinstance(value, (int, float)):
                            report_lines.append(
                                f"- {key.replace('_', ' ').title()}: {value}"
                            )
                        else:
                            report_lines.append(
                                f"- {key.replace('_', ' ').title()}: {value}"
                            )

                    report_lines.append("")

            # ê¶Œì¥ì‚¬í•­
            report_lines.extend(
                [
                    "### ğŸ’¡ ì¶”ê°€ ê°œì„  ê¶Œì¥ì‚¬í•­",
                    "",
                    "1. **ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§**: ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ì •ê¸°ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ë¥¼ ì¡°ê¸°ì— ê°ì§€",
                    "2. **ì ì§„ì  ìµœì í™”**: ì¶”ê°€ ìµœì í™” ê¸°íšŒë¥¼ ì‹ë³„í•˜ê³  ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©",
                    "3. **A/B í…ŒìŠ¤íŠ¸ í™œìš©**: ìƒˆë¡œìš´ ìµœì í™” ì ìš© ì‹œ A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ íš¨ê³¼ ê²€ì¦",
                    "4. **ì•Œë¦¼ ì‹œìŠ¤í…œ í™œìš©**: ì„±ëŠ¥ ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ìë™ ì•Œë¦¼ì„ í†µí•œ ì‹ ì†í•œ ëŒ€ì‘",
                    "",
                    "---",
                    f"ë¦¬í¬íŠ¸ ìƒì„±: {datetime.now().isoformat()}",
                ]
            )

            return "\n".join(report_lines)

        except Exception as e:
            logger.error("performance_report_generation_error", error=str(e))
            return f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def save_results(self, filename: str = None):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_comparison_{timestamp}.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(self.comparison_results, f, indent=2, ensure_ascii=False)

        logger.info("performance_comparison_results_saved", filename=filename)

    def save_report(self, report_content: str, filename: str = None):
        """ë¦¬í¬íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_improvement_report_{timestamp}.md"

        with open(filename, "w", encoding="utf-8") as f:
            f.write(report_content)

        logger.info("performance_report_saved", filename=filename)


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ë¶„ì„ ì‹œì‘...")

    comparison = PerformanceComparison()
    results = await comparison.run_performance_comparison()

    # ê²°ê³¼ ì¶œë ¥
    improvement_summary = results["comparison_results"].get("improvement_summary", {})
    overall_score = improvement_summary.get("overall_improvement_score", 0)
    optimization_count = improvement_summary.get("optimization_count", 0)

    print(f"\nğŸ“Š ì„±ëŠ¥ ê°œì„  ë¶„ì„ ê²°ê³¼:")
    print(f"   ì „ì²´ ê°œì„  ì ìˆ˜: {overall_score:.1f}%")
    print(f"   ì ìš©ëœ ìµœì í™”: {optimization_count}ê°œ")

    # ì£¼ìš” ê°œì„  ì‚¬í•­ ì¶œë ¥
    key_improvements = improvement_summary.get("key_improvements", [])
    if key_improvements:
        print(f"   ì£¼ìš” ê°œì„  ì‚¬í•­:")
        for improvement in key_improvements[:3]:
            metric = improvement["metric"]
            percent = improvement["improvement_percent"]
            print(f"     - {metric}: {percent:.1f}% ê°œì„ ")

    # ê²°ê³¼ ì €ì¥
    comparison.save_results()
    comparison.save_report(results["performance_report"])

    print(f"\nâœ… ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ë¶„ì„ ì™„ë£Œ!")
    print(f"   ìƒì„¸ ê²°ê³¼ëŠ” ì €ì¥ëœ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    return overall_score >= 10  # 10% ì´ìƒ ê°œì„ ë˜ë©´ ì„±ê³µ


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
